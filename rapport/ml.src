 % -*- Mode: latex -*-

% TEMPORARY incorporate errata reported by Luke Simon

%; whizzy-master top.tex

\contribchapter{The Essence of {ML} Type Inference}
               {Fran{\c{c}}ois Pottier and Didier R{\'{e}}my}
\label{ch:ml}

% FIXME cite Smolka & Treinen (unification on feature trees)?

\index{ML type inference|see{type inference}}
\index{HM(X)|see{type inference}}
\index{inference|see{type inference}}

\index{ML|(}
\index{programming languages!ML|(}
\index{type inference|(}
\index{type inference!HM(X)|(}

% FIXME J'ai mis l'URL qui me plaît, mais elle n'existe pas encore...
\shortfull{\showsystems{Code for \this may
    be found on the book's web site.}} 
   {\showsystems{Code for \this may be found at
    \url{http://cristal.inria.fr/attapl/}.}}

% --------------------------------------------------------------------------

%% FIXME \bcp{Minor remarks on typesetting:
%%   \begin{itemize}
%%   \item I'd rather not use any sans-serif fonts -- how about changing to typewriter?
%%   \item The big figures would look better with ``Cardelli brackets'' above
%%     and below.
%%   \end{itemize}
%% }

\begin{full}
 \section{Preliminaries}

\subsection*{Names and Renaming}

Mathematicians and computer scientists use \emph{names} to refer to arbitrary
or unknown objects in the statement of a theorem, to refer to the parameters
of a function, etc. Names are convenient because they are understandable by
humans; nevertheless, they can be tricky. An in-depth treatment of the
difficulties associated with names and renaming is beyond the scope of the
present chapter: we encourage the reader to study Gabbay and Pitts' excellent
series of papers \cite{gabbay-pitts-02,pitts-02}. Here, we merely recall a few
notions that are used throughout \this.
%
Consider, for instance, an inductive definition of the abstract syntax of a
simple programming language, the pure \lc:
$$\et ::= \evar \mid \efun\evar\et \mid \eapp\et\et$$
Here, the
\emph{meta-variable} $\evar$ ranges over an infinite set of
\emph{variables}---that is, names---while the meta-variable $\et$ ranges over
\emph{terms}. As usual in mathematics, we write ``the variable $\evar$'' and
``the term $\et$'' instead of ``the variable denoted by $\evar$'' and ``the
term denoted by $\et$.'' The above definition states that a term may be a
variable $\evar$, a pair of a variable and a term, written $\efun\evar\et$, or
a pair of terms, written $\eapp{\et_1}{\et_2}$. However, this is not quite
what we need. Indeed, according to this definition, the terms
$\efun{\evar_1}{\evar_1}$ and $\efun{\evar_2}{\evar_2}$ are distinct, while we
would like them to be a single mathematical object, because we intend
$\efun\evar\evar$ to mean ``the function that maps $\evar$ to $\evar$''---a
meaning that is independent of the name~$\evar$. To achieve this effect, we
complete the above definition by stating that the construction $\efun\evar\et$
\emph{binds} $\evar$ within $\et$. One may also say that $\lambda\evar$ is a
\emph{binder} whose \emph{scope} is $\et$. Then, $\efun\evar\et$ is no longer
a pair: rather, it is an \emph{abstraction} of the variable $\evar$ within the
term $\et$.  Abstractions have the property that the identity of the bound
variable does not matter; that is, $\efun{\evar_1}{\evar_1}$ and
$\efun{\evar_2}{\evar_2}$ are the same term. Informally, we say that terms are
considered equal modulo {\em \aconv}. Once the position and scope of binders
are known, several standard notions follow, such as the set of \emph{free
  variables} of a term $\et$, written $\fgv\et$, and the
\emph{capture-avoiding substitution} of a term $\et_1$ for a variable $\evar$
within a term $\et_2$, written $\subst\evar{\et_1}{\et_2}$. For conciseness,
we write $\fgv{\et_1,\et_2}$ for $\fgv{\et_1}\cup\fgv{\et_2}$. A term is said
to be \emph{closed} when it has no free variables.

A \emph{renaming} is a total bijective mapping from variables to variables
that affects only a finite number of variables. The sole property of a
variable is its identity, that is, the fact that it is distinct from other
variables. As a result, at a global level, all variables are interchangeable:
if a theorem holds in the absence of hypotheses about any particular variable,
then any renaming of it holds as well. We often make use of this fact. When
proving a theorem $T$, we say that a hypothesis $H$ may be assumed
\emph{wihout loss of generality} (\spdg) if the theorem $T$ follows from the
theorem $H\Rightarrow T$ via a renaming argument, which is usually left
implicit.

If $\evars_1$ and $\evars_2$ are sets of variables, we write
$\disjoint{\evars_1}{\evars_2}$ as a shorthand for
$\evars_1\cap\evars_2=\varnothing$, and say that $\evars_1$ is \emph{fresh}
for $\evars_2$ (or vice-versa). We say that $\evars$ is fresh for $\et$ if and
only if $\disjoint\evars{\operatorname{fv}(\et)}$ holds.

In this chapter, we work with several distinct varieties of names: program
variables, memory locations, and type variables, the latter of which may be
further divided into \emph{kinds}. We draw names of different varieties from
disjoint sets, each of which is infinite.

% On pourrait parler des conventions de Barendregt, mais je ne suis pas sûr
% que cela soit nécessaire: nous avons explicité les side-conditions partout,
% je crois.

% Nous n'expliquons pas le raisonnement par induction; il est supposé connu.

% -----------------------------------------------------------------------------
\end{full}

\section{What is ML?}
\label{section-intro}

\index{ML!meanings of the term}

The name ML appeared during the late seventies. It then referred to a
general-purpose programming language that was used as a meta-language (whence
its name) within the theorem prover \index{LCF}{\index{theorem provers!LCF}LCF \cite{GORDON79}. Since then, several
new programming languages, each of which offers several different
implementations, have drawn inspiration from it. So, what does ML stand
for today?

For a semanticist, ML might stand for a programming language
featuring first-class functions, data structures built out of products
and sums, mutable memory cells called \emph{references}, exception
handling, automatic memory management, and a call-by-value
semantics. This view encompasses the Standard~ML \cite{Milner&90} and
Caml \cite{OCamlManual} families of programming languages. We refer to
it as \emph{\MLlang.}

\indexlang{Haskell}
\indexlang{Clean}
\indexlang{Standard ML}
\indexlang{Caml}

For a type theorist, ML might stand for a particular breed of type
systems, based on the \stlc, but extended with a simple form of polymorphism
introduced by \kwlet declarations. These type systems have decidable type
inference; their type inference algorithms strongly rely on first-order
unification and can be made efficient in practice. Besides Standard ML
and Caml, this view encompasses programming languages such as
Haskell \cite{peyton-jones:haskell98} and Clean \cite{Brus87}, whose semantics is rather
different---indeed, it is nonstrict and pure \cite{sabry-98}---but whose type system fits this
description. We refer to it as \emph{\MLtype.} It is also referred to as
the \emph{Hindley-Milner type discipline} in the literature.
%% FIXME \bcp{I'm
%%   confused about the history here: I thought Hindley and Milner's original
%%   systems were for unification-based type inference for the simply typed
%%   lambda-calculus with type variables only, and it was Damas and Milner that
%%   added let-polymorphism.  (Perhaps, after checking, you could add a
%%   little historical 
%%   footnote about this?  There is a lot of confusion in the way people use
%%   the terms.)}

%% Didier: Hindler-Milner seems fine to me for referring to the type
%% discipline. This justifies the name HM(X) as used by Odersky. 
%% It emphasizes that ideas (of principal types) have flown from Hindler to
%% Milner even if Hindley's type system did not have Let-polymorphism.
%% 
%% Damas-Milner is always referred to for algorithm W. 
%% It would not harm to call it the Damas-Milner type discipline, either, but
%% I slightly prefer the former.

For us, ML might also stand for the particular programming language whose
formal definition is given and studied in \this. It is a core
calculus featuring first-class functions, local definitions, and
constants. It is equipped with a call-by-value semantics. By customizing
constants and their semantics, one may recover data structures, references,
and more. We refer to this particular calculus as \emph{\MLcalc.}

Why study \MLtype today, such a long time after its initial discovery? One may
think of at least two reasons.

First, its treatment in the literature is often cursory, because it is
considered either as a simple extension of the \stlc~(\TAPLCHAPTER{9}) or as a
subset of Girard and Reynolds' System~F~(\TAPLCHAPTER{23}). The former view is
supported by the claim that local (\kwlet) definitions, which distinguish
\MLtype from the \stlc, may be understood as a simple textual expansion
facility. However, this view  tells only part of the story, because it fails to
give an account of the \emph{principal types} property enjoyed by \MLtype,
leads to a naive type inference algorithm whose time complexity is exponential
not only in the worst case but in the common case, and breaks down when the
language is extended with side effects, such as state or exceptions.
The latter view is supported
by the fact that every type derivation within \MLtype is also a valid type
derivation within an implicitly-typed variant of System~F.  Such a view is
correct but again fails to give an account of type inference for \MLtype,
since type inference for System~F is undecidable \cite{wells-99}.

Second, existing accounts of type inference for \MLtype \cite{Milner78,
  DAMAS82,Tofte88:Thesis,leroy-phd-92,LeeYi:Folklore,jones-thih-99} often
involve heavy manipulations of type substitutions. Such a ubiquitous use of
type substitutions is often quite obscure. Furthermore, actual implementations
of the type inference algorithm do \emph{not} explicitly manipulate
substitutions; instead, they extend a standard first-order unification
algorithm, where terms are updated in place as new equations are
discovered \cite{HuetThesis,Martelli-Montanari-82}. Thus, it
is hard to tell, from these accounts, 
how to write an efficient type inference algorithm for \MLtype.
Yet, in spite of the increasing speed of computers, efficiency remains
crucial when \MLtype is extended with expensive features, such as Objective
Caml's object types \cite{RemyVouillon97}, variant types \cite{garrigue-98}, 
or polymorphic methods \cite{garrigue-remy-99}.

Our emphasis on efficiency might come as a surprise, since type inference for
\MLtype is known to be
\textsc{dexptime}-complete \cite{Kfoury90ml,mairson-kanellakis-mitchell-91}. In
practice, however, most implementations of it behave well. This apparent
contradiction may be explained by observing that types usually remain small
and that \kwlet constructs are never deeply nested towards the left. Indeed,
under the assumption that types have bounded size and that programs have
bounded ``scheme depth,'' type inference may be performed in quasi-linear
time \cite{mcallester-03}. In \MLlang, algebraic data type definitions allow
complex data structures to be described by concise expressions, such as
``$\tlist\tvar$,'' which helps achieve the bounded-type-size property.

In fact, in such favorable circumstances, even an inefficient algorithm may
behave well. For instance, some deployed implementations of type inference for
\MLtype contain sources of inefficiency (see
remark~\ref{remark-ml-inefficiency} on
page~\pageref{remark-ml-inefficiency}) and do not operate in quasi-linear time
under the bounded-type-size assumption. However, such implementations are put
under greater stress when types become larger, a situation that occurs in some
programs \cite{saha-al-98} and also arises when large, transparent type
expressions are used instead of algebraic data types, as in Objective Caml's
object-oriented fragment \cite{RemyVouillon97}.

% \cite{sansom-93} mentionne les bénéfices (frappants) de l'unification
% destructive dans ghc.

For these reasons, we believe it is worth giving an account of \MLtype that
focuses on \emph{type inference} and strives to be at once {elegant} and
{faithful} to an efficient implementation, such as
Rémy's \citeyr{Remy!mleth}. In this presentation, we forego type substitutions
and instead put emphasis on {constraints}, which offer a number of
advantages.

First, constraints allow a modular presentation of type inference as the
combination of a constraint generator and a constraint solver, allowing
separate reasoning about \emph{when} a program is
correct and \emph{how} to check whether it is correct. This perspective has
long been standard in the setting of 
the \stlc: see, for example, \longcite{wand87b} and \TAPLCHAPTER{22}. In the setting of \MLtype, such a
decomposition is provided by the reduction of typability problems to acyclic
semi-unification problems \cite{Henglein93, JACM::KfouryTU1994}.
% NOTE On pourrait citer aussi henglein-phd-89.
This approach, however, was apparently never used in production
implementations of \MLlang. An experimental extension of SML/NJ with
polymorphic recursion \cite{emms-leiss-96} did reduce type inference to a
semi-unification problem. Semi-unification found applications in the closely
related area of program analysis; see, for example,
\fulllongcite{faehndrich-al-00} and \longcite{Birkedal-Tofte:2001}. In \this, we
give a 
constraint-based description of a ``classic'' implementation of \MLtype, which
is based on first-order unification and a mechanism for creating and
instantiating principal \emph{type schemes}.

Second, it is often natural to define and implement the solver as a constraint
rewriting system. The constraint language allows reasoning not only
about correctness---is every rewriting step meaning-preserving?---but also \linebreak
about low-level implementation details, since constraints {are} the data
structures manipulated throughout the type inference process.  For instance,
describing unification in terms of \emph{multi-equations} allows reasoning
about the sharing of nodes in memory, which a substitution-based approach
cannot account for. Last, constraints are more general than type
substitutions, and allow smooth extensions of \MLtype with recursive types,
rows, subtyping, \iffull{first-order unification under a mixed prefix, }and
more. These arguments are developed, for example, in \longcite{jouannaud-kirchner-91}.

Before delving into the details of this new presentation of \MLtype,
it is worth recalling its standard definition. Thus, in what follows, we first
define the syntax and operational semantics of
\MLcalc, and equip it with a type system, known as \emph{{Damas and Milner's
  type system}}.

% -----------------------------------------------------------------------------

\subsection*{ML-the-Calculus}

\begin{TTCOMPONENT}{Syntax of \MLcalc}{}
\ttlabel{MLsyntax}
\def\\{\unskip\TTSyntaxAlternative}

\TTSyntaxCategoryNamed{\evid,\ewid}{}{Identifiers}
\\ {\evar}                     {Variable}
\\ {\eloc}                     {Memory location} 
\\ {\econ}                     {Constant}

\TTSyntaxCategoryNamed{\et}{}  {Expressions}
\\ {\evid}                     {Identifier}
\\ {\efun \evar \et}           {Function}
\\ {\eapp \et \et}             {Application}
\\ {\elet\evar\et\et}          {Local definition}


\TTSyntaxCategoryNamed{\ev,\ew}{}  {Values} 
\\ {\evar}               {Variable}
\\ {\eloc}                     {Memory location} 
\\ {\efun \evar \et}           {Function}
\\ {\eapp[3]\econ{\ev_1}\ldots{\ev_k}} {Data}
\\ {}                          {$\econ\in\econset^+ \wedge k\leq\arity\econ$}
\\ {\eapp[3]\econ{\ev_1}\ldots{\ev_k}} {Partial application}
\\ {}                          {$\econ\in\econset^- \wedge k<\arity\econ$}

\TTSyntaxCategoryNamed{\ec}{}  {Evaluation Contexts} 
\nobreak
\\ {\ehole}                    {Empty context}
\\ {\eapp \ec \et}             {Left side of an application} 
\\ {\eapp \ev \ec}             {Right side of an application} 
\\ {\elet \evar \ec \et}       {Local definition} 

\extraspacehack{.07in}
\end{TTCOMPONENT}

The syntax of \MLcalc is defined in Figure~\ref{fig:MLsyntax}. It is made up
of several syntactic categories.

\emph{Identifiers} group several kinds of names that may be referenced in a
program: variables, memory locations, and constants. We let $\evid$ and
$\ewid$ range over identifiers.  \emph{Variables}---also called
\emph{program variables}, to avoid ambiguity---are names that may be bound to
values using $\lambda$ or \kwlet binding forms; in other words, they are names
for function parameters or local definitions. We let $\evar$ and $\efar$ range
over program variables. We sometimes write~$\wildpat$ for a program variable
that does not occur free within its scope: for instance, $\efun\wildpat\et$
stands for $\efun\evar\et$, provided $\evar$ is fresh for $\et$. \ifshort{(We say that
$\evar$ is \emph{fresh for} $\et$ when $\evar$ does not occur free in $\et$.)} \emph{Memory
locations} are names that represent memory addresses. They are used to model
references (see Example~\ref{example-refs} below). Memory
locations never appear in \emph{source programs}, that is, programs that are
submitted to a compiler. They only appear during execution, when new memory
blocks are allocated. \emph{Constants} are fixed names for primitive values
and operations, such as integer literals and integer arithmetic operations.
Constants are elements of a finite or infinite set
$\econset$.\iffull{\indexsym {\econset}{set of constants}}
They are never 
subject to \aconv, in contrast to variables and memory locations.
Program variables, memory locations, and constants belong
to distinct syntactic classes and may never be confused.

The set of constants $\econset$ is kept abstract, so most of our development
is independent of its concrete definition. We assume that every constant
$\econ$ has a nonnegative integer \emph{arity}\iffull{\index{arity!expression}}
$\arity\econ$. We further
assume that $\econset$ is partitioned into subsets of \emph{constructors}
$\econstructors$ and \emph{destructors} $\edestructors$. Constructors and
destructors differ in that the former are used to \emph{form} values, while
the latter are used to \emph{operate} on values.
%
\begin{example}[Integers]
\label{example-integer-literals}
For every integer $n$, one may introduce a nullary constructor $\hat n$. In
addition, one may introduce a binary destructor $\hat+$, whose applications
are written infix, so $\et_1\mathbin{\hat+}\et_2$ stands for the double
application $\eapp[2]{\hat+}{\et_1}{\et_2}$ of the destructor $\hat+$ to the
expressions $\et_1$ and $\et_2$.
\end{example}

\emph{Expressions}---also known as \emph{terms} or
\emph{programs}---are the main syntactic category. Indeed, unlike procedural
languages such as C and Java, functional languages, including \MLlang,
suppress the distinction between expressions and statements. Expressions
consist of identifiers, \emph{$\lambda$-abstractions}, \emph{applications}, and local
definitions. The {$\lambda$-abstraction} $\efun\evar\et$ represents the
function of one parameter named $\evar$ whose result is the expression~$\et$,
or, in other words, the function that maps $\evar$ to $\et$. Note that the
variable $\evar$ is bound within the term $\et$, so (for instance) the notations
$\efun{\evar_1}{\evar_1}$ and $\efun{\evar_2}{\evar_2}$ denote the same entity.
The {application} $\eapp{\et_1}{\et_2}$ represents the result of calling
the function $\et_1$ with actual parameter $\et_2$, or, in other words, the
result of applying $\et_1$ to~$\et_2$.  Application is left-associative,
that is, $\eapp[2] {\et_1} {\et_2} {\et_3}$ stands for
$\eapp {(\eapp {\et_1} {\et_2})} {\et_3}$. 
The construct
$\elet\evar{\et_1}{\et_2}$ represents the result of evaluating $\et_2$ after
binding the variable $\evar$ to $\et_1$.  Note that the variable $\evar$ is
bound within $\et_2$, but not within $\et_1$, so for instance
$\elet{\evar_1}{\evar_1}{\evar_1}$ and $\elet{\evar_2}{\evar_1}{\evar_2}$ are
the same object. The construct $\elet\evar{\et_1}{\et_2}$ has the same
meaning as $\eapp{(\efun\evar{\et_2})}{\et_1}$, but is dealt with in a more
flexible way by \MLtype. To sum up, the syntax of \MLcalc is that of the pure
\lc, extended with memory locations, constants, and the \kwlet construct.

\emph{Values} form a subset of expressions. They are expressions whose
evaluation is completed. Values include identifiers, $\lambda$-abstractions,
and applications of constants, of the form $\eapp[3] \econ {\ev_1} \ldots
{\ev_k}$, where $k$ does not exceed $\econ$'s arity if $\econ$ is a
constructor, and $k$ is smaller than $\econ$'s arity if $\econ$ is a
destructor. In what follows, we are often interested in closed values---ones that
do not contain any free program variables. We
use  the meta-variables $\ev$ and $\ew$ for values.
%
\begin{example}
\label{example-integer-literals-continued}
The integer literals $\ldots, \widehat{-1}, \hat0, \hat1, \ldots$ are nullary
constructors, so they are values. Integer addition $\hat+$ is a binary
destructor, so it is a value, and so is every partial application
$\eapp{\hat+}{\ev}$.  Thus, both $\eapp {\hat+}{\hat 1}$ and $\eapp
{\hat+}{\hat+}$ are values. An application of $\hat+$ to two values, such as
$\hat2\hat+\hat2$, is not a value.
\end{example}
%
\begin{example}[Pairs]
\label{example-pair-constructor}
Let $(\cdot,\cdot)$ be a binary constructor. If $\et_1$ are $\et_2$ are
expressions, then the double application
$\eapp[2]{(\cdot,\cdot)}{\et_1}{\et_2}$ may be called the \emph{pair} of
$\et_1$ and $\et_2$, and may be written $(\et_1, \et_2)$. By the definition
above, $(\et_1, \et_2)$ is a value if and only if $\et_1$ and $\et_2$ are both
values.
\end{example}

\emph{Stores} are finite mappings from memory locations to closed values. A
store $\es$ represents what is usually called a heap, that is, a collection of
values, each of which is allocated at a particular address in memory
and may contain pointers to other elements of the heap. \MLlang allows
overwriting the contents of an existing memory block---an operation sometimes
referred to as a \emph{side effect}. In the operational semantics, this effect
is achieved by mapping an existing memory location to a new value.  We write
$\nostore$ for the empty store. We write $\etend\es\eloc\ev$ for the store
that maps $\eloc$ to $\ev$ and otherwise coincides with $\es$. When $\es$ and
$\es'$ have disjoint domains, we write $\es\es'$ for their union. We write
$\Dom\es$ for the domain of $\es$ and $\Range\es$ for the set of memory
locations that appear in its codomain.

The operational semantics of a pure language like the 
\lc{} may be defined as a rewriting system on expressions. Because \MLcalc has
side effects, however, we define its operational semantics as a rewriting
system on \emph{configurations}. A {configuration} $\et/\es$ is a pair of
an expression $\et$ and a store $\es$. The memory locations in the domain of
$\es$ are \emph{not} considered bound within $\et/\es$, so, for instance,
$\eloc_1/(\store{\eloc_1}{\hat0})$ and $\eloc_2/(\store{\eloc_2}{\hat0})$
denote {distinct} entities. (In the literature, memory locations are
often considered bound inside configurations. This offers the advantage of
making memory allocation a deterministic operation. However, there is still a
need for non-$\alpha$-convertible configurations: rules \Rule{R-Extend} and
\Rule{R-Context} in Figure~\ref{fig:MLsemantics} cannot otherwise be correctly
stated! Quite a few papers run into this pitfall.)%
% TEMPORARY possible que certaines preuves ne soient plus à jour suite à ce
% changement: 
% les configurations ne sont plus alpha-convertibles.
% \footnote {In fact, most implementations also provide a means to reveal
% addresses of memory locations as integers for debugging purposes.  In
% combination with the usual allocation schema that nondeterministically pick
% a free memory location, this results in a truely nondetermisnistic
% semantics.  Such an operation may break a form of abstraction but does not
% break type soundness. Thus, there is not reason to exclude it a priori.  Of
% course, one may choose to disallow such nondeterminism and treat all
% memory locations equivalently: one could easily show that if
% $\reduces[\delta]$ is stable by (global) renaming of memory locations, so
% will be the relations $\reduces$ and $\topreduces$.
% }

A configuration $\et/\es$ is \emph{closed} if and only if $\et$ has no free
program variables and every memory location that appears within $\et$ or
within the range of $\es$ is in the domain of $\es$. If $\et$ is a closed
source program, its evaluation begins within an empty store---that is, with
the configuration $\et/\nostore$. Because source programs do not contain
memory locations, this configuration is closed. Furthermore, we shall see that
closed configurations are preserved by reduction.

Note that, instead of separating expressions and stores, it is possible to
make store fragments part of the syntax of expressions; this idea, proposed
in \longcite{crank-felleisen-91}, has also been used for
the encoding of reference cells in process calculi.

A \emph{context}\iffull{\index{context!evaluation}\index{evaluation context}} is an expression where a single
subexpression has been 
replaced with a \emph{hole}, written $\ehole$. \emph{Evaluation contexts} form
a strict subset of contexts. In an evaluation context, the hole is meant to
highlight a point in the program where it is valid to apply a reduction rule.
Thus, the definition of evaluation contexts determines a reduction strategy:
it tells where and in what order reduction steps may occur. For instance, the
fact that $\efun\evar\ehole$ is not an evaluation context means that the body
of a function is never evaluated---that is, not until the function is applied.
The fact that $\eapp\et\ec$ is an evaluation context
only if $\et$ is a value means that, to evaluate an application
$\eapp{\et_1}{\et_2}$, one should fully evaluate $\et_1$ before attempting to
evaluate $\et_2$. More generally, in the case of a multiple application, it
means that arguments should be evaluated from left to right. Of course, other
choices could be made: for instance, defining $\ec ::= \ldots \mid \eapp\et\ec
\mid \eapp\ec\ev \mid \ldots$ would enforce a right-to-left evaluation order,
while defining $\ec ::= \ldots \mid \eapp\et\ec \mid \eapp\ec\et \mid \ldots$
would leave the evaluation order unspecified, effectively allowing reduction
to alternate between both subexpressions, and making evaluation
nondeterministic (because side effects could occur in different order). The
fact that $\elet\evar\ev\ec$ is not an evaluation 
context means that the body of a local definition is never evaluated---that
is, not until the definition itself is reduced. We
write $\ecin\ec\et$ for the expression obtained by replacing the hole in $\ec$
with the expression $\et$.

\begin{TTCOMPONENT}{Semantics of \MLcalc}{}
\ttlabel{MLsemantics}

\infax[R-Beta]{\eapp {(\efun \evar \et)} \ev \reduces \subst \evar \ev \et}

\vspace{5.5mm}

\infax[R-Let] 
    {\elet \evar \ev \et \reduces \subst \evar \ev \et}

\vspace{5mm}

\infrule[R-Delta]
    {\et/\es \reduces[\delta] \et'/\es'}
    {\et/\es \reduces \et'/\es'}

\infrule[R-Extend]
    {\et/\es \reduces \et'/\es' \\ \disjoint{\Dom {\es''}}{\Dom {\es'}} \\
     \disjoint{\Range{\es''}}{\Dom{\es'\setminus\es}}}
    {\et/\es\es'' \reduces \et'/\es'\es''}

\vspace{2mm}

\infrule[R-Context]
    {\et/\es \reduces \et'/\es'}
    {\cconf{\ecin\ec\et}\es \topreduces \cconf{\ecin\ec{\et'}}{\es'}}

% \extraspacehack{.07in}
\end{TTCOMPONENT}

Figure~\ref{fig:MLsemantics} defines first a relation $\reduces$ between
arbitrary configurations, then a relation $\topreduces$ between closed
configurations. If $\et/\es \reduces \et'/\es$ holds for every store $\es$,
then we write $\et \reduces \et'$ and say that the reduction is \emph{pure}.

\longpage

The semantics need not be deterministic. That is, a configuration may reduce
to two different configurations. In fact, our semantics is deterministic only
if the relation $\reduces[\delta]$, which is a parameter to our semantics, is
itself deterministic. In practice, $\reduces[\delta]$ is usually
deterministic, up to \aconv of memory locations. As explained above, the
semantics could also be made nondeterministic by a different choice in the
definition of evaluation contexts.

The key reduction rule is \Rule{R-Beta}, which states that a function
application $\eapp{(\efun \evar\et)}\ev$ reduces to the function body, namely
$\et$, where every occurrence of the formal argument $\evar$ has been replaced
with the actual argument $\ev$. The $\lambda$ construct, which prevented the
function body $\et$ from being evaluated, disappears, so the new term may (in
general) be further reduced. Because \MLcalc adopts a \emph{call-by-value}
strategy, rule \Rule{R-Beta} is applicable only if the actual argument is a
value $\ev$.  In other words, a function cannot be invoked until its actual
argument has been fully evaluated. Rule \Rule{R-Let} is very similar to
\Rule{R-Beta}. Indeed, it specifies that $\elet\evar\ev\et$ has the same
behavior, with respect to reduction, as
$\eapp{(\efun\evar\et)}\ev$. Substitution of a value for a program variable
throughout a term is 
expensive, so \Rule{R-Beta} and \Rule{R-Let} are never implemented literally:
they are only a simple {specification}. Actual implementations usually
employ \emph{runtime environments}, which may be understood as a form of
\emph{explicit substitutions} \cite{Abadi91:Explicit,
hardin-maranget-pagano-report}. Note that our 
choice of a call-by-value reduction strategy has
essentially no impact on the type system; the programming language
Haskell, whose reduction strategy is known as \emph{lazy} or
\emph{call-by-need}, also relies on the Hindley-Milner type discipline.

Rule \Rule{R-Delta} describes the semantics of constants. It states
that a certain relation $\reduces[\delta]$ is a subset of $\reduces$. Of
course, since the set of constants is unspecified, the relation
$\reduces[\delta]$ must be kept abstract as well. 
We require that, 
if $\et /
\es \reduces[\delta] \et'/ \es'$ holds, then 
\begin{itemize}
\item [\quad (i)] $\et$ is of the form
$\eapp[3]\econ{\ev_1}\ldots{\ev_n}$, where $\econ$ is a destructor of arity
$n$; and
\item [\quad (ii)] $\Dom{\es}$ is a subset of $\Dom {\es'}$. 
\end{itemize}
Condition (i) ensures that $\delta$-reduction concerns full applications of
destructors only, and that these are evaluated in accordance with the
call-by-value strategy.  Condition (ii) ensures that $\delta$-reduction may
allocate new memory locations, but not deallocate existing locations. In
particular, a ``garbage collection'' operator, which destroys unreachable
memory cells, cannot be made available as a constant. Doing so would not
make much sense anyway in the presence of \Rule{R-Extend}. Condition (ii)
allows 
proving that, if $\et / \es$ reduces (by $\reduces$) to $\et'/ \es'$, then
$\Dom\es$ is also a subset of $\Dom{\es'}$; checking this is left as an exercise to
the reader. 

Rule \Rule{R-Extend} states that any valid reduction is also valid in a larger
store. The initial and final stores $\es$ and $\es'$ in the original reduction
are both extended with a new store fragment $\es''$. The rule's second premise
requires that the domain of $\es''$ be disjoint with that of $\es'$ (and
consequently, also with that of $\es$), so that the new memory locations are
indeed undefined in the original reduction. (They may, however, appear in the
image of $\es$.) The last premise ensures that the new memory locations in
$\es''$ do not accidentally carry the same names as the locations allocated
during the original reduction step, that is, the locations in
$\Dom{\es'\setminus\es}$. \ifshort{The notation $\disjoint A B$ stands for $A
\cap B = \varnothing$.}

Rule \Rule{R-Context} completes the definition of the operational semantics by
defining $\topreduces$, a relation between closed configurations, in terms of
$\reduces$. The rule states that reduction may take place not only at the
term's root, but also deep inside it, provided the path from the root to the
point where reduction occurs forms an evaluation context. This is how
evaluation contexts determine an evaluation strategy. As a purely technical
point, because $\topreduces$ relates closed configurations only, we do not
need to require that the memory locations in $\Dom{\es'\setminus\es}$ be fresh
for $\ec$; indeed, every memory location that appears within $\ec$ must be a
member of $\Dom\es$.
%
\begin{example}[Integers, continued]
\label{example-integer-addition}
The operational semantics of integer addition may be defined as follows:
%
\infax[R-Add]{\hat k_1 \mathbin{\hat+}\hat k_2 \reduces[\delta]
\widehat{k_1+k_2}} 
%
The left-hand term is the double application $\eapp[2]{\hat+}{\hat k_1}{\hat
  k_2}$, while the right-hand term is the integer literal $\hat k$, where $k$
is the sum of $k_1$ and $k_2$. The distinction between object level and meta
level (that is, between $\hat k$ and $k$) is needed here to avoid ambiguity.
\end{example}
%
\begin{example}[Pairs, continued]
\label{example-pair-destructors}
In addition to the pair constructor defined in
Example~\ref{example-pair-constructor}, we may introduce two destructors
$\fst$ and $\snd$ of arity $1$. We may define their operational semantics as
follows, for $i\in\{1,2\}$:
%
\infax[R-Proj]{\eapp{\eproj{i}}{(\ev_1,\ev_2)} \reduces[\delta] \ev_i}
%
Thus, our treatment of constants is general enough to account for pair
construction and destruction; we need not build these features explicitly
into the language.
\end{example}
%
\begin{exercise}[Booleans, \Recommended, \Easy\fullsolution]
Let $\etrue$ and $\efalse$ be nullary constructors. Let $\eifc$ be a ternary
destructor. Extend the semantics with
%
\infax[R-True]{\eapp[3]\eifc\etrue{\ev_1}{\ev_2} \reduces[\delta] \ev_1}
\infax[R-False]{\eapp[3]\eifc\efalse{\ev_1}{\ev_2} \reduces[\delta] \ev_2}
%
Let us use the syntactic sugar $\eif{\et_0}{\et_1}{\et_2}$ for the triple
application of $\eapp[3]\eifc{\et_0}{\et_1}{\et_2}$. Explain why these
definitions do not quite provide the expected behavior. Without modifying the
semantics of $\eifc$, suggest a new definition of the syntactic sugar
$\eif{\et_0}{\et_1}{\et_2}$ that corrects the problem.
\iffull{\solref{conditional}}
\end{exercise}
%
\begin{example}[Sums]
\label{example-sums}
Booleans may in fact be viewed as a special case of the more general concept
of \emph{sum}. Let $\einj{1}$ and $\einj{2}$ be unary constructors, called
respectively \emph{left} and \emph{right injections}. Let $\ecase$ be a
ternary destructor, whose semantics is defined as follows, for $i\in\{1,2\}$:
%
\infax[R-Case]{
\eapp[3]\ecase{(\einj{i}\ev)}{\ev_1}{\ev_2} \reduces[\delta]
\eapp{\ev_i}\ev                             }
%
Here, the value $\einj{i}\ev$ is being scrutinized, while the values $\ev_1$
and~$\ev_2$, which are typically functions, represent the two arms of a
standard $\ecase$ construct. The rule selects an appropriate arm (here,
$\ev_i$) based on whether the value under scrutiny was formed using a left or
right injection. The arm $\ev_i$ is executed and given access to the data
carried by the injection (here, $\ev$).
\end{example}
%
\begin{exercise}[\QuickCheck, \nosolution]
Explain how to encode $\etrue$, $\efalse$, and the $\eifc$ construct in terms
of sums. Check that the behavior of \Rule{R-True} and \Rule{R-False} is
properly emulated.
\end{exercise}
%
\begin{example}[References]
\label{example-refs}
Let $\eref$ and $\ederef$ be unary destructors. Let $\eassign{}{}$ be a binary
destructor. We write $\eassign{\et_1}{\et_2}$ for the double application
$\eapp[2]{\mathord{:=}}{\et_1}{\et_2}$. Define the operational semantics
of these three destructors as follows:
%
\infax[R-Ref]{
\eref\ev/\nostore                       \reduces[\delta]
\eloc / (\store\eloc\ev)
\rlap{\quad\text{if $\eloc$ is fresh for $\ev$}}}

\infax[R-Deref]{
\ederef\eloc / (\store\eloc\ev)         \reduces[\delta]
\ev / (\store\eloc\ev)                  }

\infax[R-Assign]{
\eassign\eloc\ev / (\store\eloc{\ev_0}) \reduces[\delta]
\ev / (\store\eloc\ev)                  }
%
According to \Rule{R-Ref}, evaluating $\eref\ev$ allocates a fresh memory
location $\eloc$ and binds $\ev$ to it. The name $\eloc$ must be chosen fresh
for $\ev$ to prevent inadvertent capture of the memory locations that
appear free within $\ev$. By \Rule{R-Deref}, evaluating $\ederef\eloc$ returns
the value bound to the memory location $\eloc$ within the current store. By
\Rule{R-Assign}, evaluating $\eassign\eloc\ev$ discards the value $\ev_0$
currently bound to $\eloc$ and produces a new store where $\eloc$ is bound to
$\ev$. Here, the value returned by the assignment $\eassign\eloc\ev$ is $\ev$
itself; in \MLlang, it is usually a nullary constructor $\eunit$, pronounced
\emph{unit}.
\end{example}
%
\begin{example}[Recursion]
\label{example-fix}
Let $\efix$ be a binary destructor, whose operational semantics is:
%
\infax[R-Fix]{
\eapp[2]\efix{\ev_1}{\ev_2}                \reduces[\delta]
\eapp[2]{\ev_1}{(\eapp\efix{\ev_1})}{\ev_2}  }
%
$\efix$ is a fixpoint combinator: it effectively allows recursive definitions
of functions. Indeed, the construct $\eletrec\efar\evar{\et_1}{\et_2}$
provided by \MLlang may be viewed as syntactic sugar for
$\elet  \efar {\eapp \efix {(\efun\efar{\efun\evar{\et_1}})}}{\et_2}$.
\end{example}
%
\begin{exercise}[\Recommended, \Easy, \nosolution]
Assuming the availability of Booleans and conditionals, integer literals,
subtraction, multiplication, integer comparison, and a fixpoint combinator,
most of which were defined in previous examples, define a function that
computes the factorial of its integer argument, and apply it to $\hat3$.
Determine, step by step, how this expression reduces to a value.~\endpf
\end{exercise}

It is straightforward to check that, if $\et / \es$ reduces to $\et'/ \es'$,
then $\et$ is not a value. In other words, values are irreducible: they
represent completed computations.
\shortfull
{The proof is left as an exercise to the reader. }
{This fact is established by the next three lemmas.
%
\begin{lemma}
\label{lemma-value-irreducible}
If $\et / \es \reduces \et'/ \es'$ holds, then $\et$ is not a value.
\end{lemma}
%
\begin{Proof}
By structural induction on a derivation of $\et / \es \reduces \et'/ \es'$.
It is clear, by definition of values, that the left-hand sides of
\Rule{R-Beta} and \Rule{R-Let} are not 
values. For \Rule{R-Delta}, the left-hand side must be of the form
$\eapp[3]\econ{\ev_1}\ldots{\ev_n}$, where $\econ$ is a destructor of arity
$n$, so it is not a value. For \Rule{R-Extend}, the result follows from the
induction hypothesis.
\end{Proof}
%
\begin{lemma}
\label{lemma-econ-value}
If $\ecin\ec\et$ is a value, then $\et$ is a value.
\end{lemma}
%
\begin{Proof}
By induction on the structure of the evaluation context.

\proofcase $\ehole$. Immediate.

\proofcase $\eapp\ec{\et'}$. Assume $\eapp{\ecin\ec\et}{\et'}$ is a value.
Then, by the definition of values, it must be of the form
$\eapp[3]\econ{\ev_1}\ldots{\ev_k}$, where $k$ is bounded (strictly bounded,
if $\econ$ is a destructor) by $\econ$'s arity. So, $\ecin\ec\et$ is
$\eapp[3]\econ{\ev_1}\ldots{\ev_{k-1}}$, which is a value as well. The result
follows by the induction hypothesis.

\proofcase $\eapp\ev\ec$. Assume $\eapp\ev{\ecin\ec\et}$ is a value. Then, by
the definition of values, it must be of the form
$\eapp[3]\econ{\ev_1}\ldots{\ev_k}$. So, $\ecin\ec\et$ is $\ev_k$, that is, a
value. The result follows by the induction hypothesis.

\proofcase $\elet\evar\ec{\et'}$. Immediate, since
$\elet\evar{\ecin\ec\et}{\et'}$ cannot be a value.
\end{Proof}
%
\begin{lemma}
\label{lemma-value-irreducible-top}
If $\et / \es \topreduces \et'/ \es'$ holds, then $\et$ is not a value.
\end{lemma}
%
\begin{Proof}
By \Rule{R-Context}, Lemma~\ref{lemma-value-irreducible}, and the
contrapositive of Lemma~\ref{lemma-econ-value}.
\end{Proof}

}The converse, however, does not hold: if the closed configuration
$\cconf\et\es$ is irreducible with respect to $\topreduces$, then $\et$ is not
necessarily a value. In that case, the configuration $\cconf\et\es$ is said to
be \emph{stuck}. It represents a \emph{runtime error}, that is, a situation
that does not allow computation to proceed, yet is not considered a valid
outcome. A closed source program $\et$ is said to \emph{go wrong} if and only
if the initial configuration $\cconf\et\nostore$ reduces to a stuck
configuration.
%
\begin{example}
Runtime errors typically arise when destructors are applied to arguments of an
unexpected nature. For instance, the expressions $\eapp[2]{\hat +}{\hat 1}\eloc$ and
$\eapp\fst{\hat 2}$ and $\ederef{\hat 3}$ are stuck, regardless of the current store.
%
The program $\elet\evar{\eapp{\hat+}{\hat+}}{\eapp\evar{1}}$ is not stuck,
because $\eapp{\hat+}{\hat+}$ is a value. However, its reduct
through \Rule{R-Let} is $\eapp{\eapp{\hat+}{\hat+}}{1}$, which is
stuck, so this program goes wrong.
%
The primary purpose of type systems is to prevent such situations from
arising.
\end{example}
%
\begin{remark}
The configuration $\ederef\eloc/\es$ is stuck if $\eloc$ is not in the domain
of $\es$. In that case, however, $\ederef\eloc/\es$ is not closed. Because we
consider $\topreduces$ as a relation between closed configurations only, this
situation cannot arise. In other words, the semantics of \MLcalc never allows
the creation of \emph{dangling pointers}. As a result, no particular
precautions need be taken to guard against them. Several strongly typed
programming languages do nevertheless allow dangling pointers in a controlled
fashion \cite{IC::TofteT1997,Walker-Crary-Morrisett:2000,deline+:vault,
Grossman-al:2002}.
\end{remark}

% -----------------------------------------------------------------------------

\subsection*{Damas and Milner's Type System}

\index{Damas--Milner type system|(}

\MLtype was originally defined by \longcite{Milner78}. Here, we
reproduce the definition given a few years later by
\longcite{DAMAS82}, which is written in a more standard style: typing
judgments are defined inductively by a collection of typing rules. We
refer to this type system as \dm.

We must first define \emph{types}. In \dm, types are terms built out of
\emph{type constructors} and \emph{type variables}. Furthermore, they are
\emph{first-order} terms: that is, in the grammar of types, none of the
productions \emph{binds} a type variable. This situation is identical to that
of the \stlc.

We begin with several considerations concerning the specification of type
constructors.

First, we do not wish to fix the set of type constructors. Certainly, since
\MLcalc has functions, we need to be able to form an arrow type
$\ttyp\arw\ttyp'$ out of arbitrary types $\ttyp$ and $\ttyp'$; that is, we
need a binary type constructor $\arw$. However, because \MLcalc includes an
unspecified set of constants, we cannot say much else in general.
%
% We could say something about the type constructor \kwd{ref}, but it would
% perhaps be a bit early.
%
If constants include integer literals and integer operations, as in
Example~\ref{example-integer-literals}, then a nullary type constructor
$\tint$ is needed; if they include pair construction and destruction, as in
Examples~\ref{example-pair-constructor} and~\ref{example-pair-destructors},
then a binary type constructor $\times$ is needed; etc.

Second, it is common to refer to the parameters of a type constructor {by
position}, that is, by numeric index. For instance, when one writes
$\ttyp\arw\ttyp'$, it is understood that the type constructor $\arw$ has arity
2, that $\ttyp$ is its {first} parameter, known as its \emph{domain}, and
that $\ttyp'$ is its {second} parameter, known as its
\emph{codomain}. Here, however, we refer to parameters {by names}, known
as \emph{directions}. For instance, we define two directions $\dirdomain$ and
$\dircodomain$ and let the type constructor $\arw$ have arity $\{\dirdomain,
\dircodomain\}$. The extra generality afforded by directions is exploited in
the definition of nonstructural subtyping
(Example~\ref{example-subtyping-models}) and in the definition of rows
(\S\ref{section-rows}).

Last, we allow types to be classified using \emph{kinds}. As a result, every
type constructor must come not only with an arity, but with a richer
\emph{\tcsignature}, which describes the kinds of the types to which it is
applicable and the kind of the type that it produces. A distinguished kind
$\normalkind$ is associated with ``normal'' types, that is, types that are
directly ascribed to expressions and values. For instance, the {\tcsignature}
of the type constructor $\arw$ is $\{ \dirdomain\mapsto\normalkind,
\dircodomain\mapsto\normalkind \} \kindarrow\normalkind$, because it is
applicable to two normal types and produces a normal type. Introducing
kinds other than $\normalkind$ allows viewing some types as ill-formed:
this is illustrated, for instance, in \S\ref{section-rows}. In the
simplest case, however, $\normalkind$ is really the only kind, so the
{\tcsignature} of a type constructor is nothing but its arity (a set of
directions), and every term is a well-formed type, provided every application
of a type constructor respects its arity.
%
\begin{definition}
\label{def-signature}
Let $\dird$ range over a finite or denumerable set of \emph{directions} and
$\kind$ over a finite or denumerable set of \emph{kinds}. Let
$\normalkind$ be a distinguished kind. Let $\dk$ range over partial mappings
from directions to kinds.  Let $\tycon$ range over a finite or denumerable set
of \emph{type constructors}, each of which has a \emph{\tcsignature} of the
form $\dk\kindarrow\kind$. The domain of $\dk$ is called the
\emph{arity}\iffull{\index{arity!types}} of $\tycon$, while $\kind$ is referred to as
its \emph{image kind.} We write $\kind$ instead of $\dk\kindarrow\kind$ when
$\dk$ is empty. Let $\arw$ be a type constructor of {\tcsignature} $\{
\dirdomain\mapsto\normalkind, \dircodomain\mapsto\normalkind \}
\kindarrow\normalkind$.~\endpf
\end{definition}

The type constructors and their {\tcsignature s} collectively form a
\emph{signature} $\sig$. In the following, we assume that a fixed signature
$\sig$ is given and that every type constructor in it has \emph{finite} arity,
so as to ensure that types are machine representable. However, in
\S\ref{section-rows}, we shall explicitly work with several distinct
signatures, one of which involves type constructors of denumerable arity.

A \emph{type variable} is a name that is used to stand for a type. For
simplicity, we assume that every type variable is branded with a kind, or in
other words, that type variables of distinct kinds are drawn from disjoint
sets. Each of these sets of type variables is individually subject to \aconv:
that is, renamings must preserve kinds.  Attaching kinds to type variables is
only a technical convenience; in practice, every operation performed during
type inference preserves the property that every type is well-kinded, so it is
not necessary to keep track of the kind of every type variable. It is only
necessary to check that all types supplied by the programmer, within type
declarations, type annotations, or module interfaces, are well-kinded.
%
\begin{definition}
\label{def-type-variables}
For every kind $\kind$, let $\tyvarsetk{\kind}$ be a disjoint, denumerable set
of \emph{type variables}. Let $\tvar$, $\twar$, and $\tzar$ range over the set
$\tyvarset$\iffull{\indexsym{\tyvarset}{set of type variables}} of all type variables.
Let $\tvars$ and $\twars$ range over finite sets of type variables. We write
$\tvars\twars$ for the set $\tvars\cup\twars$ and often write $\tvar$ for the
singleton set $\{\tvar\}$. We write $\ftv{o}$%
\iffull{\indexsym{\protect\ftv\cdot}{free type variables}}
for the set of \emph{free type variables} of an object $o$.
\end{definition}

The set of types, ranged over by $\ttyp$%
\iffull{\indexsym{\ttyp}{type}}, is the 
free many-kinded term algebra that arises out of the type constructors and
type variables. Types are given by the following inductive definition:
%
\begin{definition}
\label{def-types}
A \emph{type}\iffull{\index{type}} of kind $\kind$ is either a member of $\tyvarsetk{\kind}$,
or a term of the form
$\tycon\,\{\dird_1\mapsto\ttyp_1,\ldots,\dird_n\mapsto\ttyp_n\}$, where
$\tycon$ has {\tcsignature}
$\{\dird_1\mapsto\kind_1,\ldots,\dird_n\mapsto\kind_n\}\kindarrow\kind$ and
$\ttyp_1, \ldots, \ttyp_n$ are types of kind $\kind_1, \ldots, \kind_n$,
respectively.
\end{definition}
%
% I am able to index everything by 1..n because every type constructor is
% assumed to have finite arity.

As a notational convention, we assume that, for every type constructor
$\tycon$, the directions that form the arity of $\tycon$ are implicitly
ordered, so that we may say that $\tycon$ has {\tcsignature}
$\kind_1\kindprod\ldots\kindprod\kind_n\kindarrow\kind$ and employ the syntax
$\tycon\,\ttyp_1\,\ldots\,\ttyp_n$ for applications of $\tycon$. Applications
of the type constructor $\arw$ are written infix and associate to the right,
so $\ttyp \arw \ttyp' \arw \ttyp''$ stands for $\ttyp \arw (\ttyp' \arw
\ttyp'')$.

% FIXME: expanser l'exemple de la fleche

In order to give meaning to the free type variables of a type, or more
generally, of a typing judgment, traditional presentations of \MLtype,
including Damas and Milner's, employ \emph{type substitutions}. Most of our
presentation avoids substitutions and uses {constraints}
instead. However, we do need substitutions on a few occasions, especially when
relating our presentation to Damas and Milner's.
%
\begin{definition}
\label{def-type-substitution}
A \emph{type substitution}\iffull{\index{substitution!type}}
 $\unifier$ \iffull{(sometimes $\refiner$) }is a total, kind-preserving mapping of
type variables to types that is the identity everywhere but on a finite subset
of $\tyvarset$, which we call the \emph{domain} of $\unifier$ and write
$\Dom\unifier$. The \emph{range} of $\unifier$, which we write
$\Range\unifier$, is the set $\ftv{\unifier(\Dom\unifier)}$. A type
substitution may naturally be viewed as a total, kind-preserving mapping of
types to types.
\begin{full}
We write $\disjoint\tvars\unifier$ for
$\disjoint\tvars{(\Dom\unifier\cup\Range\unifier)}$. We write
$\corestrict\unifier\tvars$ for the restriction of $\unifier$ outside
$\tvars$, that is, the restriction of $\theta$ to $\tyvarset \setminus
\tvars$.
\end{full}
\end{definition}

If $\tvarc$ and $\ttypc$ are respectively a vector of {distinct} type
variables and a vector of types of the same (finite) length such that, for
every index $i$, $\tvar_i$ and $\ttyp_i$ have the same kind, then
$\subst\tvarc\ttypc$ denotes the substitution that maps $\tvar_i$ to $\ttyp_i$
for every index $i$ and is the identity elsewhere. The domain of
$\subst\tvarc\ttypc$ is a subset of $\tvars$, the set underlying the vector
$\tvarc$. Its range is a subset of $\ftv\ttyps$, where $\ttyps$ is the set
underlying the vector $\ttypc$.  (These may be {strict} subsets; for
instance, the domain of $\subst\tvar\tvar$ is the empty set, since this
substitution is the identity.) Every substitution $\unifier$ may be written
under the form $\subst\tvarc\ttypc$, where $\tvars=\Dom\unifier$. Then,
$\unifier$ is \emph{idempotent} if and only if $\disjoint\tvars{\ftv\ttyps}$
holds.

As pointed out earlier, types are first-order terms. As a result, every type
variable that appears within a type $\ttyp$ appears \emph{free} within
$\ttyp$. Things become more interesting when we introduce \emph{type
schemes}. As its name implies, a type scheme may describe an entire family of
types; this effect is achieved via \emph{universal quantification} over a set
of type variables.
%
\begin{definition}
\label{def-type-scheme}
\index{type scheme}
\index{scheme|see{type scheme}}
#>A type scheme $\dms$\iffull{\index{ $\protect \dms$@S|see{type scheme}}} is an 
object of the form $\dmscheme\tvars\ttyp$, where 
$\ttyp$ is a type of kind $\normalkind$ and the type variables $\tvars$ are
considered bound within $\ttyp$. Any type of the form $\subst\tvarc\ttypc\ttyp$
is called an \iffull{\index{instance of!type scheme}}\emph{instance} of the type scheme
$\dmscheme\tvars\ttyp$.
\end{definition}

One may view the type $\ttyp$ as the trivial type scheme
$\dmscheme\varnothing\ttyp$, where no type variables are universally
quantified, so types of kind $\normalkind$ may be viewed as a subset of type
schemes. The type scheme $\dmscheme\tvars\ttyp$ may be viewed as a finite way
of describing the possibly infinite family of its instances. Note that,
throughout most of this chapter, we work with \emph{constrained type schemes},
a generalization of \dm type schemes (Definition~\ref{def-constraints}).

\emph{Typing environments}, or \emph{environments} for short, are used to collect
assumptions about an expression's free identifiers.
%
\begin{definition}
\label{def-environment}
\iffull{\index{environment}} An \emph{environment}
$\env$\iffull{\indexsym{\env}{typing environment}} is a finite ordered
sequence of 
pairs of a program identifier and a type scheme.  We write
$\noenv$\iffull{\indexsym{\noenv}{typing environment (empty)}} for the empty
environment and 
``$;$'' for the concatenation of environments.  An environment may be viewed
as a finite mapping from program identifiers to type schemes by letting
$\env(\evid)=\dms$ if and only if $\env$ is of the form
$\env_1;\evid:\dms;\env_2$, where $\env_2$ contains no assumption about
$\evid$. The set of \emph{defined program identifiers} of an environment
$\env$, written $\dpv\env$\iffull{\indexsym{\protect\dpv\cdot}{defined program identifiers}}, is defined by $\dpv\noenv=\varnothing$ and
$\dpv{\env;\evid:\dms}=\dpv\env\cup\{\evid\}$.
\end{definition}

\index{generalization of a type scheme|(}
\index{instantiation of a type scheme|(}
\index{type scheme|(}

\begin{TTCOMPONENT}{Typing rules for \dm}{}
\ttlabel{Damas-Milner}

\infrule[dm-Var]
  {\env(\evid)=\dms}
  {\dmj\env\evid\dms}

\infrule[dm-Abs]
  {\dmj{\env;\evar:\ttyp}\et{\ttyp'}}
  {\dmj\env{\efun\evar\et}{\ttyp\arw\ttyp'}}

\infrule[dm-App]
  {\dmj\env{\et_1}{\ttyp\arw\ttyp'} \andalso
   \dmj\env{\et_2}\ttyp}
  {\dmj\env{\eapp{\et_1}{\et_2}}{\ttyp'}}

\infrule[dm-Let]
  {\dmj\env{\et_1}\dms \andalso
   \dmj{\env;\evar:\dms}{\et_2}\ttyp}
  {\dmj\env{\elet\evar{\et_1}{\et_2}}\ttyp}

\infrule[dm-Gen]
  {\dmj\env\et\ttyp \andalso
   \disjoint\tvars{\ftv\env}}
  {\dmj\env\et{\dmscheme\tvars\ttyp}}

\infrule[dm-Inst]
  {\dmj\env\et{\dmscheme\tvars\ttyp}}
  {\dmj\env\et{\subst\tvarc\ttypc\ttyp}}

% \extraspacehack{.07in}
\end{TTCOMPONENT}

To complete the definition of Damas and Milner's type system, there remains to
define \emph{typing judgments}. A typing judgment takes the form
$\dmj\env\et\dms$, where $\et$ is an expression of interest, $\env$ is an
environment, which typically contains assumptions about $\et$'s free program
identifiers, and $\dms$ is a type scheme. Such a judgment may be read:
\emph{under assumptions $\env$, the expression $\et$ has the type scheme
$\dms$}. By abuse of language, it is sometimes said that $\et$ \emph{has type
$\dms$}. A typing judgment is \emph{valid} (or \emph{holds}) if and only if
it may be derived using the rules that appear in
Figure~\ref{fig:Damas-Milner}. An expression~$\et$ is \emph{well-typed} within
the environment $\env$ if and only if there exists some type scheme $\dms$ 
such that the judgment $\dmj\env\et\dms$ holds; it is \emph{ill-typed}
within $\env$ otherwise.  

Rule \Rule{dm-Var} allows fetching a type scheme for an identifier $\evid$
from the environment. It is equally applicable to program variables, memory
locations, and constants. If no assumption concerning $\evid$ appears in the
environment $\env$, then the rule isn't applicable. In that case, the
expression $\evid$ is ill-typed within~$\env$. Assumptions about constants
are usually collected in a so-called \emph{initial environment} $\ienv$. It
is the environment under which closed programs are typechecked, so every
subexpression is typechecked under some extension $\env$ of $\ienv$. Of
course, the type schemes assigned by $\ienv$ to constants must be consistent
with their operational semantics; we say more about this later
(\S\ref{sec.soundness}). Rule \Rule{dm-Abs} specifies how to typecheck a
$\lambda$-abstraction $\efun\evar\et$. Its premise requires the body of the
function, $\et$, to be well-typed under an extra assumption that
causes all free occurrences of $\evar$ within $\et$ to receive a common type
$\ttyp$. Its conclusion forms the arrow type $\ttyp\arw\ttyp'$ out of the
types of the function's formal parameter, $\ttyp$, and result, 
$\ttyp'$. It is worth noting that this rule always augments the environment
with a type $\ttyp$---recall that, by convention, types form a subset of
type schemes---but never with a nontrivial type scheme.  Rule \Rule{dm-App}
states that the type of a function application is the codomain of the
function's type, provided that the domain of the function's type is a valid
type for the actual argument. Rule \Rule{dm-Let} closely mirrors the
operational semantics: whereas the semantics of the local definition
$\elet\evar{\et_1}{\et_2}$ is to augment the {runtime} environment by
binding $\evar$ to the {value} of $\et_1$ prior to evaluating $\et_2$,
the effect of \Rule{dm-Let} is to augment the {typing} environment by
binding $\evar$ to a {type scheme} for $\et_1$ prior to typechecking
$\et_2$. Rule \Rule{dm-Gen} turns a type into a type scheme by universally
quantifying over a set of type variables that do not appear free in the
environment; this restriction is discussed in Example~\ref{example-gen}
below.  Rule \Rule{dm-Inst}, on the contrary, turns a type scheme into one
of its instances, which may be chosen arbitrarily. These two operations are
referred to as \emph{generalization} and \emph{instantiation}.  The notion
of type scheme and the rules \Rule{dm-Gen} and \Rule{dm-Inst} are
characteristic of
\MLtype: they distinguish it from the \stlc.
\index{generalization of a type scheme|)}
\index{instantiation of a type scheme|)}
\index{type scheme|)}
%
\begin{example}
\label{example-gen}
\demoreset It is unsound to allow generalizing type variables that appear free
in the environment. For instance, consider the typing judgment
$\dmj{\evar:\tvar}\evar\tvar$~\dlabel{j}, which, according to \Rule{dm-Var},
is valid.  Applying an unrestricted version of \Rule{dm-Gen} to it, we obtain
$\dmj{\evar:\tvar}\evar{\dmscheme\tvar\tvar}$~\dlabel{k}, whence, by
\Rule{dm-Inst}, $\dmj{\evar:\tvar}\evar\twar$~\dlabel{l}. By \Rule{dm-Abs} and
\Rule{dm-Gen}, we then have
$\dmj\noenv{\efun\evar\evar}{\dmscheme{\tvar\twar}{\tvar\arw\twar}}$. In other
words, the identity function has unrelated argument and result types!  Then,
the expression $\eapp[2]{(\efun\evar\evar)}{\hat0}{\hat0}$, which reduces to
the stuck expression $\eapp{\hat0}{\hat0}$, has type scheme
$\dmscheme\tzar\tzar$. So, well-typed programs may cause runtime errors: the
type system is unsound.

What happened? It is clear that the judgment \dref{j} is correct only because
the type assigned to $\evar$ is the \emph{same} in its assumption and in its
right-hand side. For the same reason, the judgments \dref{k} and
\dref{l}---the former of which may be written
$\dmj{\evar:\tvar}\evar{\dmscheme\twar\twar}$---are incorrect.  Indeed, such
judgments defeat the very purpose of environments, since they disregard their
assumption. By universally quantifying over $\tvar$ {in the right-hand
  side only}, we break the connection between occurrences of $\tvar$ in the
assumption, which remain free, and occurrences in the right-hand side, which
become bound. This is correct only if there are in fact no free occurrences
of $\tvar$ in the assumption.
%
% The judgment $\dmj{\evar:\ttyp}\evar\ttyp$, which is a substitution instance
% of \dref{j}, is correct. In fact, it is valid to view all of a judgment's
% free type variables as (implicitly) externally universally quantified: that
% is, \dref{j} may informally be understood as
% $\forall\tvar\quad\dmj{\evar:\tvar}\evar\tvar$.  Furthermore, since the
% environment $\env$ plays the role of an assumption, a typing judgment
% $\dmj\env\et\dms$ may informally be understood as the universally quantified
% implication $\forall\tyvarset\quad\env \Rightarrow (\et:\dms)$. It is a
% well-known law of logic that universal quantification commutes with
% implication provided the quantified variables do not appear free in the
% assumption. Thus, the informal formula above is equivalent to
% $\forall\tyvarset\quad\env \Rightarrow \forall\tvars.(\et:\dms)$, where
% $\tvars$ is any set of type variables that do not appear free in $\env$. This
% explains, in an informal way, the meaning of \Rule{dm-Gen}.
\end{example}
%
\begin{remark}
\label{remark-ml-inefficiency}
A naive implementation of \Rule{dm-Gen} would traverse the environment $\env$
in order to compute the set of its free type variables. However, the number of
entries in $\env$ may be linear in the program size, so, even if types have
bounded size, the time required by this computation may be linear in the
program size. Since it is performed at every \kwlet node, this naive approach
gives type inference \emph{quadratic} time complexity. To avoid this pitfall,
our constraint solver annotates every type variable with an integer rank,
which allows telling, in constant time, whether it appears free in $\env$
(page~\pageref{page-ranks}).
\end{remark}

It is a key feature of \MLtype that \Rule{dm-Abs} may only introduce a type
$\ttyp$, rather than a type scheme, into the environment. Indeed, this allows
the rule's conclusion to form the arrow type $\ttyp\arw\ttyp'$. If instead the
rule were to introduce the assumption $\evar:\dms$ into the environment, then
its conclusion would have to form $\dms\arw\ttyp'$, which is not a well-formed
type. In other words, this restriction is necessary to preserve the
stratification between types and type schemes. If we were to remove this
stratification, thus allowing universal quantifiers to appear deep inside
types, we would obtain an implicitly-typed version of
System~F~(\TAPLCHAPTER{23}). Type inference for System~F is
undecidable \cite{wells-99}, while type inference for \MLtype is decidable, as
we show later, so this design choice has a rather drastic impact.
%
\begin{exercise}[\Recommended, \QuickCheck]
Build a type derivation for the expression
$\efun{\evar_1}{\elet{\evar_2}{\evar_1}{\evar_2}}$.
\solref{exdm}
\end{exercise}
%
\begin{exercise}[\Recommended, \QuickCheck]
% NOTE Remarque de Vincent : cet environnement initial est infini, ce qui ne rentre pas
% exactement dans nos définitions. Il est néanmoins finiment représentable, ce qui est
% l'essentiel, car les environnements font partie de la syntaxe des contraintes. On
% pourrait souhaiter détailler ce point.
Let $\tint$ be a nullary type constructor of {\tcsignature} $\normalkind$.
Let $\ienv$ consist of the bindings $\hat+ : \tint\arw\tint\arw\tint$ and
$\hat{k} : \tint$, for every integer $k$. Can you find derivations of the
following valid typing judgments? Which of these judgments are valid in
the \stlc, where $\elet\evar{\et_1}{\et_2}$ is syntactic sugar for
$\eapp{(\efun\evar{\et_2})}{\et_1}$?
$$\begin{array}{c}
\dmj\ienv{\efun\evar\evar}{\tint\arw\tint} \\
\dmj\ienv{\efun\evar\evar}{\dmscheme\tvar{\tvar\arw\tvar}} \\
\dmj\ienv{\elet\efar{\efun\evar{\evar\hat+\hat1}}{\eapp\efar{\hat2}}}\tint \\
\dmj\ienv{\elet\efar{\efun\evar\evar}{\eapp[2]\efar\efar{\hat2}}}\tint
\end{array}$$
Show that the expressions $\eapp{\hat1}{\hat2}$ and
$\efun\efar{(\eapp\efar\efar)}$ are ill-typed within $\ienv$. Could these
expressions be well-typed in a more powerful type system?
\solref{mlexamples}
\end{exercise}
%
\begin{full}
% En version longue uniquement, car ce n'est pas si facile, et d'un intérêt limité pour les non-experts.
\begin{exercise}[\Moderate]
In fact, the rules shown in Figure~\ref{fig:Damas-Milner} are not exactly
Damas and Milner's original rules. In \longcite{DAMAS82}, the
generalization and instantiation rules are:
%
\infrule[dm-Gen']
  {\dmj\env\et\dms \andalso
   \tvar\not\in\ftv\env}
  {\dmj\env\et{\dmscheme\tvar\dms}}
%
\infrule[dm-Inst']
  {\dmj\env\et{\dmscheme\tvars\ttyp} \andalso
   \disjoint\twars{\ftv{\dmscheme\tvars\ttyp}}}
  {\dmj\env\et{\dmscheme\twars{\subst\tvarc\ttypc\ttyp}}}
%
where $\dmscheme\tvar\dms$ stands for $\dmscheme{\tvar\tvars}\ttyp$ if
$\dms$ stands for $\dmscheme\tvars\ttyp$.
Show that the combination of \Rule{dm-Gen'} and \Rule{dm-Inst'} is
equivalent to the combination of \Rule{dm-Gen} and \Rule{dm-Inst}.
\solref{diffdm}
\end{exercise}
\end{full}

% NOTE Aspects that we might wish to discuss:
% A program has several types.
% The typing rule for `let' emulates textual expansion.

\dm enjoys a number of nice theoretical properties, which have practical
implications.

First, it is sound: that is, {well-typed programs do not go wrong}. This
essential property ensures that programs that are accepted by the typechecker
may be compiled without runtime checks. Establishing this property requires
(i) suitable hypotheses about the semantics of constants and the type schemes
assigned to constants in the initial environment, and (ii) in the presence of
side effects, a slight restriction of the syntax of \kwlet constructs, known
as the \emph{value restriction}.

\index{principal type schemes}%
Furthermore, there exists an algorithm that, given a (closed) environment
$\env$ and a program $\et$, tells whether $\et$ is well-typed with respect to
$\env$, and if so, produces a \emph{principal} type scheme $\dms$.  A
principal type scheme is such that (i) it is valid, that is, $\dmj\env\et\dms$
holds, and (ii) it is most general, that is, every judgment of the form
$\dmj\env\et{\dms'}$ follows from $\dmj\env\et\dms$ by \Rule{dm-Inst} and
\Rule{dm-Gen}. (For the sake of simplicity, we have stated the properties of
the type inference algorithm only in the case of a closed environment $\env$;
the specification is slightly heavier in the general case.)
% NOTE On pourrait préciser ce qui se passe dans le cas général, et en quoi l'énoncé général pourrait être utile.
This implies that {type inference is decidable}: the compiler need not
require expressions to be annotated with types. The fact that, under a fixed
environment $\env$, all of the type information associated with an expression
$\et$ may be summarized in the form of a single, principal type scheme is also
key to modular programming. Indeed, exporting a value out of a module requires
explicitly assigning a type scheme to it as part of the module's signature.
If the chosen type scheme is not principal, then part of the value's (hence,
of the module's) potential for reuse is lost.

\index{Damas--Milner type system|)}

%% TEMPORARY (long version): explain principal typings and cite wells-02?

\subsection*{Road Map}

Before proving the above claims, we first generalize our presentation by
moving to a \emph{constraint-based} setting. The necessary tools---the
constraint language, its interpretation, and a number of constraint
equivalence laws---are introduced in \S\ref{section-constraints}. In
\S\ref{section-hmx}, we describe the standard constraint-based type
system
\hmx \cite{Sulzmann&97}.
We prove that, when constraints are made up of equations between free, finite
terms, \hmx is a reformulation of \dm. In the presence of a more powerful
constraint language, \hmx is an extension of \dm.
\iffull
{In \S\ref{section-pcb}, we propose an original reformulation of \hmx,
dubbed \pcb, whose distinctive feature is to exploit \emph{type scheme
introduction} and \emph{instantiation} constraints.  }In
\S\ref{section-constraint-generation}, we show that\shortfull{}{,
thanks to the extra expressive power afforded by these constraint forms,}
type inference may be viewed as a combination of constraint generation and
constraint solving, as promised earlier. Then, in
\S\ref{sec.soundness}, we give a type soundness theorem. It is stated
purely in terms of constraints, but---thanks to the results developed in the
previous sections---applies equally to
\shortfull{\hmx}{\pcb, \hmx,} and \dm.

Throughout this core material, the syntax and interpretation of constraints
are left partly unspecified. Thus, the development is \emph{parameterized}
with respect to them---hence the unknown $X$ in the \shortfull {name
\hmx}{names \hmx and \pcb}. We really describe a {family} of
constraint-based type systems, all of which
{share} a common constraint generator and a common type soundness proof.
Constraint solving, however, cannot be independent of $X$: on the contrary,
the design of an efficient solver is heavily dependent on the syntax and
interpretation of constraints. In \S\ref{section-solver}, we consider
constraint solving in the particular case where constraints are made up of
equations interpreted in a free tree model, and define a constraint solver on
top of a standard first-order unification algorithm.

The remainder of \this deals with extensions of the framework. In
\S\ref{section-calc-lang}, we explain how to extend \MLcalc with a number of
features, including products, sums, references, recursion, algebraic data
types\iffull{, pattern matching, type annotations}, and recursive
types. \iffull {In \S\ref{section-univ}, we extend the constraint language
with universal quantification and describe a number of extra features that
require this extension, including a different flavor of type annotations,
polymorphic recursion, and first-class universal and existential types. }Last,
in \S\ref{section-rows}, we extend the type language with \emph{rows} and use
them to assign polymorphic type schemes to operations on records and variants.

% --------------------------------------------------------------------------
% --------------------------------------------------------------------------

\section{Constraints}
\label{section-constraints}

\index{constraints!for type inference|(}

In this section, we define the syntax and logical meaning of constraints.
Both are partly unspecified. Indeed, the set of \emph{type constructors}
(Definition~\ref{def-signature}) must contain at least the binary type
constructor $\arw$, but might contain more. Similarly, the syntax of
constraints involves a set of so-called \emph{predicates} on types, which we
require to contain at least a binary \emph{subtyping} predicate
$\subtype$\indexsymfull{\subtype}{subtyping predicate}, but might contain
more. (The introduction of subtyping, which is absent in \dm, has little
impact on the complexity of our proofs, yet increases the framework's
expressive power. When subtyping is not desired, we interpret the predicate
$\subtype$ as equality.) The logical interpretation of type constructors and
of predicates is left almost entirely unspecified. This freedom allows
reasoning not only about Damas and Milner's type system, but also about a
family of constraint-based extensions of it.

\begin{full}
Type constructors other than $\arw$ and predicates other than $\subtype$ will
never explicitly appear in the definition of our constraint-based type
systems, precisely because the definition is parametric with respect to them.
They can (and usually do) appear in the type schemes assigned to constructors
and destructors in the initial environment~$\ienv$.
%
% Il y a tout de même le constructeur ref qui est built-in... TEMPORARY
%
\end{full}

% --------------------------------------------------------------------------

\subsection*{Syntax of Constraints}

\begin{TTCOMPONENT}{Syntax of type schemes and constraints}{}
\ttlabel{constraints}
\let \\ \TTSyntaxAlternative%

\TTSyntaxCategoryNamed{\ts}{}        {type scheme} \\
{\scheme\tvars\co\ttyp}              {}

\TTSyntaxCategoryNamed{\co,\cp}{}    {constraint} \\
{\ctrue}                             {truth} \\
{\cfalse}                            {falsity} \\
{\predicate\,\ttyp_1\,\ldots\ttyp_n} {predicate application} \\
{\co \wedge \co}                     {conjunction} \\
{\cexists\tvars\co}                  {existential quantification} \\
{\cplet{\evid:\ts}\co}               {type scheme introduction} \\
{\ccall\evid\ttyp}                   {type scheme instantiation}

\columnbreak % Wastes some space, but avoids a widow.

% I'd rather not give such a definition of environments here; they are
% defined as sequences of bindings, so \env ; \env is *notation*, not
% a primitive form. -fpottier
%\TTSyntaxCategoryNamed{\env}{}        {Environments} \\
%{\noenv}                              {} \\
%{x : \ts}                             {} \\
%{\env ; \env}                         {} 

\TTSyntaxCategoryNamed{\co,\cp}{}     {Syntactic sugar for constraints} \\
{\ldots}                              {As before} \\
{\ccall{\ts}{\ttyp}}                  {Definition~\ref{def-ts-instance}} \\
{\cxlet {\evid:\ts}\co}               {Definition~\ref{def-ts-instance}} \\
{\exists \ts}                         {Definition~\ref{def-ts-instance}} \\
{\cplet {\env}\co}                    {Definition~\ref{def-env-constraints}} \\
{\cxlet {\env}\co}                    {Definition~\ref{def-env-constraints}} \\
{\exists \env}                        {Definition~\ref{def-env-constraints}} 

\extraspacehack{.07in}
\end{TTCOMPONENT}

We now define the syntax of constrained type schemes and of constraints and
introduce some extra constraint forms as syntactic sugar.
%
\begin{definition}
\label{def-signature-continued}
Let $\predicate$ range over a finite or denumerable set of \emph{predicates},
each of which has a \emph{\psignature} of the form $\typicalpredicatesig$,
where $n\geq 0$.  For every kind $\kind$, 
let $=_\kind$ and $\subtype_\kind$ be distinguished
predicates of {\psignature} $\kind\kindprod \kind \kindarrow \cdot$.
\end{definition}
%
\begin{definition}
\label{def-constraints}
\iffull{\index{constraints}}
\index{type scheme}
\index{instantiation of a type scheme}
\indexsymfull{\ccall\cdot\cdot}{instance of}
The syntax of \emph{type schemes} and \emph{constraints} is given in
Figure~\ref{fig:constraints}. It is further restricted by the following
requirements. In the type scheme $\scheme\tvars\co\ttyp$ and in the constraint
$\ccall{\evid}{\ttyp}$, the type $\ttyp$ must have kind $\normalkind$. In the
constraint $\predicate\,\ttyp_1\,\ldots\ttyp_n$, the types
$\ttyp_1,\ldots,\ttyp_n$ must have kind $\kind_1,\ldots,\kind_n$,
respectively, if $\predicate$ has {\psignature} $\typicalpredicatesig$.
We write $\dmscheme\tvars\ttyp$ for $\scheme\tvars\ctrue\ttyp$,
which allows viewing \dm type schemes as a subset of constrained type schemes.
\end{definition}
%
We write $\ttyp_1=_\kind \ttyp_2$ and $\ttyp_1 \subtype_\kind \ttyp_2$
for the binary predicate applications
$\mathord{=_\kind}\,\ttyp_1\,\ttyp_2$ and
$\mathord{\subtype_\kind}\,\ttyp_1\,\ttyp_2$, and refer to them as equality and 
subtyping constraints, respectively.
We often omit the subscript $\kind$, so 
$\ttyp_1=\ttyp_2$ and $\ttyp_1 \subtype \ttyp_2$ are well-formed
constraints whenever $\ttyp_1$ and $\ttyp_2$ have the same kind. 
%
%
By convention, $\exists$ and \kwd{def} bind tighter than $\wedge$; that is,
$\exists\tvars.\co\wedge\cp$ is $(\exists\tvars.\co)\wedge\cp$ and
$\cplet{\evid:\ts}\co\wedge\cp$ is $(\cplet{\evid:\ts}\co)\wedge\cp$.
%
In $\scheme\tvars\co\ttyp$, the type variables $\tvars$ are bound within $\co$
and $\ttyp$. In $\cexists\tvars\co$, the type variables $\tvars$ are bound
within $\co$. The sets of free type variables of a type scheme $\ts$ and of a
constraint $\co$, written $\ftv\ts$ and $\ftv\co$%
\indexsymfull{\protect\ftv\cdot}{free type variables}, respectively, are defined
accordingly.
%
In $\cplet{\evid:\ts}\co$, the identifier $\evid$ is bound within $\co$. 
%
The sets of free program identifiers of a
type scheme $\ts$ and of a constraint $\co$, written $\fpv\ts$ and
$\fpv\co$%
\indexsymfull{\protect\fpv\cdot}{free program identifiers},
respectively, are defined accordingly. Note that $\evid$ occurs free in
the constraint $\ccall\evid\ttyp$.

The constraint $\ctrue$, which is always satisfied, mainly serves to indicate
the absence of a nontrivial constraint, while $\cfalse$, which has no
solution, may be understood as the indication of a type error. Composite
constraints include conjunction and existential quantification, which have
their standard meaning, as well as \emph{type scheme introduction} and
\emph{type scheme instantiation} constraints, which are similar to Gustavsson
and Svenningsson's constraint
abstractions \citeyr{gustavsson-svenningsson-01}. In order to be able to
explain these last two forms, we must first introduce a number of derived
constraint forms:
%
\begin{definition}
\label{def-ts-instance}
\indexsymfull{\ccall\cdot\cdot}{instance of}
\index{instantiation of a type scheme}
Let $\ts$ be $\scheme{\tvars}{\cp}{\ttyp}$. If
$\disjoint\tvars{\ftv{\ttyp'}}$ holds, then $\ccall\ts{\ttyp'}$ (read:
\emph{$\ttyp'$ is an instance of $\ts$}) stands for the constraint
$\exists\tvars.(\cp\wedge\ttyp\subtype\ttyp')$. We write
#>$\exists\ts$\indexfull{ $\exists\ts$|see{constraints}} 
(read:
\emph{$\ts$ has an instance}) for $\exists\tvars.\cp$ and
$\cxlet{\evid:\ts}{\co}$\indexsymfull{\cxlet\cdot\cdot}{!constraints}
for $\exists\ts\wedge\cplet{\evid:\ts}\co$.
\end{definition}
Constrained type schemes generalize Damas and Milner's type schemes, while
this definition of instantiation constraints generalizes Damas and Milner's
notion of instance (Definition~\ref{def-type-scheme}). Let us draw a
comparison. First, Damas and Milner's instance relation is binary (given a
type scheme $\dms$ and a type $\ttyp$, either $\ttyp$ is an instance of
$\dms$, or it isn't), and is purely syntactic. For instance, the type
$\twar\arw\tzar$ is \emph{not} an instance of $\dmscheme\tvar{\tvar\arw\tvar}$
in Damas and Milner's sense, because $\twar$ and $\tzar$ are distinct type
variables. In our presentation, on the other hand,
$\ccall{\dmscheme\tvar{\tvar\arw\tvar}} {\twar\arw\tzar}$ is not an assertion;
rather, it is a constraint, which by definition is
$\exists\tvar.(\ctrue\wedge\tvar\arw\tvar\subtype\twar\arw\tzar)$. We later
prove that it is equivalent to
$\exists\tvar.(\twar\subtype\tvar\wedge\tvar\subtype\tzar)$ and to
$\twar\subtype\tzar$, and, if subtyping is interpreted as equality, to
$\twar=\tzar$. That is, $\ccall\ts{\ttyp'}$ represents a condition on (the
ground types denoted by) the type variables in $\ftv{\ts,\ttyp'}$ for $\ttyp'$
to be an instance of $\ts$, in a logical, rather than purely syntactic, sense.
Second, the definition of instantiation constraints involves subtyping, 
to ensure that any supertype of an instance of $\ts$ is again an instance of
$\ts$ (see \shortfull{rule \textsc{C-ExTrans} on
page~\pageref{fig:reasoning-C}}{Lemmas~\ref{lemma-instance-exists}
and~\ref{lemma-instance-covariant}}). This is consistent with the purpose of
subtyping: to allow a subtype where a supertype is
expected~(\TAPLCHAPTER{15}). Third and last, every type scheme $\ts$ is now of
the form $\scheme \tvars\co\ttyp$. The constraint $\co$, whose free type
variables may or may not be members of $\tvars$, is meant to restrict the set
of instances of the type scheme $\scheme\tvars\co\ttyp$. This is evident in
the instantiation constraint $\ccall{\scheme{\tvars}{\co}{\ttyp}}{\ttyp'}$,
which by Definition~\ref{def-ts-instance} stands for
$\exists\tvars.(\co\wedge\ttyp\subtype\ttyp')$: the values that $\tvars$ may
assume are restricted by the demand that $\co$ be satisfied. This
requirement vanishes in the case of \dm type schemes, where $\co$ is $\ctrue$.
Our notions of constrained type scheme and of instantiation constraint are
standard, coinciding with those of \hmx \cite{Sulzmann&97}.

Let us now come back to an explanation of type scheme introduction and
instantiation constraints. In brief, the construct $\cplet{\evid:\ts}\co$
binds the name $\evid$ to the type scheme $\ts$ within the constraint
$\co$. If $\co$ contains a subconstraint of the form $\ccall\evid\ttyp$, where
this occurrence of $\evid$ is free in $\co$, then this subconstraint acquires
the meaning $\ccall\ts\ttyp$. Thus, the constraint $\ccall\evid\ttyp$ is
indeed an instantiation constraint, where the type scheme that is being
instantiated is referred to by name. The constraint $\cplet{\evid:\ts}\co$ may
be viewed as an \emph{explicit substitution} of the type scheme $\ts$ for the
name $\evid$ within $\co$.  Later ({\shortfull {\S\ref
{section-constraint-generation}}{\S\ref {section-pcb}}}), we use such explicit
substitutions to supplant typing environments. That is, where Damas and
Milner's type system augments the current typing environment (\Rule{dm-Abs},
\Rule{dm-Let}), we introduce a new \kwd{def} binding in the current
constraint; where it looks up the current typing environment (\Rule{dm-Var}),
we employ an instantiation constraint. (The reader may wish to look
ahead at Figure~\ref {fig:genrules} on page~\pageref{fig:genrules}.)  The
point is that it is then up to a constraint solver to choose a strategy for
reducing explicit substitutions---for instance, one might wish to
{simplify} $\ts$ before substituting it for $\evid$ within
$\co$---whereas the use of environments in standard type systems such as \dm
and \hmx imposes an eager substitution strategy, which is inefficient and thus
never literally implemented. The use of type scheme introduction and
instantiation constraints allows separating constraint generation and
constraint solving {without compromising efficiency}, or, in other words,
without introducing a gap between the description of the type inference
algorithm and its actual implementation. Although the algorithm that we plan
to describe is not new \cite {Remy!mleth}, its description in terms of
constraints is: to the best of our knowledge, the only close relative of our
\kwd{def} constraints is to be found in \longcite{gustavsson-svenningsson-01}. An
earlier work that contains similar ideas is \longcite{mueller-94}. Approaches
based on semi-unification \cite{he89a,Henglein93} achieve a similar
separation between constraint generation and constraint solving, but are based
on a rather different constraint language.

In the type system of  Damas and Milner, every type scheme $\dms$ has a fixed,
nonempty set of instances. In a constraint-based setting, things are more
complex: given a type scheme $\ts$ and a type $\ttyp$, whether $\ttyp$ is an
instance of $\ts$ (that is, whether the constraint $\ccall\ts\ttyp$ is
satisfied) depends on the meaning assigned to the type variables in
$\ftv{\ts,\ttyp}$. Similarly, given a type scheme, whether \emph{some} type is
an instance of $\ts$ (that is, whether the constraint
$\exists\tzar.\ccall\ts\tzar$, where $\tzar$ is fresh for $\ts$, is satisfied)
depends on the meaning assigned to the type variables in $\ftv\ts$. Because we
do not wish to allow forming type schemes that have no instances, we often use
the constraint $\exists\tzar.\ccall\ts\tzar$. In fact, we later prove that it
is equivalent to $\exists\ts$, as defined above\iffull{
(Exercise~\ref{exercise-instance})}. We also use the constraint form
$\cxlet{\evid:\ts}\co$, which requires $\ts$ to have an instance and at the
same time associates it with the name $\evid$. Because the \kwd{def} form is
more primitive, it is easier to work with at a low level, but it is no longer
explicitly used after \S\ref{section-constraints}; we always use \kwd{let}
instead.

\begin{definition}
\label{def-env-constraints}
Environments $\env$\indexsymfull{\env}{typing environment}
remain as in Definition~\ref{def-environment}, except \dm
type schemes $\dms$ are replaced with constrained type schemes $\ts$.
The set of \emph{free program identifiers} of an environment
$\env$, written $\fpv\env$\indexsymfull{\protect\fpv\cdot}{free program
identifiers}, is defined by $\fpv\noenv=\varnothing$ and
$\fpv{\env;\evid:\ts}=\fpv\env\cup\fpv\ts$.
% TEMPORARY Vincent a suggéré la définition suivante:
%$\fpv{\env;\evid:\ts}=\fpv\env\cup(\fpv\ts\setminus\dpv\env)$.
% Celle-ci a l'intérêt que $\fpv{\cxlet\env\co}$ coïncide avec $\fpv\env\cup(\fpv\co\setminus\dpv\env)$,
% tandis qu'avec notre définition, il y a seulement inclusion entre les deux. Néanmoins, je n'adopte pas
% la définition de Vincent, puisque certaines de nos preuves utilisent effectivement la définition
% ci-dessus. Il faudrait réfléchir un peu pour savoir s'il y a un véritable problème là-dessous.
We write $\dfpv\env$%
\indexsymfull{\protect\dfpv\cdot}{defined and free program variable identifiers}
for $\dpv\env\cup\fpv\env$. We define $\cplet\noenv\co$ as $\co$ and
$\cplet{\env;\evid:\ts}\co$ as $\cplet\env{\cplet{\evid:\ts}\co}$. Similarly, we
define $\cxlet\noenv\co$ as $\co$ and
$\cxlet{\env;\evid:\ts}\co$ as $\cxlet\env{\cxlet{\evid:\ts}\co}$. We define
$\exists\noenv$ as $\ctrue$ and
$\exists{(\env;\evid:\ts)}$ as $\exists\env\wedge\cplet\env{\exists\ts}$\indexfull 
{ $\exists\env$|see{constraints}}. 
\end{definition}
\begin{full}
These three constraint forms will be related by Lemma~\ref{lemma-def-let}.
\end{full}

In order to establish or express certain laws of equivalence between
constraints, we need \emph{constraint contexts}\indexfull{context!constraint}. A
constraint context is a 
constraint with zero, one, or several \emph{holes}, written $\chole$. The
syntax of contexts is as follows:
\label{syntax-ccon}
%
$$\ccon ::= \chole
       \mid \co
       \mid \ccon\wedge\ccon
       \mid \exists\tvars.\ccon
       \mid \cplet{\evid:\ts}\ccon
       \mid \cplet{\evid:\scheme\tvars\ccon\ttyp}\co$$
%
The application of a constraint context $\ccon$ to a constraint $\co$,
written $\ccon[\co]$, is defined in the usual way. Because a constraint
context may have any number of holes, $\co$ may disappear or be duplicated in
the process. Because a hole may appear in the scope of a binder, some
of $\co$'s free type variables and free program identifiers may become
bound in $\ccon[\co]$. We write $\dtv\ccon$\indexsymfull
{\dtv}{defined type variables} and $\dpv\ccon$\indexsymfull
{\dpv}{defined program identifiers} for the
sets of type variables and program identifiers, respectively, that
$\ccon$ may thus capture. We write
$\cxlet{\evid:\scheme\tvars\ccon\ttyp}\co$ for $\exists\tvars.\ccon
\wedge \cplet{\evid:\scheme\tvars\ccon\ttyp}\co$. (Being able to state
such a definition is why we require multi-hole contexts.) We let 
$\xc$\indexsymfull{\xc}{Existential constraint context} range
over \emph{existential constraint contexts}, defined by $\xc ::= 
\exists\tvars.\chole$.

% --------------------------------------------------------------------------

\subsection*{Meaning of Constraints}

\longpage

We have defined the syntax of constraints and given an informal description of
their meaning. We now give a formal definition of the interpretation of
constraints. We begin with the definition of a \emph{model}:
%
\begin{definition}
\label{def-model}
For every kind $\kind$, let $\modelk{\kind}$ be a nonempty set, whose elements
are called the \emph{ground types} of kind $\kind$. In the following, $\gtyp$ ranges
over $\modelk\kind$, for some $\kind$ that may be determined from the
context. For every type constructor $\tycon$ of {\tcsignature}
$\dk\kindarrow\kind$, let $\tycon$ denote a total function from $\modelk\dk$
into $\modelk\kind$, where the indexed product $\modelk\dk$ is the set of all
mappings $\dgtyp$ of domain $\Dom\dk$ that map every $\dird\in\Dom\dk$ to an element of
$\modelk{\dk(\dird)}$. For every predicate symbol $\predicate$ of {\psignature}
$\typicalpredicatesig$, let $\predicate$ denote a predicate on
$\modelk{\kind_1}\times\ldots\times\modelk{\kind_n}$. 
%
For every kind $\kind$, we require the predicate $=_\kind$ to be equality on 
$\modelk\kind$ and the predicate $\subtype_\kind$
to be a partial order on $\modelk\kind$. 
\end{definition}

For the sake of convenience, we abuse notation and write $\tycon$ for both
the type constructor and its interpretation, and similarly for predicates.

\label{page-parameter-x}
By varying the set of type constructors, the set of predicates, the set of
ground types, and the interpretation of type constructors and predicates,
one may define an entire family of related type systems. We refer
to the collection of these choices as $X$. Thus, the type \shortfull {system
\hmx}{systems \hmx and \pcb}, described in \shortfull {\S\ref
{section-hmx}}{\S\ref {section-hmx} and~\S\ref{section-pcb}},
\shortfull {is}{are} \emph {parameterized} by $X$.

The following examples give standard ways of defining the set of ground
types and the interpretation of type constructors.
%
\begin{example}[Syntactic models]
\label{example-syntactic-model}
For every kind $\kind$, let $\modelk{\kind}$ consist of the \emph{closed}
types of kind $\kind$. Then, ground types are types that do not have any free
type variables, and form the so-called \emphindex{Herbrand universe}. Let every
type constructor $\tycon$ be interpreted as itself. Models that define ground
types and interpret type constructors in this manner are referred to as
\emph{syntactic}.
\end{example}
%
\begin{example}[Tree models]
\label{example-tree-model}
Let a \emph{path} $\chemin$ be a finite sequence of directions. The empty path
is written $\epsilon$ and the concatenation of the paths $\chemin$ and
$\chemin'$ is written $\chemin \cdot \chemin'$. Let a \emph{tree} be a partial
function $\gtyp$ from paths to type constructors whose domain is nonempty and
prefix-closed and such that, for every path $\chemin$ in the domain of
$\gtyp$, if the type constructor $\gtyp(\chemin)$ has {\tcsignature}
$\dk\kindarrow\kind$, then $\chemin\cdot\dird \in \Dom\gtyp$ is equivalent to
$\dird\in\Dom\dk$ and, furthermore, for every $\dird\in\Dom\dk$, the type
constructor $t(\chemin\cdot\dird)$ has image kind $\dk(\dird)$. If $\chemin$
is in the domain of $\gtyp$, then the \emph{subtree} of $\gtyp$ rooted at
$\chemin$, written $\gtyp/\chemin$, is the partial function $\chemin'\mapsto
\gtyp(\chemin\cdot \chemin')$. A tree is \emph{finite} if and only if it has
finite domain. A tree is \emph{regular} if and only if it has a finite number
of distinct subtrees. Every finite tree is thus regular. Let $\modelk\kind$
consist of the \emph{finite} (respectively\ \emph{regular}) trees $\gtyp$ such that
$\gtyp(\epsilon)$ has image kind $\kind$: then, we have a \emph{finite} (respectively\
\emph{regular}) \emph{tree model}.

If $\tycon$ has {\tcsignature} $\dk\kindarrow\kind$, one may interpret
$\tycon$ as the function that maps $\dgtyp \in \modelk\dk$ to the ground type
$\gtyp\in\modelk\kind$ defined by $\gtyp(\epsilon)=\tycon$ and
$\gtyp/\dird=\dgtyp(\dird)$ for $\dird\in\Dom\dgtyp$,
that is, the unique ground type whose head symbol is $\tycon$ and whose
subtree rooted at $\dird$ is $\dgtyp(\dird)$. Then, we have a \emph{free}
tree model.
%
Note that free finite tree models coincide with syntactic models, as
defined in the previous example.
\end{example}

Rows (\S\ref{section-rows}) are interpreted in a tree model, albeit
not a free one.
%
The following examples suggest different ways of interpreting the subtyping
predicate.
%
\begin{example}[Equality models]
\label{example-equality-models}
The simplest way of interpreting the subtyping predicate is to let $\subtype$
denote equality on every $\modelk\kind$. Models that do so are referred to as
\emph{equality models}. When no predicate other than equality is available, we
say that the model is \emph{equality-only}.
% Stricto sensu, ce n'est pas le modèle qui est equality-only, mais bien
% le tout formé par la syntaxe et son interprétation.
\end{example}
%
\begin{example}[Structural, nonstructural subtyping]
\label{example-subtyping-models}
\index{structural subtyping}
\index{nonstructural subtyping}
\index{subtyping!structural vs. nonstructural}
Let a \emph{variance} $\nu$ be a nonempty subset of $\{-,+\}$, written $-$
(\emphindexfull{contravariant}), $+$ (\emphindexfull{covariant}), or $\pm$
(\emphindexfull{invariant}) for short. Define the \emph{composition} of two
variances as an associative, commutative operation with $+$ as neutral
element, $\pm$ as absorbing element (that is, $\pm -=\pm+=\pm\pm=\pm$), and
such that $--=+$.
%
Now, consider a free (finite or regular) tree model, where every direction
$\dird$ comes with a fixed variance $\nu(\dird)$. Define the variance
$\nu(\chemin)$ of a path $\chemin$ as the composition of the variances of its
elements.
%
Let $\symleq$ be a partial order on type constructors such that (i) if
$\tycon_1\symleq\tycon_2$ holds and $\tycon_1$ and $\tycon_2$ have
{\tcsignature} $\dk_1\kindarrow\kind_1$ and $\dk_2\kindarrow\kind_2$,
respectively, then $\dk_1$ and $\dk_2$ agree on the intersection of
their domains and $\kind_1$ and $\kind_2$ coincide; and (ii)
$\tycon_0\symleq\tycon_1\symleq\tycon_2$ implies
$\Dom{\tycon_0}\cap\Dom{\tycon_2}\subseteq\Dom{\tycon_1}$. Let
$\symleq^+$, $\symleq^-$, and $\symleq^\pm$ stand for $\symleq$,
$\symgeq$, and $=$, respectively. Then, define the interpretation of
subtyping as follows: if $\gtyp_1,\gtyp_2\in\modelk\kind$, let
$\gtyp_1\subtype\gtyp_2$ hold if and only if, for every path
$\chemin\in\Dom{\gtyp_1}\cap\Dom{\gtyp_2}$,
$\gtyp_1(\chemin)\symleq^{\nu(\chemin)}\gtyp_2(\chemin)$ holds. It is
not difficult to check that $\subtype$ is a partial order on every
$\modelk\kind$. The reader is referred to
\longcite{AmadioCardelli93}, 
% space added [bcp]
\fulllongcite{kozen-palsberg-schwartzbach-95},
and \longcite{BrandtHenglein97} for more details about this
construction. Models that define subtyping in this manner are referred
to as \emph{nonstructural subtyping models}.

A simple nonstructural subtyping model is obtained by: letting the directions
$\dirdomain$ and $\dircodomain$ be contra- and covariant, respectively;
introducing, in addition to the type constructor $\arw$, two type constructors
$\bot$ and $\top$ of {\tcsignature} $\normalkind$; and letting
$\bot\symleq\mathord{\arw}\symleq\top$. This gives rise to a model where
$\bot$ is the least ground type, $\top$ is the greatest ground type, and the
arrow type constructor is, as usual, contravariant in its domain and covariant
in its codomain. This form of subtyping is called \emph{nonstructural} because
comparable ground types may have different shapes: consider, for instance,
$\bot$ and $\bot\arw\top$.

A typical use of nonstructural subtyping is in type systems for records.  One
may, for instance, introduce a covariant direction $\dircontent$ of kind
$\normalkind$, a kind $\fieldkind$, a type constructor $\tcabs$ of
{\tcsignature} $\fieldkind$, a type constructor $\tcpre$ of {\tcsignature}
$\{\dircontent\mapsto\normalkind\}\kindarrow\fieldkind$, and let
$\tcpre\symleq\tcabs$. This gives rise to a model where
$\tpre\gtyp\subtype\tabs$ holds for every $\gtyp\in\modelk\normalkind$. Again,
comparable ground types may have different shapes: consider, for instance,
$\tpre\top$ and $\tabs$. \S\ref{section-rows} says more about typechecking
operations on records.

Nonstructural subtyping has been studied, for example, in
\fulllongcite{kozen-palsberg-schwartzbach-95},
\fulllongcite{palsberg-wand-okeefe-97}, 
\fulllongcite{JimPalsberg99},
\fulllongcite{pottier-ic-01}, 
\longcite{su-al-02}, and
\fulllongcite{niehren-priesnitz-03}.

An important particular case arises when any two type constructors related by
$\symleq$ have the same arity (and thus also the same signatures). In that
case, it is not difficult to show that {any two ground types related by
subtyping must have the same shape}, that is, if $\gtyp_1\subtype\gtyp_2$
holds, then $\Dom{\gtyp_1}$ and $\Dom{\gtyp_2}$ must coincide. For this
reason, such an interpretation of subtyping is usually referred to as
\emph{atomic} or \emph{structural} subtyping. It has been studied in the
finite \cite{Mitchell84a, mitchell-91, tiuryn-92, pratt-tiuryn-96, frey-97,
rehof-minimality-97, kuncak-rinard-03, simonet-solver-03} and
regular \cite{TiurynWand1993TAPSOFT} cases.
%
% When dealing with structural subtyping, it may be useful to introduce an
% auxiliary predicate $\approx$, whose interpretation is the symmetric
% transitive closure of $\subtype$, that is, a congruence. Solving
% $\approx$-constraints is a unification problem and gives an efficient way of
% discovering the shape of every type.
%
Structural subtyping is often used in automated program analyses that enrich
standard types with atomic annotations without altering their shape.
% On pourrait ajouter des citations, mais ce serait ouvrir une boîte de Pandore.
\end{example}

\begin{full}
Our last example suggests a predicate other than equality and
subtyping.
%
\begin{example}[Conditional constraints]
\label{example-conditional-constraints}
\newcommand{\cc}[4]{#1\symleq#2\Rightarrow#3\subtype#4} Consider a
nonstructural subtyping model. For every type constructor $\tycon$ of
image kind $\kind$ and for every kind $\kind'$, let
$(\cc\tycon\cdot\cdot\cdot)$ be a predicate of {\psignature}
$\kind\kindprod\kind'\kindprod\kind'\kindarrow\cdot$. Thus, if
$\ttyp_0$ has kind $\kind$ and $\ttyp_1$, $\ttyp_2$ have the same
kind, then $\cc\tycon{\ttyp_0}{\ttyp_1}{\ttyp_2}$ is a well-formed
constraint, called a \emph{conditional subtyping constraint}. Its
interpretation is defined as follows: if $\gtyp_0\in\modelk\kind$ and
$\gtyp_1, \gtyp_2\in\modelk{\kind'}$, then
$\cc\tycon{\gtyp_0}{\gtyp_1}{\gtyp_2}$ holds if and only if
$\tycon\symleq\gtyp_0(\epsilon)$ implies $\gtyp_1\subtype\gtyp_2$. In
other words, if $\gtyp_0$'s head symbol exceeds $\tycon$ according to
the ordering on type constructors, then the subtyping constraint
$\gtyp_1\subtype\gtyp_2$ must hold; otherwise, the conditional
constraint holds vacuously. Conditional constraints have been studied, for
example, in \longcite{Reynolds:1969}, \longcite{heintze94},
\longcite{aiken-wimmers-lakshman-94}, \longcite{pottier-njc-00}, and
\longcite{su-aiken-01}.
\end{example}
\end{full}
%
Many other kinds of constraints exist, which we lack space to list;
see \longcite{comon-93} for a short survey.
% TEMPORARY autres citations?
% il faudrait déterminer si les set constraints peuvent être vues
% comme un modèle particulier; il faudrait ajouter U et /\ à la syntaxe; citer la thèse de Fähndrich
% évoquer aussi les tree set models (INES), les feature constraints, etc.

Throughout \this, we assume (unless otherwise stated) that the set of type
constructors, the set of predicates, and the model---which, together, form the
parameter $X$---are arbitrary, but fixed.

%% Because of the large breaks around environments
\medskip

As usual, the meaning of a constraint is a function of the meaning of its free
type variables and of its free program identifiers, which are respectively
given by a \emph{ground assignment} and a \emph{ground environment}.
%
\begin{definition}
\label{def-ga}
A \emphindexfull{ground assignment} $\ga$\indexsymfull{\ga}{ground assignment} is a
total, kind-preserving mapping from $\tyvarset$ into~$\model$. Ground
assignments are extended to types by
$\ga(\tycon\,\ttyp_1\,\ldots\,\ttyp_n)=\tycon (\ga(\ttyp_1), \ldots,
\ga(\ttyp_n))$. Then, for every type $\ttyp$ of kind $\kind$, $\ga(\ttyp)$ is
a ground type of kind $\kind$.

A \emphindexfull{ground type scheme} $\gs$\indexsymfull{\gs}{ground type scheme} is a
set of ground types, which we require to be \emph{upward-closed} with respect
to subtyping: that is, $\gtyp\in\gs$ and $\gtyp\leq\gtyp'$ must imply
$\gtyp'\in\gs$. A \emphindexfull{ground environment}
$\genv$\indexsymfull{\genv}{ground environment} is a partial mapping from
identifiers to ground type schemes.

Because the syntax of type schemes and constraints is mutually recursive,
so is their interpretation.
%
The interpretation of a type scheme $\ts$ under a ground assignment $\ga$ and
a ground environment $\genv$ is a ground type scheme, written
$\schi\ga\genv\ts$. It is defined in Figure~\ref{fig:meaning-of-constraints}.
% $$\schi\ga\genv{(\scheme\tvars\co\ttyp)} =
%   \mathop\uparrow\{ \etend\ga\tvarc\gtypc(\ttyp) \;;\; \etend\ga\tvarc\gtypc,\genv\satisfies\co \}$$
The $\uparrow$ is the upward closure operator and $\satisfies$ is the
constraint 
satisfaction predicate, defined next. The interpretation of a constraint $\co$
under a ground assignment $\ga$ and a ground environment $\genv$ is a truth value,
written $\ga,\genv\satisfies\co$ (read: $\ga$ and $\genv$
\emph{satisfy} $\co$). The three-place predicate $\satisfies$ is defined by the rules in
Figure~\ref{fig:meaning-of-constraints}.
%
A constraint $\co$ is \emphindexfull{satisfiable} if and only if
$\ga,\genv\satisfies\co$ holds for some $\ga$ and $\genv$. It is
\emph{false} (or \emph{unsatisfiable})
otherwise.
\end{definition}

% NOTE Vincent a suggéré qu'au lieu de trimballer le préfixe \cplet\env
% sous forme syntaxique dans les règles, on ferait mieux de trimballer un
% `ground environment', qui à chaque \evid associe un ensemble (clos vers
% le haut) de types ground. On pourrait alors interpréter la construction
% \cplet comme une extension du ground environment courant, et la construction
% \ccall comme un simple test d'appartenance. Ça nous libérerait probablement
% de pas mal de problèmes syntaxiques dans les lemmes de bas niveau.

% TEMPORARY C'est fait en version courte, mais les preuves ne sont pas à jour!
% Il faut revoir toutes les preuves qui font appel à la définition de l'interprétation des contraintes (règles CM-)

% TEMPORARY on devrait probablement aussi axiomatiser les contraintes dans
% un premier temps, et ne donner une construction explicite du modèle que
% dans un second temps.

% TEMPORARY on pourrait peut-être en profiter pour prendre let comme forme
% primitive et supprimer def


\begin{TTCOMPONENT}{Meaning of constraints}{}
\ttlabel{meaning-of-constraints}

\hbox to \linewidth {\hbox to \syntaxboxwidth{\vbox{%
\textbf{Interpretation of type schemes:}
$$
\begin{tabular}{LR}
\schi\ga\genv{(\scheme\tvars\co\ttyp)} =\\
\qquad
  \mathop\uparrow\{ \etend\ga\tvarc\gtypc(\ttyp) \;;\; \etend\ga\tvarc\gtypc,\genv\satisfies\co \}
\end{tabular}
$$
}}}

\hbox{\textbf{Interpretation  of constraints:}}

\infax[CM-True]
  {\ga,\genv\satisfies\ctrue}

\infrule[CM-Predicate]
  {\predicate(\ga(\ttyp_1),\ldots,\ga(\ttyp_n))}
  {\ga,\genv\satisfies{\predicate\,\ttyp_1\,\ldots\,\ttyp_n}}

\goodbreak

\infrule[CM-And]
  {\ga,\genv\satisfies\co_1 \\ \ga,\genv\satisfies\co_2}
  {\ga,\genv\satisfies\co_1\wedge\co_2}

\infrule[CM-Exists]
  {\etend\ga\tvarc\gtypc,\genv\satisfies\co}
  {\ga,\genv\satisfies\exists\tvars.\co}

\infrule[CM-Def]
  {\ga,\etend\genv\evid{\schi\ga\genv\ts}\satisfies\co}
  {\ga,\genv\satisfies\cplet{\evid:\ts}\co}

\infrule[CM-Instance]
  {\ga(\ttyp)\in\genv(\evid)}
  {\ga,\genv\satisfies\ccall\evid\ttyp}

\extraspacehack{.07in}
\end{TTCOMPONENT}

Let us now explain these definitions. The interpretation of the type scheme
$\scheme\tvars\co\ttyp$ is a set of ground types, which we may refer to as the
type scheme's \emph{ground instances}. It contains the images of $\ttyp$ under
extensions of $\ga$ with new values for the universally quantified variables
$\tvars$; these values may be arbitrary, but must be such that the constraint
$\co$ is satisfied. We implicitly require $\tvarc$ and $\gtypc$ to have
matching kinds, so that $\etend\ga\tvarc\gtypc$ remains a kind-preserving
ground assignment. This set is upward closed, so any ground type that lies
above a ground instance of $\ts$ is also a ground instance of $\ts$. This
interpretation is standard; see, for example, \longcite{pottier-hmx-01}.

The rules that define $\satisfies$ (Figure~\ref{fig:meaning-of-constraints})
are syntax-directed. \Rule{CM-True} states that the constraint $\ctrue$ is a
tautology, that is, holds in every context. No rule matches the constraint
$\cfalse$, which means that it holds in no context. \Rule{CM-Predicate} states
that the meaning of a predicate application is given by the predicate's
interpretation within the model. More specifically, if $\predicate$'s
{\psignature} is $\typicalpredicatesig$, then, by well-formedness of the
constraint, every $\ttyp_i$ is of kind $\kind_i$, so $\ga(\ttyp_i)$ is a
ground type in $\modelk{\kind_i}$. By Definition~\ref{def-model}, $\predicate$
denotes a predicate on $\modelk{\kind_1}\times\ldots\times\modelk{\kind_n}$,
so the rule's premise is mathematically well-formed. It is independent of
$\genv$, which is natural, since a predicate application has no free program
identifiers. \Rule{CM-And} requires each of the conjuncts to be valid in
isolation.  \Rule{CM-Exists} allows the type variables $\tvarc$ to denote
arbitrary ground types $\gtypc$ within $\co$, independently of their image
through $\ga$. \Rule{CM-Def} deals with type scheme introduction constraints, of
the form $\cplet{\evid:\ts}\co$. It binds $\evid$, within $\co$, to the ground
type scheme currently denoted by $\ts$. Last, \Rule{CM-Instance} concerns type
scheme instantiation constraints of the form $\ccall\evid\ttyp$. Such a
constraint is valid if and only if the ground type denoted by $\ttyp$ is a
member of the ground type scheme denoted by $\evid$.

It is possible to prove that the constraints $\cplet{\evid:\ts}\co$ and
$\subst\evid\ts\co$ have the same meaning, where the latter denotes the
capture-avoiding substitution of $\ts$ for $\evid$ throughout $\co$.
%
% NOTE Il y a là un léger abus de terminologie, puisqu'il s'agit d'une
% substitution `intelligente', qui remplace $\ccall\evid\ttyp$ par
% $\ccall\ts\ttyp$. Ça me paraît suffisamment bénin pour être ignoré.
%
As a matter of fact, it would have been possible to use this equivalence as a
{definition} of the meaning of \kwd{def} constraints, but the present
style is pleasant as well.
\begin{full}
A related equivalence law is established and
exploited below (Lemma~\ref{lemma-pushcon}).
\end{full}
This confirms our claim
that the \kwd{def} form is an explicit substitution form.

\begin{full}
In a judgment of the form $\ga\satisfies\co$, the ground assignment $\ga$
applies to the free type variables of $\co$. This is made precise by the two
following statements. In the second one, $\circ$ is composition and
$\unifier(\co)$ is the capture-avoiding application of the type substitution
$\unifier$ to $\co$.
%
\begin{lemma}
\label{lemma-sat-free}
If $\disjoint\tvars{\ftv\co}$ holds, then $\ga\satisfies\co$ and
$\etend\ga\tvarc\gtypc\satisfies\co$ are equivalent.
\end{lemma}
%
\begin{lemma}
\label{lemma-sat-subst}
$\ga\circ\unifier\satisfies\co$
and $\ga\satisfies\unifier(\co)$ are equivalent.
\end{lemma}
\end{full}

Because constraints lie at the heart of our treatment of \MLtype, most of our
proofs involve establishing logical properties of constraints. These
properties are usually not stated in terms of the satisfaction predicate
$\satisfies$, which is too low-level. Instead, we reason in terms of
\emph{entailment} or \emph{equivalence} assertions. Let us first define these
notions.
%
\begin{definition}
\label{def-entailment}
\iffull{\index{entailment!constraint}}
We write $\co_1\entails\co_2$\indexsymfull{\entails}{entailment},
and say that $\co_1$ \emph{entails} $\co_2$, if 
and only if, for every ground assignment $\ga$ and for every ground environment
$\genv$, the assertion $\ga,\genv\satisfies\co_1$ implies
$\ga,\genv\satisfies\co_2$. We write 
$\co_1\logeq\co_2$, and say that $\co_1$ and $\co_2$ are \emph{equivalent}, if
and only if $\co_1\entails\co_2$ and $\co_2\entails\co_1$ hold.
\end{definition}

In other words, $\co_1$ entails $\co_2$ when $\co_1$ imposes stricter
requirements on its free type variables and program identifiers than $\co_2$
does. Note that $\co$ is unsatisfiable if and only if $\co\logeq\cfalse$
holds. It is straightforward to check that entailment is reflexive and
transitive and that $\logeq$ is indeed an equivalence relation.

\begin{full}
The fact that the predicates $=$ and $\subtype$ are respectively interpreted
as equality and as an ordering implies that the constraints $\ttyp_1 =
\ttyp_2$ and $\ttyp_1 \subtype \ttyp_2 \wedge \ttyp_2 \subtype \ttyp_1$ are
equivalent.
\end{full}

We immediately exploit the notion of constraint equivalence to define what it
means for a type constructor to be \emphindexfull{covariant},
\emphindexfull{contravariant}, or \emphindexfull{invariant} with 
respect to one of its parameters. Let $\tycon$ be a type constructor of
{\tcsignature} $\kind_1\kindprod\ldots\kindprod\kind_n\kindarrow\kind$. Let
$i\in\{1,\ldots,n\}$. $\tycon$ is {covariant} (respectively\
{contravariant}, {invariant}) with respect to its \kth{i}
parameter if and only if, for all types $\ttyp_1,\ldots,\ttyp_n$ and
$\ttyp'_i$ of appropriate kinds, the constraint
$\tycon\,\ttyp_1\,\ldots\ttyp_i\,\ldots\,\ttyp_n \subtype
\tycon\,\ttyp_1\,\ldots\ttyp'_i\,\ldots\,\ttyp_n$ is equivalent to
$\ttyp_i\subtype\ttyp'_i$ (respectively\ $\ttyp'_i\subtype\ttyp_i$,
$\ttyp_i=\ttyp'_i$).
%
% NOTE For the sake of brevity, these notions are defined with respect to a numeric
% index $i$, but may just as well be defined with respect to a direction
% $\dird$.
%
\begin{exercise}[\QuickCheck, \nosolution]
Check the following facts: (i) in an equality model, covariance,
contravariance, and invariance coincide; (ii) in an equality free tree model,
every type constructor is invariant with respect to each of its parameters;
and (iii) in a nonstructural subtyping model, if the direction $\dird$ has
been declared covariant (respectively\ contravariant, invariant), then every type
constructor whose arity includes $\dird$ is covariant (respectively\ contravariant,
invariant) with respect to $\dird$.
\end{exercise}
%

In the following, {we require the type constructor $\arw$ to be
contravariant with respect to its domain and covariant with respect to its
codomain}---a standard requirement in type systems with
subtyping~(\TAPLCHAPTER{15}). This requirement is summed up by the following
equivalence law:
%
\infax[C-Arrow]
  {\ttyp_1\arw\ttyp_2 \subtype \ttyp_1'\arw\ttyp_2' \logeq
   \ttyp_1'\subtype\ttyp_1 \wedge \ttyp_2\subtype\ttyp_2'}
%
Note that this requirement bears on the interpretation of
types and of the subtyping predicate. In an equality free tree model, by (i)
and (ii) in the exercise above,
it is always satisfied. In a nonstructural subtyping model, it boils
down to requiring that the directions $\dirdomain$ and $\dircodomain$ be
declared contravariant and covariant, respectively. In the general case, we do
not have any knowledge of the model and cannot formulate a more precise
requirement. Thus, it is up to the designer of the model to ensure that
\Rule{C-Arrow} holds.

We also exploit the notion of constraint equivalence to define what it means
for two type constructors to be \emphindexfull{incompatible}.  Two type constructors
$\tycon_1$ and $\tycon_2$ with the same image kind are
{incompatible} if and only if all constraints of the form
$\tycon_1\,\ttypc_1 \subtype \tycon_2\,\ttypc_2$ and $\tycon_2\,\ttypc_2
\subtype \tycon_1\,\ttypc_1$ are false. Note that in an equality
free tree model, any two distinct type constructors are incompatible.
%
In the following, we often indicate that a newly introduced type constructor
must be \emphindexfull{isolated}. We implicitly require that,
whenever both % NOTE j'exige both pour permettre les comparaisons avec \bot et \top. Est-ce bizarre?
$\tycon_1$ and $\tycon_2$ are isolated, $\tycon_1$ and $\tycon_2$ be
incompatible. Thus, the notion of isolation provides a concise and modular
way of stating a collection of incompatibility requirements. We require the
type constructor $\arw$ to be isolated.
% TEMPORARY We should have a list of requirements in some place.

% -----------------------------------------------------------------------------
% Constraint equivalence laws.

\subsection*{Reasoning with Constraints}

In this section, we give a number of \iffull{entailment and }equivalence laws that
are often useful and help understand the meaning of constraints.%
\begin{short}
To begin, we note that entailment is preserved by arbitrary constraint
contexts, as stated by the  theorem below. As a result, constraint
equivalence is a congruence. Throughout \this, these facts are often used
implicitly.
%
\begin{theorem}[Congruence]
\label{theorem-entailment-congruence}
$\co_1\entails\co_2$ implies
$\ccon[\co_1]\entails\ccon[\co_2]$.
\end{theorem}
\end{short}
%
\begin{full}
The following lemma provides a slightly weaker characterization of entailment,
and is sometimes a useful tool in establishing entailment assertions.
%
\begin{lemma}
\label{lemma-gamma}
Let $\tvars$ be an arbitrary finite set of type variables.
If, for every ground assignment $\ga$ and for every environment $\env$ such
that $\disjoint\tvars{\ftv\env}$, $\satdef\ga\env{\co_1}$ implies
$\satdef\ga\env{\co_2}$, then $\co_1$ entails $\co_2$.
\end{lemma}
%
\begin{Proof}
\demoreset Let us note that, if the hypothesis
$\disjoint\tvars{\ftv{\co_1,\co_2}}$ were added to the statement of the present
Lemma, then the result would follow immediately via a simple renaming
argument. In the absence of this extra hypothesis, however, we must rely on
Lemma~\ref{lemma-sat-subst} and on the fact that entailment involves a
universal quantification on $\ga$, as we shall now do.

Let $\satdef\ga\env{\co_1}$~\dlabel{h}, where $\tvars$ and
$\ftv\env$ are not necessarily disjoint. Let $\twars=\ftv\env$~\dlabel{y}. Let
$\subst\twarc\tzarc$ be a one-to-one, kind-preserving mapping of $\twars$ onto
a set of type variables $\tzars$, where $\disjoint\tzars\tvars$~\dlabel{zx}
and $\disjoint\tzars{\ftv{\co_1,\co_2}}$~\dlabel{zc}. We may view
$\subst\tzarc\twarc$ as a type substitution of domain $\tzars$. (Note that it
is \emph{not} a renaming, because even though it is a bijection when viewed as
a \emph{finite} mapping, it is no longer one when viewed as a \emph{total}
mapping.) Then, thanks to \dref{y} and \dref{zc}, the constraint
$\cplet\env{\co_1}$ may be written
$\subst\tzarc\twarc{(\cplet{\subst\twarc\tzarc\env}{\co_1})}$.  By this
observation and by Lemma~\ref{lemma-sat-subst}, \dref{h} may be written
$\satdef{\ga\circ\subst\tzarc\twarc}
{\subst\twarc\tzarc\env}{\co_1}$~\dlabel{k}. Now, we have
$\ftv{\subst\twarc\tzarc\env}=\tzars$, which by \dref{zx} is disjoint
with $\tvars$. Thus, the Lemma's main hypothesis applies to \dref{k} and yields
$\satdef{\ga\circ\subst\tzar\twar}
{\subst\twarc\tzarc\env}{\co_2}$. Performing the same steps in reverse,
we find that this assertion may be written $\satdef\ga\env{\co_2}$.
\end{Proof}

The remainder of this section offers a constraint manipulation toolbox. First,
we prove that entailment and equivalence are preserved by conjunction,
existential quantification, and \kwd{def} forms. Then, we establish a number
of entailment and equivalence laws that are heavily used in the forthcoming
sections.
Entailment is preserved by conjunction and by existential quantification. In
the following, this property is often used implicitly.
%
\begin{lemma}
\label{lemma-entail-and}
$\co_1\entails\co_2$ and
$\co'_1\entails\co'_2$ imply
$\co_1\wedge\co'_1\entails\co_2\wedge\co'_2$.
\end{lemma}
%
\begin{Proof}
\demoreset Assume $\co_1\entails\co_2$~\dlabel{hyp1} and
$\co'_1\entails\co'_2$~\dlabel{hyp2} and
$\satdef\ga\env{\co_1\wedge\co'_1}$~\dlabel{h}. By \Rule{CM-And}, \dref{h}
implies $\satdef\ga\env{\co_1}$~\dlabel{one} and
$\satdef\ga\env{\co'_1}$~\dlabel{two}.  Together, \dref{hyp1} and \dref{one}
imply $\satdef\ga\env{\co_2}$~\dlabel{trois}. Similarly, \dref{hyp2} and
\dref{two} imply $\satdef\ga\env{\co'_2}$~\dlabel{quatre}. By \Rule{CM-And},
\dref{trois} and \dref{quatre} yield $\satdef\ga\env{\co_2\wedge\co'_2}$.
Discharging \dref{h} yields the goal.
\end{Proof}
%
\begin{lemma}
\label{lemma-entail-exists}
$\co_1\entails\co_2$ implies
$\exists\tvars.\co_1\entails\exists\tvars.\co_2$.
\end{lemma}
%
\begin{Proof}
\demoreset Assume $\co_1\entails\co_2$~\dlabel{hyp1} and
$\satdef\ga\env{\exists\tvars.\co_1}$~\dlabel{h} and
$\disjoint\tvars{\ftv\env}$~\dlabel{j}. By \Rule{CM-Exists}, \dref{h} and
\dref{j} imply $\satdef{\etend\ga\tvarc\gtypc}\env{\co_1}$~\dlabel{k} for some
$\gtypc$. By definition of entailment, \dref{hyp1} and \dref{k} imply
$\satdef{\etend\ga\tvarc\gtypc}\env{\co_2}$ which, by \dref{j} and
\Rule{CM-Exists} again, yields $\satdef\ga\env{\exists\tvars.\co_2}$. By
Lemma~\ref{lemma-gamma}, discharging \dref{h} and \dref{j} yields the goal.
% C'est la forme faible du lemme~\ref{lemma-gamma} qui est employée.
\end{Proof}

Conjunction is commutative and associative. Adding a new member to a
conjunction strengthens it. Furthermore, a conjunct is redundant if it is
entailed by another conjunct. In the following, these properties are often
used implicitly.
%
\begin{lemma}
\label{lemma-conjunction}
$\co_1\wedge\co_2\logeq\co_2\wedge\co_1$.
$(\co_1\wedge\co_2)\wedge\co_3\logeq\co_1\wedge(\co_2\wedge\co_3)$.
\end{lemma}
%
\begin{Proof}
By \Rule{CM-And}, both $\satdef\ga\env{(\co_1\wedge\co_2)}$ and
$\satdef\ga\env{(\co_2\wedge\co_1)}$ are equivalent to
$(\satdef\ga\env{\co_1})\wedge(\satdef\ga\env{\co_2})$.  Similarly, both
$\satdef\ga\env{((\co_1\wedge\co_2)\wedge\co_3)}$ and
$\satdef\ga\env{(\co_1\wedge(\co_2\wedge\co_3))}$ are equivalent to
$(\satdef\ga\env{\co_1})\wedge(\satdef\ga\env{\co_2})
\wedge(\satdef\ga\env{\co_3})$.
\end{Proof}
%
\begin{lemma}
\label{lemma-dup}
$\co_1\wedge\co_2 \entails \co_1$. 
\iffull {Furthermore, $\co_1\entails\co_2$ implies
$\co_1\entails\co_1\wedge\co_2$.} 
\end{lemma}
\begin{Proof}
By \Rule{CM-And}, $\satdef\ga\env{(\co_1\wedge\co_2)}$
implies $\satdef\ga\env{\co_1}$, so $\co_1\wedge\co_2$ entails $\co_1$.
Conversely, assume $\satdef\ga\env{\co_1}$. Then, given the hypothesis
$\co_1\entails\co_2$, we have $\satdef\ga\env{\co_2}$ as well. By
\Rule{CM-And}, the two imply $\satdef\ga\env{(\co_1\wedge\co_2)}$. So, $\co_1$
entails $\co_1\wedge\co_2$.
\end{Proof}

The following laws are known as \emph{scope extrusion} of the existential
quantifier.
%
\begin{lemma}
\label{lemma-ex-and}
If $\disjoint\tvars{\ftv{\co_2}}$ then $\exists\tvars.(\co_1\wedge\co_2)
\logeq (\exists\tvars.\co_1)\wedge\co_2$.
\end{lemma}
%
\begin{Proof}
\demoreset Assume $\disjoint\tvars{\ftv{\co_2}}$~\dlabel{f} and
$\disjoint\tvars{\ftv\env}$~\dlabel{j}.  By \dref{j}, \Rule{CM-Exists}, and
\Rule{CM-And}, the assertion $\satdef\ga\env{\exists\tvars.(\co_1 \wedge
  \co_2)}$~\dlabel{one} is equivalent to the existence of $\gtypc$ such that
$\satdef{\etend\ga\tvarc\gtypc}\env{\co_1} \wedge
\satdef{\etend\ga\tvarc\gtypc}\env{\co_2}$~\dlabel{conj} holds. By \dref{f},
\dref{j}, and Lemma~\ref{lemma-sat-free}, the second conjunct of \dref{conj}
is equivalent to $\satdef\ga\env{\co_2}$. Thus, by \dref{j}, \Rule{CM-Exists},
and \Rule{CM-And}, \dref{one} is equivalent to
$\satdef\ga\env{(\exists\tvars.\co_1) \wedge \co_2}$.
Lemma~\ref{lemma-gamma} allows discharging \dref{j} and yields the goal.
% C'est la forme faible du lemme~\ref{lemma-gamma} qui est employée.
\end{Proof}
%
\begin{lemma}
\label{lemma-def-ex}
If $\disjoint\tvars{\ftv\ts}$ then
$\exists\tvars.\cplet{\evid:\ts}\co \logeq
\cplet{\evid:\ts}{\exists\tvars.\co}$.
\end{lemma}
%
\begin{Proof}
\demoreset Assume $\disjoint\tvars{\ftv\ts}$~\dlabel{f} and
$\disjoint\tvars{\ftv\env}$~\dlabel{j}. By \dref{j} and \Rule{CM-Exists}, the
assertion $\satdef\ga\env{\exists\tvars.\cplet{\evid:\ts}\co}$ is equivalent
to the existence of $\gtypc$ such that
$\sat{\env;\evid:\ts}{\etend\ga\tvarc\gtypc}\co$ holds, which, by \dref{f},
\dref{j}, and \Rule{CM-Exists}, is equivalent to
$\satdef\ga{\env;\evid:\ts}{\exists\tvars.\co}$.
Lemma~\ref{lemma-gamma} allows discharging \dref{j} and yields the goal.
% C'est la forme faible du lemme~\ref{lemma-gamma} qui est employée.
\end{Proof}

Here are some further properties of existential quantification.
%
\begin{lemma}
\label{lemma-exists-entail}
$\co \entails \exists\tvars.\co$.
\end{lemma}
%
\begin{Proof}
\demoreset Assume $\disjoint\tvars{\ftv\env}$~\dlabel{fresh} and
$\satdef\ga\env\co$~\dlabel{h}. Applying \Rule{CM-Exists} to \dref{fresh} and
\dref{h} yields $\satdef\ga\env{\exists\tvars.\co}$. Lemma~\ref{lemma-gamma}
allows discharging \dref{fresh} and yields the goal.
% C'est la forme FORTE du lemme~\ref{lemma-gamma} qui est employée.
\end{Proof}
%
\begin{lemma}
\label{lemma-ex-ex}
$\exists\tvars.\exists\twars.\co \logeq
\exists\tvars\twars.\co$.
\end{lemma}
%
\begin{Proof}
\demoreset Let $\disjoint{\tvars\twars}{\ftv\env}$~\dlabel{j}. We find, by
\dref{j} and \Rule{CM-Exists}, that the assertions
$\satdef\ga\env{\exists\tvars.\exists\twars.\co}$ and
$\satdef\ga\env{\exists\tvars\twars.\co}$ are both equivalent to the existence
of a ground assignment $\ga'$ such that $\ga$ and $\ga'$ agree outside of
$\tvars\twars$ and $\satdef{\ga'}\env\co$ holds.
Lemma~\ref{lemma-gamma} allows discharging \dref{j} and yields the goal.
% C'est la forme faible du lemme~\ref{lemma-gamma} qui est employée.
\end{Proof}

The following result states that it is equivalent for a type $\ttyp'$ to be an
instance of $\ts$ or to be a supertype of some instance of $\ts$.
%
\begin{lemma}
\label{lemma-instance-exists}
If $\tzar\not\in\ftv{\ts,\ttyp'}$ then $\ccall\ts{\ttyp'} \logeq
\exists\tzar.(\ccall\ts\tzar\wedge\tzar\subtype\ttyp')$.
\end{lemma}
%
\begin{Proof}
\demoreset Assume $\tzar\not\in\ftv{\ts,\ttyp'}$~\dlabel{f} and
$\tzar\not\in\ftv\env$~\dlabel{j}. Write $\ts$ as $\scheme\tvars\co\ttyp$,
where $\disjoint\tvars{\ftv{\ttyp',\tzar,\env}}$~\dlabel{x}. Then, the
following statements are equivalent:
$$\begin{array}{rll}
& \satdef\ga\env{\ccall{\scheme\tvars\co\ttyp}{\ttyp'}} \\
& \satdef\ga\env{\exists\tvars.(\co\wedge\ttyp\subtype\ttyp')}
& \dlabel{zero} \\
\exists\gtypc & 
\satdef{\etend\ga\tvarc\gtypc}\env\co \wedge
\etend\ga\tvarc\gtypc(\ttyp)\subtype\ga(\ttyp')
& \dlabel{un} \\
\exists\gtyp\exists\gtypc &
\satdef{\etend\ga\tvarc\gtypc}\env\co \wedge
\etend\ga\tvarc\gtypc(\ttyp)\subtype\gtyp \wedge
\gtyp\subtype\ga(\ttyp')
& \dlabel{deux} \\
\exists\gtyp\exists\gtypc &
\satdef{\etend{\ga'}\tvarc\gtypc}\env\co \wedge
\etend{\ga'}\tvarc\gtypc(\ttyp)\subtype\ga'(\tzar) \wedge
\ga'(\tzar)\subtype\ga'(\ttyp')
& \dlabel{trois} \\
\exists\gtyp &
\satdef{\ga'}\env{\ccall{\scheme\tvars\co\ttyp}\tzar} \wedge
\ga'(\tzar)\subtype\ga'(\ttyp')
& \dlabel{quatre} \\
& \satdef\ga\env{\exists\tzar.(\ccall{\scheme\tvars\co\ttyp}\tzar \wedge
\tzar\subtype\ttyp'
)}
& \dlabel{cinq}
\end{array}$$
Indeed, \dref{zero} follows from \dref{x} and
Definition~\ref{def-ts-instance}; \dref{un} is by \dref{x}, \Rule{CM-Exists},
\Rule{CM-And}, and \Rule{CM-Predicate}; \dref{deux} follows from the fact that
subtyping in the model is reflexive and transitive; in \dref{trois}, $\ga'$
stands for $\etend\ga\tzar\gtyp$, and the equivalence follows from
Lemma~\ref{lemma-sat-free} because \dref{f}, \dref{j}, and \dref{x} imply
$\tzar\not\in\ftv{\env,\co,\ttyp,\ttyp'}$; \dref{quatre} is by \dref{x},
\Rule{CM-Predicate}, \Rule{CM-And}, \Rule{CM-Exists}, and by
Definition~\ref{def-ts-instance}; \dref{cinq} is by \Rule{CM-Predicate},
\Rule{CM-And}, \dref{j}, and \Rule{CM-Exists}.  Lemma~\ref{lemma-gamma} allows
discharging \dref{j} and yields the goal.
% C'est la forme faible du lemme~\ref{lemma-gamma} qui est employée.
\end{Proof}

The following lemma states that any supertype of an instance of $\ts$ is also
an instance of $\ts$.
%
\begin{lemma}
\label{lemma-instance-covariant}
$\ccall\ts\ttyp\wedge\ttyp\subtype\ttyp'\entails\ccall\ts{\ttyp'}$.
\end{lemma}
%
\begin{Proof}
\demoreset Let $\tzar\not\in\ftv{\ts,\ttyp,\ttyp'}$~\dlabel{z}. By \dref{z}
and Lemma~\ref{lemma-instance-exists}, the
constraint $\ccall\ts\ttyp\wedge\ttyp\subtype\ttyp'$~\dlabel{c1} is equivalent
to $\exists\tzar.(\ccall\ts\tzar\wedge\tzar\subtype\ttyp)
\wedge\ttyp\subtype\ttyp'$~\dlabel{c2}. By \dref{z} and
Lemma~\ref{lemma-ex-and}, \dref{c2} is equivalent to
$\exists\tzar.(\ccall\ts\tzar\wedge\tzar\subtype\ttyp
\wedge\ttyp\subtype\ttyp')$~\dlabel{c3}. Because $\tzar\subtype\ttyp
\wedge\ttyp\subtype\ttyp'\entails\tzar\subtype\ttyp'$ holds, \dref{c3}
entails $\exists\tzar.(\ccall\ts\tzar\wedge\tzar\subtype\ttyp')$, which by
\dref{z} and Lemma~\ref{lemma-instance-exists} is equivalent to
$\ccall\ts{\ttyp'}$.
\end{Proof}

The next lemma gives another interesting simplification law.
\begin{lemma}
\label{lemma-collect}
$\tvar\not\in\ftv\ttyp$ implies $\exists\tvar.(\tvar=\ttyp)\logeq\ctrue$.
\end{lemma}
%
\begin{Proof}
\demoreset Let $\tvar\not\in\ftv\ttyp$~\dlabel{x}.  The constraint
$\exists\tvar.(\tvar=\ttyp)$ may be written
$\exists\tvar.(\ttyp\subtype\tvar\wedge\tvar\subtype\ttyp)$. By \dref{x} and
Lemma~\ref{lemma-instance-exists}, it is equivalent to $\ttyp\subtype\ttyp$,
which by \Rule{CM-Predicate} is equivalent to $\ctrue$.
\end{Proof}
%
The following lemma states that, provided $\cp$ is satisfied, the type $\ttyp$
is an instance of the constrained type scheme $\scheme\tvars\cp\ttyp$.
\begin{lemma}
\label{lemma-identity-instance}
$\cp\entails\ccall{\scheme\tvars\cp\ttyp}\ttyp$.
\end{lemma}
%
\begin{Proof}
\demoreset Let $\disjoint\tzar{\ftv{\tvars,\cp,\ttyp}}$~\dlabel{z}. By
Lemma~\ref{lemma-instance-exists}, the constraint
$\ccall{\scheme\tvars\cp\ttyp}\ttyp$ is equivalent to
$\exists\tzar.(\ccall{\scheme\tvars\cp\ttyp}\tzar\wedge\tzar\subtype\ttyp)$,
which by \dref{z} and Definition~\ref{def-ts-instance} is
$\exists\tzar.(\exists\tvars.(\cp\wedge\ttyp\subtype\tzar)
\wedge\tzar\subtype\ttyp)$~\dlabel{c}. By Lemma~\ref{lemma-exists-entail} and
by congruence of entailment, \dref{c} is entailed by
$\exists\tzar.(\cp\wedge\ttyp\subtype\tzar\wedge\tzar\subtype\ttyp)$, that is,
$\exists\tzar.(\cp\wedge\tzar=\ttyp)$~\dlabel{d}. By \dref{z}, \Rule{C-ExAnd},
and Lemma~\ref{lemma-collect}, \dref{d} is equivalent to $\cp$.
\end{Proof}

This technical lemma helps justify Definition~\ref{def-csubsume} below.
%
\begin{lemma}
\label{lemma-csubsume-unambiguous}
Let $\tzar\not\in\ftv{\co,\ts,\ttyp}$. Then, 
$\co \entails \ccall\ts\ttyp$ holds if and only if
$\co\wedge\ttyp\subtype\tzar \entails \ccall\ts\tzar$ holds.
\end{lemma}
%
\begin{Proof}
\demoreset 
Assume $\co$ entails $\ccall\ts\ttyp$.  Then,
$\co\wedge\ttyp\subtype\tzar$ entails
$\ccall\ts\ttyp\wedge\ttyp\subtype\tzar$, which, by
Lemma~\ref{lemma-instance-covariant}, entails $\ccall\ts\tzar$.

Conversely, 
let $\tzar\not\in\ftv{\co,\ts,\ttyp}$~\dlabel{z} and
$\co\wedge\ttyp\subtype\tzar\entails\ccall\ts\tzar$~\dlabel{e}. By
Lemmas~\ref{lemma-entail-and} and~\ref{lemma-entail-exists}, we have
$\exists\tzar.(\co\wedge\ttyp\subtype\tzar\wedge\tzar\subtype\ttyp)
\entails\exists\tzar.(\ccall\ts\tzar\wedge\tzar\subtype\ttyp)$~\dlabel{f}.  By
Lemma~\ref{lemma-ex-and}, \dref{z}, and Lemma~\ref{lemma-collect}, the
left-hand side of \dref{f} is equivalent to $\co$. By \dref{z} and
Lemma~\ref{lemma-instance-exists}, the right-hand side of \dref{f} is
equivalent to $\ccall\ts\ttyp$. So, $\co\entails\ccall\ts\ttyp$ holds.
\end{Proof}

It is useful to define what it means for a type scheme $\ts_1$ to be \emph{more
  general} than a type scheme $\ts_2$. Our informal intent is for
$\ccall{\ts_1}{\ts_2}$ to mean: \emph{every instance of $\ts_2$ is an instance
  of $\ts_1$}. In Definition~\ref{def-ts-instance}, we have introduced the
constraint form $\ccall\ts\ttyp$ as syntactic sugar. Similarly, one might wish
to make $\ccall{\ts_1}{\ts_2}$ a derived constraint form; however, this is
impossible, because neither universal
quantification nor implication are available in the constraint language.
We can, however, exploit the fact that these logical connectives are implicit
in entailment assertions by defining a judgment of the
form $\csubsume\co{\ts_1}{\ts_2}$, whose meaning is: \emph{under the constraint
$\co$, $\ts_1$ is more general than $\ts_2$.}
%
\begin{definition}
\label{def-csubsume}
\index{instantiation(of a type scheme)} We write
$\csubsume\co{\ts_1}{\ts_2}$\indexsymfull{\ccall\cdot\cdot}{instance of} if and
only if $\tzar\not\in\ftv{\co,\ts_1,\ts_2}$ implies
$\co\wedge\ccall{\ts_2}\tzar\entails \ccall{\ts_1}\tzar$. We write
$\csubsumeq\co{\ts_1}{\ts_2}$ when both $\csubsume\co{\ts_1}{\ts_2}$ and
$\csubsume\co{\ts_2}{\ts_1}$ hold.
\end{definition}
%
This notation is not ambiguous because the assertion
$\co\entails\ccall\ts\ttyp$, whose meaning was initially given by
Definitions~\ref{def-ts-instance} and~\ref{def-entailment}, retains the same
meaning under the new definition---this is shown by
Lemma~\ref{lemma-csubsume-unambiguous} above.

The next lemma shows that quantifying over a fresh $\tzar$ in
Definition~\ref{def-csubsume} amounts to quantifying over an
arbitrary type $\ttyp$.
%
\begin{lemma}
\label{lemma-csubsume}
$\csubsume\co{\ts_1}{\ts_2}$ implies
$\co\wedge\ccall{\ts_2}\ttyp\entails\ccall{\ts_1}\ttyp$.
\end{lemma}
%
\begin{Proof}
\demoreset Assume $\tzar\not\in\ftv{\co,\ts_1,\ts_2,\ttyp}$~\dlabel{z} and
$\co\wedge\ccall{\ts_2}\tzar\entails \ccall{\ts_1}\tzar$~\dlabel{e}.
Then, we have
$$\begin{array}{rll}
         & \co\wedge\ccall{\ts_2}\ttyp \\
\logeq   & \co\wedge\exists\tzar.(\ccall{\ts_2}\tzar\wedge\tzar\subtype\ttyp)
         & \dlabel{un} \\
\logeq   & \exists\tzar.(\co\wedge\ccall{\ts_2}\tzar\wedge\tzar\subtype\ttyp)
         & \dlabel{deux} \\
\entails & \exists\tzar.(\ccall{\ts_1}\tzar\wedge\tzar\subtype\ttyp)
         & \dlabel{trois} \\
\logeq   & \ccall{\ts_1}\ttyp
         & \dlabel{quatre}
\end{array}$$
where \dref{un} is by \dref{z} and Lemma~\ref{lemma-instance-exists};
\dref{deux} is by \dref{z} and Lemma~\ref{lemma-ex-and}; \dref{trois}
follows from \dref{e}; and \dref{quatre} is by \dref{z} and
Lemma~\ref{lemma-instance-exists} again.
\end{Proof}

The next lemma provides a way of exploiting the ordering between type schemes
introduced by Definition~\ref{def-csubsume}. It states that a type scheme
occurs in \emph{contravariant} position when it is within a \kwd{def}
prefix. In other words, the more general the type scheme, the weaker the
entire constraint.
This is a key stepping stone for several forthcoming proofs.
%
\begin{lemma}
\label{lemma-subsume-env}
$\csubsume\co{\ts_1}{\ts_2}$ implies
$\co\wedge\cplet{\evid:\ts_2}\cp\entails\cplet{\evid:\ts_1}\cp$.
\end{lemma}
%
\begin{Proof}
\demoreset Assume $\csubsume\co{\ts_1}{\ts_2}$~\dlabel{hyp1}.  Let us write
$\cp$ as $\cplet{\env'}{\co'}$, where $\co'$ is not a \kwd{def} form. By
\Rule{CM-And}, the goal is to prove that $\satdef\ga\env\co$~\dlabel{hyp2} and
$\sat{\env;\evid:\ts_2;\env'}\ga{\co'}$~\dlabel{hyp3} imply
$\sat{\env;\evid:\ts_1;\env'}\ga{\co'}$. The proof is by induction on the
derivation of \dref{hyp3}.

\proofcase\Rule{CM-True}, \Rule{CM-Predicate}. Immediate.

\proofcase\Rule{CM-And}. $\co'$ is $\co'_1\wedge\co'_2$. The rule's premises
are $\sat{\env;\evid:\ts_2;\env'}\ga{\co'_1}$ and
$\sat{\env;\evid:\ts_2;\env'}\ga{\co'_2}$. The induction hypothesis yields
$\sat{\env;\evid:\ts_1;\env'}\ga{\co'_1}$ and
$\sat{\env;\evid:\ts_1;\env'}\ga{\co'_2}$. The result follows by
\Rule{CM-And}.

\proofcase\Rule{CM-Exists}. $\co'$ is $\exists\tvars.\co''$. The rule's
premises are
$\sat{\env;\evid:\ts_2;\env'}{\etend\ga\tvarc\gtypc}{\co''}$~\dlabel{p1} and
$\disjoint\tvars{\ftv{\env,\ts_2,\env'}}$~\dlabel{p2}. We may further require,
\spdg, $\disjoint\tvars{\ftv{\ts_1,\co}}$~\dlabel{w}. By \dref{p2}, \dref{w},
and Lemma~\ref{lemma-sat-free}, \dref{hyp2} implies
$\sat\env{\etend\ga\tvarc\gtypc}\co$. This allows applying the induction
hypothesis to \dref{p1}, yielding
$\sat{\env;\evid:\ts_1;\env'}{\etend\ga\tvarc\gtypc}{\co''}$.  The result
follows by \dref{p2}, \dref{w}, and \Rule{CM-Exists}.

\proofcase\Rule{CM-Instance}. $\co'$ is $\ccall\ewid\ttyp$.
We distinguish three subcases.

\proofsubcase $\ewid\in\dpv{\env'}$. The premise
is of the form $\sat{\env;\evid:\ts_2;\env'_1}\ga{\co''}$. The result
follows by the induction hypothesis and by \Rule{CM-Instance}.

\proofsubcase $\ewid\not\in\dpv{\env'}$ and $\ewid=\evid$.
The premise is $\satdef\ga\env{\ccall{\ts_2}\ttyp}$.
By \dref{hyp2} and \Rule{CM-And}, this implies
$\satdef\ga\env{({\co \wedge \ccall{\ts_2}\ttyp})}$.
By \dref{hyp1} and Lemma~\ref{lemma-csubsume}, this
implies $\satdef\ga\env{\ccall{\ts_1}\ttyp}$. The result follows
by \Rule{CM-Instance}.

\proofsubcase $\ewid\not\in\dpv{\env'}$ and $\ewid\not=\evid$.
Then, both \dref{hyp3} and the goal are equivalent to
$\satdef\ga\env{\co'}$.
\end{Proof}
%
The following exercise generalizes this result to \kwd{let} forms.
%
\begin{exercise}[\Easy, \nosolution]
\label{exercise-instance}
Prove that $\tzar\not\in\ftv\ts$ implies
$\exists\ts\logeq\exists\tzar.\ccall\ts\tzar$. Explain why, as a result,
$\csubsume\co{\ts_1}{\ts_2}$ implies
$\co\wedge\exists\ts_2\entails\exists\ts_1$. Use this fact to prove that
$\csubsume\co{\ts_1}{\ts_2}$ implies
$\co\wedge\cxlet{\evid:\ts_2}\cp\entails\cxlet{\evid:\ts_1}\cp$.
\end{exercise}
%
The following lemma tells how to compare type schemes that differ only in
their constraints.
%
\begin{lemma}
\label{lemma-subsume-constraint}
Let $\ts_1$ and $\ts_2$ stand for $\scheme\tvars{\co_1}\ttyp$ and
$\scheme\tvars{\co_2}\ttyp$, respectively. Then,
$\disjoint\tvars{\ftv\co}$ and
$\co\wedge\co_2\entails\co_1$ imply
$\csubsume\co{\ts_1}{\ts_2}$.
\end{lemma}
%
\begin{Proof}
\demoreset Assume $\disjoint\tvars{\ftv\co}$~\dlabel{x} and
$\co\wedge\co_2\entails\co_1$~\dlabel{e}. Let
$\tzar\not\in\tvars$~\dlabel{z}. Then, we have
$$\begin{array}{rll}
         & \co\wedge\ccall{\ts_2}\tzar \\
\logeq   & \co\wedge\exists\tvars.(\co_2\wedge\ttyp\subtype\tzar) 
         & \dlabel{un} \\
\logeq   & \exists\tvars.(\co\wedge\co_2\wedge\ttyp\subtype\tzar) 
         & \dlabel{deux} \\
\entails & \exists\tvars.(\co_1\wedge\ttyp\subtype\tzar) 
         & \dlabel{trois} \\
\logeq   & \ccall{\ts_1}\tzar
         & \dlabel{quatre}
\end{array}$$
where \dref{un} is by \dref{z} and Definition~\ref{def-ts-instance};
\dref{deux} is by \dref{x} and Lemma~\ref{lemma-ex-and}; \dref{trois} follows
from \dref{e}; \dref{quatre} is by \dref{z} and
Definition~\ref{def-ts-instance} again.  By Definition~\ref{def-csubsume}, we
have established $\csubsume\co{\ts_1}{\ts_2}$.
\end{Proof}

We now establish that entailment is preserved by arbitrary constraint
contexts. As a result, constraint equivalence is a congruence. In what
follows, these facts are often used implicitly.
%
\begin{theorem}[Congruence]
%\label{theorem-entailment-congruence}
$\co_1\entails\co_2$ implies
$\ccon[\co_1]\entails\ccon[\co_2]$.
\end{theorem}
%
\begin{Proof}
By induction on the structure of $\ccon$.

\proofcase$\chole$. The goal is the hypothesis $\co_1\entails\co_2$.

\proofcase $\co$. The goal is $\co\entails\co$, a tautology.

\proofcase $\ccon_1\wedge\ccon_2$. By the induction hypothesis
and lemma~\ref{lemma-entail-and}.

\proofcase $\exists\tvars.\ccon_1$. By the induction hypothesis
and lemma~\ref{lemma-entail-exists}.

\proofcase $\cplet{\evid:\ts}{\ccon_1}$. By the induction
hypothesis and using the fact that---by definition---entailment is
preserved by all \kwd{def} contexts.

\proofcase $\cplet{\scheme\tvars{\ccon_1}\ttyp}\co$. 
By the induction hypothesis and by Lemmas~\ref{lemma-subsume-constraint}
and~\ref{lemma-subsume-env}.
\end{Proof}

We proceed with a number of simple properties of \kwd{def} constraints.
%
\begin{lemma}
\label{lemma-sat-fpv}
$\disjoint{\dpv\env}{\fpv\co}$ implies $\cplet\env\co\logeq\co$.
\end{lemma}
% TEMPORARY preuve?
%
\begin{lemma}
\label{lemma-def-and}
$\cplet\env{(\co_1\wedge\co_2)} \logeq
(\cplet\env{\co_1})\wedge(\cplet\env{\co_2})$.
\end{lemma}
%
\begin{Proof}
By \Rule{CM-And}, the assertion $\satdef\ga{\env';\env}{(\co_1\wedge\co_2)}$
is equivalent to the conjunction of $\satdef\ga{\env';\env}{\co_1}$ and
$\satdef\ga{\env';\env}{\co_2}$. By \Rule{CM-And} again, so is the assertion
$\satdef\ga{\env'}{((\cplet\env{\co_1})\wedge(\cplet\env{\co_2}))}$.
\end{Proof}
%
\begin{lemma}
\label{lemma-def-let}
$\cxlet\env\co\logeq\exists\env\wedge\cplet\env\co$.
\end{lemma}
%
\begin{Proof}
The proof is by induction on the structure of $\env$.

\proofcase$\noenv$. The goal is $\co\logeq\ctrue\wedge\co$, a consequence of
Lemma~\ref{lemma-dup}.

\demoreset\proofcase$\env;\evid:\ts$. The goal is to show that
$\cxlet{\env;\evid:\ts}\co$~\dlabel{left} and $\exists(\env;\evid:\ts) \wedge
\cplet{\env;\evid:\ts}\co$~\dlabel{right} are equivalent. By definition,
\dref{left} is $\cxlet\env{(\exists\ts\wedge\cplet{\evid:\ts}\co)}$, which, by
the induction hypothesis, is equivalent to $\exists\env\wedge
\cplet\env{(\exists\ts\wedge\cplet{\evid:\ts}\co)}$~\dlabel{c}.  By
Lemma~\ref{lemma-def-and}, \dref{c} is
$\exists\env\wedge\cplet\env{\exists\ts}\wedge \cplet{\env;\evid:\ts}\co$,
which by definition is \dref{right}.
\end{Proof}

The next lemma states that, modulo equivalence, the only constraint that
constrains $\evid$ without explicitly referring to it is $\cfalse$.
%
\begin{lemma}
\label{lemma-entail-fpv}
$\co\entails\ccall\evid\ttyp$ and $\evid\not\in\fpv\co$ imply
$\co\logeq\cfalse$.
\end{lemma}
%
\begin{Proof}
\demoreset Let $\co\entails\ccall\evid\ttyp$~\dlabel{e} and
$\evid\not\in\fpv\co$~\dlabel{x}. Let $\ts$ stand for
$\scheme\tvar\cfalse\tvar$. By \dref{e} and by congruence of entailment, we
have
$\cplet{\evid:\ts}\co\entails\cplet{\evid:\ts}{\ccall\evid\ttyp}$~\dlabel{j}.
By \dref{x} and Lemma~\ref{lemma-sat-fpv}, the left-hand side is equivalent
to $\co$. By \Rule{CM-Instance}, the right-hand side is equivalent to
$\ccall\ts\ttyp$, which by definition of $\ts$ is equivalent to $\cfalse$.
Thus, \dref{j} may be read $\co\entails\cfalse$.
\end{Proof}

The following definition and lemmas exploit the fact that the \kwd{def} form
is an explicit substitution form and show how to evaluate it, that is, how to
push it down through a context. In other words, they show that, modulo a few
side-conditions, a context of the form $\cplet\env\chole$ commutes with an
arbitrary context $\ccon$.

\begin{definition}
\label{def-pushcon}
Given an environment $\env$ and a type scheme $\ts=\scheme\tvars\co\ttyp$ such
that $\disjoint\tvars{\ftv\env}$, let $\pushcon\env\ts$ stand for
$\scheme\tvars{\cplet\env\co}\ttyp$. Given an environment $\env$ and a context
$\ccon$ such that $\disjoint{\dpv\ccon}{\dfpv\env}$ and
$\disjoint{\dtv\ccon}{\ftv\env}$, define the context $\pushcon\env\ccon$ as
follows:
$$\begin{array}{rcl}
\pushcon\env\chole & = & \chole \\
\pushcon\env\co & = & \cplet\env\co \\
\pushcon\env{(\ccon_1\wedge\ccon_2)} & = &
(\pushcon\env{\ccon_1})\wedge(\pushcon\env{\ccon_2}) \\
\pushcon\env{(\exists\tvars.\ccon)} & = &
\exists\tvars.(\pushcon\env\ccon) \\
\pushcon\env{(\cplet{\evid:\ts}\ccon)} & = &
\cplet{\evid:(\pushcon\env\ts)}{(\pushcon\env\ccon)} \\
\pushcon\env{(\cplet{\evid:\scheme\tvars\ccon\ttyp}\co)} & = &
\cplet{\evid:\scheme\tvars{\pushcon\env\ccon}\ttyp;\env}\co
\end{array}$$
\end{definition}

\begin{lemma}
\label{lemma-pushcon-scheme}
$\cplet\env{\ccall\ts\ttyp'} \logeq
\ccall{(\pushcon\env\ts)}{\ttyp'}$.
\end{lemma}
%
\begin{Proof}
Let $\ts=\scheme\tvars\co\ttyp$, where $\disjoint\tvars{\ftv{\env,\ttyp'}}$.
By Definitions~\ref{def-ts-instance} and~\ref{def-pushcon}, the goal is
$\cplet\env{\exists\tvars.(\co\wedge\ttyp\subtype\ttyp')} \logeq
\exists\tvars.((\cplet\env\co)\wedge\ttyp\subtype\ttyp')$, whose validity may
be viewed as a consequence of Lemmas~\ref{lemma-def-ex},~\ref{lemma-def-and},
and~\ref{lemma-sat-fpv}.
\end{Proof}

\begin{lemma}
\label{lemma-pushcon-scheme-inductive}
$\evid\not\in\dfpv\env$ 
implies
$\cplet{\env;\evid:\ts}\co \logeq \cplet{\evid:(\pushcon\env\ts);\env}\co$.
\end{lemma}

\begin{lemma}
\label{lemma-pushcon}
$\disjoint{\dpv\ccon}{\dfpv\env}$ and
$\disjoint{\dtv\ccon}{\ftv\env}$ imply
$\cplet\env{\ccon[\co]} \logeq
(\pushcon\env\ccon)[\cplet\env\co]$.
\end{lemma}
%
\begin{Proof}
By induction on the structure of $\ccon$.

\proofcases $\ccon=\chole$, $\ccon=\co_1$. Immediate.

\proofcase $\ccon=\ccon_1\wedge\ccon_2$. By the induction hypothesis
and by Lemma~\ref{lemma-def-and}.

\proofcase $\ccon=\exists\tvars.\ccon_1$. By the induction hypothesis and by
Lemma~\ref{lemma-def-ex}.

\proofcases $\ccon=\cplet{\evid:\ts}{\ccon_1}$ and
$\ccon=\cplet{\evid:\scheme\tvars{\ccon_1}\ttyp}{\co'}$. By the induction
hypothesis and by Lemma~\ref{lemma-pushcon-scheme-inductive}.
%
% Voici le détail du dernier cas. Il est un peu difficile à faire de tête mais
% en principe pas plus dur que les autres.
%
% \demoreset\proofcase
% $\ccon=\cplet{\evid:\scheme\tvars{\ccon_1}\ttyp}{\co'}$.  By the induction
% hypothesis, the constraints $\cplet\env{\ccon_1[\co]}$ and
% $(\pushcon\env{\ccon_1})[\cplet\env\co]$ are equivalent. By congruence of
% entailment, the constraints
% $\cplet{\evid:\scheme\tvars{\cplet\env{\ccon_1[\co]}}\ttyp;\env}{\co'}
% $~\dlabel{une} and $\cplet{\evid:
%   \scheme\tvars{(\pushcon\env{\ccon_1})[\cplet\env\co]}\ttyp;\env}
% {\co'}$~\dlabel{deux} are equivalent as well.  By
% lemma~\ref{lemma-pushcon-scheme-inductive}, \dref{une} is none other than
% $\cplet{\env;\evid:\scheme\tvars{\ccon_1[\co]}\ttyp}{\co'}$, that is,
% $\cplet\env{\ccon[\co]}$. On the other hand, \dref{deux} is precisely
% $(\pushcon\env\ccon)[\cplet\env\co]$.
\end{Proof}

The following lemma states that the more universal quantifiers are present,
the more general the type scheme.
%
\begin{lemma}
\label{lemma-younger-entail}
$\cxlet{\evid:\scheme\tvars{\co_1}\ttyp}{\co_2} \entails
\cxlet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2}$.
\end{lemma}
%
\begin{Proof}
\demoreset Let us first prove that
$\csubsume\ctrue{\scheme{\tvars\twars}\co\ttyp}
{\scheme\tvars\co\ttyp}$~\dlabel{but} holds. Let
$\disjoint\tzar{\tvars\twars}$~\dlabel{z}. By \dref{z} and
Definition~\ref{def-ts-instance},
$\ccall{\scheme\tvars\co\ttyp}\tzar$ is
$\exists\tvars.(\co\wedge\ttyp\subtype\tzar)$~\dlabel{c}. By
lemmas~\ref{lemma-exists-entail} and~\ref{lemma-ex-ex}, \dref{c} entails
$\exists\tvars\twars.(\co\wedge\ttyp\subtype\tzar)$, which by \dref{z} and
Definition~\ref{def-ts-instance} is
$\ccall{\scheme{\tvars\twars}\co\ttyp}\tzar$.
By Definition~\ref{def-csubsume}, this proves \dref{but}.
%
Now, Lemmas~\ref{lemma-exists-entail} and~\ref{lemma-ex-ex} yield
$\exists\tvars.\co\entails\exists\tvars\twars.\co$~\dlabel{ex}.
Furthermore, by \dref{but} and Lemma~\ref{lemma-subsume-env}, we obtain
$\cplet{\evid:\scheme\tvars{\co_1}\ttyp}{\co_2} \entails
 \cplet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2}$~\dlabel{def}.
The result follows from \dref{ex} and \dref{def} by definition of the
\kwd{let} form.
\end{Proof}

Conversely, and perhaps surprisingly, it is sometimes possible to
\emph{remove} some type variables from the universal quantifier prefix of a
type scheme without compromising its generality. This is the case when the
value of these type variables is determined in a unique way. 
\end{full}
\begin{short}
Next, we define what it means for a constraint to {determine} a set of type
variables.
\end{short}
%
In brief, $\co$ {determines} $\twars$ if and only if, given a ground
assignment for $\ftv\co\setminus\twars$ and given that $\co$ holds, it is
possible to reconstruct, in a unique way, a ground assignment for $\twars$.
Determinacy appears in the equivalence law \Rule{C-LetAll} on
page~\pageref{fig:reasoning-C} and is exploited by the constraint solver in
\S\ref{section-solver}.
%
\begin{definition}
\label{def-determines}
$\co$ \emph{determines} $\twars$ if and only if, for every environment $\env$,
two ground assignments that satisfy $\cplet\env\co$ and that coincide outside
$\twars$ must coincide on $\twars$ as well.
\end{definition}

\begin{full}
The following lemma gives an abstract consequence of determinacy.
%
\begin{lemma}
\label{lemma-abstract-determinacy}
If $\co$ determines $\twars$, then
$\cp\entails\co$ implies $\co\wedge\exists\twars.\cp\entails\cp$.
\end{lemma}
%
\begin{Proof}
\demoreset Assume $\co$ determines $\twars$~\dlabel{det} and
$\cp\entails\co$~\dlabel{e}. Let $\disjoint\twars{\ftv\env}$~\dlabel{dj} and
$\satdef\ga\env{(\co\wedge\exists\twars.\cp)}$~\dlabel{h}. By \Rule{CM-And},
\dref{dj}, and \Rule{CM-Exists}, \dref{h} implies
$\satdef\ga\env\co$~\dlabel{satc} and
$\satdef{\etend\ga\twarc\gtypc}\env\cp$~\dlabel{satd}, for some ground types
$\gtypc$. By definition of entailment, \dref{satd} and \dref{e} imply
$\satdef{\etend\ga\twarc\gtypc}\env\co$~\dlabel{satcp}. By definition of
determinacy, \dref{det}, \dref{satc}, and \dref{satcp} imply
$\ga=\etend\ga\twarc\gtypc$. Thus, \dref{satd} may be read
$\satdef\ga\env\cp$~\dlabel{satdp}. We have shown that, assuming \dref{dj},
\dref{h} implies \dref{satdp}; by Lemma~\ref{lemma-gamma}, this proves that
$\co\wedge\exists\twars.\cp$ entails $\cp$.
\end{Proof}

Exploiting the notion of determinacy, the next lemma states when it is safe to
take type variables out of a universal quantifier prefix.
%
\begin{lemma}
\label{lemma-older-ok}
$\exists\tvars.\co_1$ determines $\twars$ and
$\disjoint\twars{\ftv{\co_2}}$ imply
$\cxlet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2} \entails
\exists\twars.\cxlet{\evid:\scheme{\tvars}{\co_1}\ttyp}{\co_2}$.
\end{lemma}
%
\begin{Proof}
\newcommand{\eco}{{\cexists\tvars{\co_1}}}
%
\demoreset Assume $\eco$ determines $\twars$~\dlabel{hyp} and
$\disjoint\twars{\ftv{\co_2}}$~\dlabel{y}. Let
$\tzar\not\in\tvars\twars$~\dlabel{z}. By \dref{z} and
Definition~\ref{def-ts-instance}, the constraint
$\eco\wedge\ccall{\scheme{\tvars\twars}{\co_1}\ttyp}\tzar$~\dlabel{c1} is
$\eco\wedge\exists\tvars\twars.(\co_1\wedge\ttyp\subtype\tzar)$~\dlabel{c2}.
By lemma~\ref{lemma-ex-ex}, \dref{c2} may be written $\eco\wedge
\exists\twars.\exists\tvars.(\co_1\wedge\ttyp\subtype\tzar)$~\dlabel{c3}.
Now, because $\exists\tvars.(\co_1\wedge\ttyp\subtype\tzar)$ entails $\eco$,
\dref{hyp} and Lemma~\ref{lemma-abstract-determinacy} imply that \dref{c3}
entails $\exists\tvars.(\co_1\wedge\ttyp\subtype\tzar)$, which by \dref{z} and
Definition~\ref{def-ts-instance} means
$\ccall{\scheme\tvars{\co_1}\ttyp}\tzar$~\dlabel{c4}. Thus, \dref{c1}
entails \dref{c4}: by Definition~\ref{def-csubsume}, this establishes
$\csubsume\eco
{\scheme\tvars{\co_1}\ttyp}{\scheme{\tvars\twars}{\co_1}\ttyp}$~\dlabel{sub}.
By Lemma~\ref{lemma-subsume-env}, \dref{sub} implies
$\exists\tvars.\co_1\wedge
\cplet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2}\entails
\cplet{\evid:\scheme\tvars{\co_1}\ttyp}{\co_2}$~\dlabel{def}. Let us now place
\dref{def} within the context $\exists\twars.(\eco\wedge\chole)$.  On the
left-hand side, where the conjunct $\eco$ is redundant, we obtain
$\exists\twars.(\exists\tvars.\co_1\wedge
\cplet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2})$, which by \dref{y},
Lemma~\ref{lemma-ex-and}, and Lemma~\ref{lemma-ex-ex}, is equivalent to
$\exists\tvars\twars.\co_1\wedge
\cplet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2}$, that is,
$\cxlet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2}$~\dlabel{left}.  On
the right-hand side, we obtain
$\exists\twars.\cxlet{\evid:\scheme\tvars{\co_1}\ttyp}{\co_2}$~\dlabel{right}.
Thus, \dref{left} entails \dref{right}.
\end{Proof}
\end{full}

\begin{widefigure}
\TTtoprule
\vspace*{-1ex}
%\newcommand{\cacher}[1]{\hbox to 0pt{$#1$\hfilneg}}
\newcommand{\sideand}{\text{\ and }}%
% \newcommand{\regleeq}[4]{%
% \global\let \foo \relax
% \ifx #2\empty\else
% \setbox0=\hbox{\footnotesize \sloppy If $#2$}%
% \ifdim \wd0<0.2\linewidth\hbox{\unhbox0}\else 
% \global\let\foo\empty \rlap{\vbox {\vskip 2mm\hbox{\unhbox0}}}\\[1mm]\fi%
% \fi
% &&                 #3 & \logeq & #4 & (\DefRule{#1})
% \\[2mm]%
% \ifx \foo \relax \else \noalign{\vskip 2mm}\fi
% }
\newcommand{\regleeq}[4]{%
\global\let \foo \relax
&&                 #3 & \logeq & #4 &
\ifthenelse{\equal{#2}{}}{& (\DefRule{#1})\\[2mm]}
{\setbox0=\hbox{\footnotesize \sloppy if $#2$}%
\ifdim \wd0<0.2\linewidth \global\let\foo\empty\fi
\ifthenelse{\equal{\foo}{\empty}}{\hbox{\unhbox0} & (\DefRule{#1})\\[1mm]}
{ &(\DefRule{#1})\\[-1mm]
&&&&\rlap{\vbox {\vskip 2mm\hbox{\footnotesize \sloppy if $#2$}}}\\[1mm]}
}
\ifx \foo \relax \else \noalign{\vskip 2mm}\fi
}%
$$\begin{tabular*}{\linewidth}{.L:l,RCL.l:R:}
\regleeq{C-And}
  {}
  {\co_1\wedge\co_2}
  {\co_2\wedge\co_1}

\regleeq{C-AndAnd}
  {}
  {(\co_1\wedge\co_2)\wedge\co_3}
  {\co_1\wedge(\co_2\wedge\co_3)}

\regleeq{C-Dup}
  {\co_1\entails\co_2}
  {\co_1\wedge\co_2}
  {\co_1}

\regleeq{C-ExEx}
  {}
  {\exists\tvars.\exists\twars.\co}
  {\exists\tvars\twars.\co}

\regleeq{C-Ex*}
  {\disjoint\tvars{\ftv\co}}
  {\exists\tvars.\co}
  {\co}

\regleeq{C-ExAnd}
  {\disjoint\tvars{\ftv{\co_2}}}
  {(\exists\tvars.\co_1) \wedge \co_2}
  {\exists\tvars.(\co_1 \wedge \co_2)}

\regleeq{C-ExTrans}
  {\tzar\not\in\ftv{\ts,\ttyp}}
  {\exists\tzar.(\ccall\ts\tzar\wedge\tzar\subtype\ttyp)}
  {\ccall\ts{\ttyp}}

\regleeq{C-InId}
{
  \evid\not\in\dpv\ccon \sideand
  \disjoint{\dtv\ccon}{\ftv\ts} \sideand
  \disjoint{\{\evid\}\cup\dpv\ccon}{\fpv\ts}
}
  {\cxlet{\evid:\ts}{\ccon[\ccall\evid{\ttyp}]}}
  {\cxlet{\evid:\ts}{\ccon[\ccall\ts{\ttyp}]}}

\regleeq{C-In*}
  {\disjoint{\dpv\env}{\fpv\co}}
  {\cxlet\env\co}
  {\exists\env\wedge\co}

\regleeq{C-InAnd}
  {}
  {\cxlet\env{(\co_1\wedge\co_2)}}
  {(\cxlet\env{\co_1})\wedge(\cxlet\env{\co_2})}

\regleeq{C-InAnd*}
  {\disjoint{\dpv\env}{\fpv{\co_2}}}
  {\cxlet\env{(\co_1\wedge\co_2)}}
  {(\cxlet\env{\co_1})\wedge\co_2}

\regleeq{C-InEx}
  {\disjoint\tvars{\ftv\env}}
  {\cxlet\env{\exists\tvars.\co}}
  {\exists\tvars.\cxlet\env\co}

\regleeq{C-LetLet}
{
  \disjoint{\dpv{\env_1}}{\dpv{\env_2}} \sideand
  \disjoint{\dpv{\env_2}}{\fpv{\env_1}} \sideand
  \disjoint{\dpv{\env_1}}{\fpv{\env_2}}
}
  {\cxlet{\env_1;\env_2}\co}
  {\cxlet{\env_2;\env_1}\co}

\regleeq{C-LetAnd}
  {\disjoint\tvars{\ftv{\co_1}}}
  {\cxlet{\evid:\scheme\tvars{\co_1\wedge\co_2}\ttyp}{\co_3}}
  {\co_1\wedge\cxlet{\evid:\scheme\tvars{\co_2}\ttyp}{\co_3}}

\regleeq{C-LetDup}
  {\disjoint\tvars{\ftv\env} \sideand \disjoint{\dpv\env}{\fpv\env}}
  {\cxlet{\env;\evid:\scheme\tvars{\co_1}\ttyp}{\co_2}}
  {\cxlet{\env;\evid:\scheme\tvars{\cxlet\env{\co_1}}\ttyp}{\co_2}}

\regleeq{C-LetEx}
  {\disjoint\twars{\ftv\ttyp}}
  {\cxlet{\evid:\scheme\tvars{\exists\twars.\co_1}\ttyp}{\co_2}}
  {\cxlet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2}}

\regleeq{C-LetAll}
  {\disjoint\twars{\ftv{\co_2}} \sideand
   \text{$\exists\tvars.\co_1$ determines $\twars$}}
  {\cxlet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2}}
  {\exists\twars.\cxlet{\evid:\scheme\tvars{\co_1}\ttyp}{\co_2}}

\regleeq{C-LetSub}
  {\tvar\not\in\ftv{\ttyp,\co}}
  {\exists\tvar.(
     \ttyp\subtype\tvar \wedge
     \cxlet{\evid:\tvar}\co
   )}
  {\cxlet{\evid:\ttyp}\co}

\regleeq{C-Eq}
  {}
  {\tvarc=\ttypc \wedge \subst\tvarc\ttypc\co}
  {\tvarc=\ttypc\wedge\co}

\regleeq{C-Name}
  {\disjoint\tvars{\ftv\ttyps}}
  {\ctrue}
  {\exists\tvars.(\tvarc=\ttypc)}

\regleeq{C-NameEq}
  {\disjoint\tvars{\ftv\ttyps}}
  {\subst\tvarc\ttypc\co}
  {\exists\tvars.(\tvarc=\ttypc\wedge\co)}
\end{tabular*}$$ \\
\vspace*{-2ex} 
\TTbottomrule \\
\vspace*{-2ex}
\bcpcaption{reasoning-C}{Constraint equivalence laws}
\end{widefigure}

We now give a toolbox of constraint equivalence laws. It is worth noting that
they do not form a complete axiomatization of constraint equivalence; in
fact, they cannot, since the syntax and meaning of constraints is partly
unspecified.
%
\begin{theorem}
%\label{theorem-constraint-equivalences}
All equivalence laws in Figure~\ref{fig:reasoning-C} hold.
\end{theorem}
\begin{full}
\begin{Proof}
By examination of every law.

\proofcases\Rule{C-And}, \Rule{C-AndAnd}. By Lemma~\ref{lemma-conjunction}.

\proofcase\Rule{C-Dup}. By Lemma~\ref{lemma-dup}.

\proofcase\Rule{C-ExEx}. By Lemma~\ref{lemma-ex-ex}.

\demoreset\proofcase\Rule{C-Ex*}. Let $\disjoint\tvars{\ftv\co}$~\dlabel{x}
and $\disjoint\tvars{\ftv\env}$~\dlabel{j}. By \Rule{CM-Exists},
$\satdef\ga\env{\exists\tvars.\co}$~\dlabel{a} is equivalent to
$\exists\gtypc\quad\satdef{\etend\ga\tvarc\gtypc}\env\co$, which, by
\dref{x} and Lemma~\ref{lemma-sat-free}, is equivalent to
$\exists\gtypc\quad\satdef\ga\env\co$~\dlabel{b}. Because every
$\modelk\kind$ is nonempty (Definition~\ref{def-model}), \dref{b} may be
written
$\satdef\ga\env\co$~\dlabel{c}. By Lemma~\ref{lemma-gamma}, % forme faible
the equivalence of \dref{a} and \dref{c} under \dref{j} establishes
$\exists\tvars.\co\logeq\co$.

\proofcase\Rule{C-ExAnd}. By Lemma~\ref{lemma-ex-and}.

\proofcase\Rule{C-ExTrans}. By Lemma~\ref{lemma-instance-exists}.

\demoreset\proofcase\Rule{C-InId}. Assume
$\disjoint{\dtv\ccon}{\ftv\ts}$~\dlabel{h1} and
$\disjoint{\dpv\ccon}{\{\evid\}\cup\fpv\ts}$~\dlabel{h2} and
$\evid\not\in\fpv\ts$~\dlabel{h3}. By \dref{h1}, \dref{h2}, and
Lemma~\ref{lemma-pushcon},
$\cplet{\evid:\ts}{\ccon[\ccall\evid{\ttyp'}]}$~\dlabel{c} is equivalent to
$(\pushcon{(\evid:\ts)}\ccon)
[\cplet{\evid:\ts}{\ccall\evid{\ttyp'}}]$~\dlabel{d}.  Now, by
\Rule{CM-Instance}, $\cplet{\evid:\ts}{\ccall\evid{\ttyp'}}$ is equivalent to
$\ccall\ts{\ttyp'}$, which, by \dref{h3} and Lemma~\ref{lemma-sat-fpv}, is
equivalent to $\cplet{\evid:\ts}{\ccall\ts{\ttyp'}}$.  Thus, \dref{d} may be
written $(\pushcon{(\evid:\ts)}\ccon) [\cplet{\evid:\ts}{\ccall\ts{\ttyp'}}]$,
which, by Lemma~\ref{lemma-pushcon} again, is
$\cplet{\evid:\ts}{\ccon[\ccall\ts{\ttyp'}]}$~\dlabel{e}. Placing the
assertion $\dref{c}\logeq
\dref{e}$ within the context $\exists\ts\wedge\chole$ yields the result.

\proofcase\Rule{C-In*}. By Lemmas~\ref{lemma-def-let} and~\ref{lemma-sat-fpv}.

\proofcase\Rule{C-InAnd}. By Lemma~\ref{lemma-def-and}, we have
$\cplet{\evid:\ts}{(\co_1\wedge\co_2)} \logeq
(\cplet{\evid:\ts}{\co_1})\wedge(\cplet{\evid:\ts}{\co_2})$.
Placing this assertion within the context
$\exists\ts\wedge\chole$ and applying \Rule{C-Dup} to the right-hand side
yields
$\cxlet{\evid:\ts}{(\co_1\wedge\co_2)} \logeq
 (\cxlet{\evid:\ts}{\co_1})\wedge(\cxlet{\evid:\ts}{\co_2})$. The
result follows by structural induction on $\env$.

\proofcase\Rule{C-InAnd*}. By
\Rule{C-InAnd}, \Rule{C-In*} and \Rule{C-Dup}.

\demoreset\proofcase\Rule{C-InEx}. Let $\disjoint\tvars{\ftv\ts}$~\dlabel{x}.
The constraint $\cxlet{\evid:\ts}{\exists\tvars.\co}$ is short for $\exists\ts
\wedge \cplet{\evid:\ts}{\exists\tvars.\co}$. By \dref{x},
Lemma~\ref{lemma-def-ex}, and \Rule{C-ExAnd}, this is
$\exists\tvars.(\exists\ts\wedge\cplet{\evid:\ts}\co)$, that is,
$\exists\tvars.\cxlet{\evid:\ts}\co$. The result follows by induction on the
structure of $\env$.

\demoreset\proofcase\Rule{C-LetLet}. Let $\evid_1\not=\evid_2$~\dlabel{diff}
and $\evid_1\not\in\fpv{\ts_2}$~\dlabel{x1} and
$\evid_2\not\in\fpv{\ts_1}$~\dlabel{x2}. By \dref{diff}, \dref{x2}, and
Lemma~\ref{lemma-pushcon},
$\cplet{\evid_1:\ts_1;\evid_2:\ts_2}\co$~\dlabel{c1} is equivalent to
$\cplet{\evid_2:\pushcon{(\evid_1:\ts_1)}{\ts_2};\evid_1:\ts_1}\co$, which by
\dref{x1} and Lemma~\ref{lemma-sat-fpv} is
$\cplet{\evid_2:\ts_2;\evid_1:\ts_1}\co$~\dlabel{c2}. Furthermore,
\dref{x1}, \dref{x2}, and Lemma~\ref{lemma-sat-fpv} yield
$\cplet{\evid_1:\ts_1}{\exists\ts_2}\logeq\exists\ts_2$~\dlabel{eq2} and
$\cplet{\evid_2:\ts_2}{\exists\ts_1}\logeq\exists\ts_1$~\dlabel{eq1}.
Placing the assertion
$\dref{c1}\logeq\dref{c2}$ within the context
$\exists\ts_1\wedge\exists\ts_2\wedge\chole$ and using \dref{eq2}, \dref{eq1},
and Lemma~\ref{lemma-def-let}, we obtain
$\cxlet{\evid_1:\ts_1;\evid_2:\ts_2}\co \logeq
\cxlet{\evid_2:\ts_2;\evid_1:\ts_1}\co$. The result follows by induction
on the structure of $\env_1$ and $\env_2$.

\demoreset\proofcase\Rule{C-LetAnd}. Let
$\disjoint\tvars{\ftv{\co_1}}$~\dlabel{h}.  By
\dref{h} and Lemma~\ref{lemma-subsume-constraint}, we have
$\csubsumeq{\co_1}{\scheme\tvars{\co_1\wedge\co_2}\ttyp}
{\scheme\tvars{\co_2}\ttyp}$. By Lemma~\ref{lemma-subsume-env},
this implies
$\co_1\wedge\cplet{\evid:\scheme\tvars{\co_1\wedge\co_2}\ttyp}{\co_3} \logeq
\co_1\wedge\cplet{\evid:\scheme\tvars{\co_2}\ttyp}{\co_3}$. 
Placing this assertion within the context $\exists\tvars.\co_2\wedge\chole$
and applying \Rule{C-ExAnd} to the left-hand side yields the result.

\demoreset\proofcase\Rule{C-LetDup}. Let
$\disjoint\tvars{\ftv\env}$~\dlabel{un} and
$\disjoint{\dpv\env}{\fpv\env}$~\dlabel{auto}. By \dref{un} and
Lemma~\ref{lemma-pushcon},
$\cplet\env{\cxlet{\evid:\scheme\tvars{\co_1}\ttyp}{\co_2}}$~\dlabel{c1} is
equivalent to $\cxlet{\evid:\scheme\tvars{\cplet\env{\co_1}}\ttyp}
{\cplet\env{\co_2}}$~\dlabel{c2}. Now, by \dref{auto} and
Lemma~\ref{lemma-sat-fpv}, we have
$\cplet\env{\co_1}\logeq\cplet{\env;\env}{\co_1}$~\dlabel{h}. Using \dref{h},
then applying Lemma~\ref{lemma-pushcon} in the reverse direction, we find
that \dref{c2} is equivalent to
$\cplet\env{\cxlet{\evid:\scheme\tvars{\cplet\env{\co_1}}\ttyp}
  {\co_2}}$~\dlabel{c3}. Placing the assertion $\dref{c1}\logeq\dref{c3}$
within the context $\exists\env\wedge\chole$ and applying
Lemma~\ref{lemma-def-let}, we find that
$\cxlet\env{\cxlet{\evid:\scheme\tvars{\co_1}\ttyp}{\co_2}}$ is
equivalent to $\cxlet\env{\cxlet{\evid:\scheme\tvars{\cplet\env{\co_1}}\ttyp}
  {\co_2}}$~\dlabel{c4}. Last, by \Rule{C-Dup}, \dref{c4} is equivalent to
$\exists\env\wedge\dref{c4}$, which by \dref{auto} and \Rule{C-InAnd*}, then
\dref{un} and \Rule{C-LetAnd}, is $\cxlet\env{\cxlet{\evid:\scheme\tvars
    {\exists\env\wedge\cplet\env{\co_1}}\ttyp}{\co_2}}$, that is,
$\cxlet\env{\cxlet{\evid:\scheme\tvars {\cxlet\env{\co_1}}\ttyp}{\co_2}}$.

\demoreset\proofcase\Rule{C-LetEx}. Let
$\disjoint\twars{\ftv\ttyp}$~\dlabel{y}.  Let
$\tzar\not\in\tvars\twars$~\dlabel{z}. By \dref{z} and
Definition~\ref{def-ts-instance},
$\ccall{\scheme\tvars{\exists\twars.\co}\ttyp}\tzar$ is
$\exists\tvars.(\exists\twars.\co\wedge\ttyp\subtype\tzar)$. Similarly,
$\ccall{\scheme{\tvars\twars}\co\ttyp}\tzar$ is
$\exists\tvars\twars.(\co\wedge\ttyp\subtype\tzar)$. By \dref{y}, \dref{z},
Lemma~\ref{lemma-ex-and}, and Lemma~\ref{lemma-ex-ex}, these constraints are
equivalent. By Definition~\ref{def-csubsume}, this establishes
$\csubsumeq\ctrue{\scheme\tvars{\exists\twars.\co}\ttyp}
{\scheme{\tvars\twars}\co\ttyp}$.  The result follows by
Lemmas~\ref{lemma-ex-ex} and~\ref{lemma-subsume-env}.

\proofcase\Rule{C-LetAll}. Assume $\disjoint\twars{\ftv{\co_2}}$~\dlabel{j}
and $\exists\tvars.\co_1$ determines $\twars$~\dlabel{det}.
By Lemma~\ref{lemma-younger-entail},
$\cxlet{\evid:\scheme\tvars{\co_1}\ttyp}{\co_2}$ entails
$\cxlet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2}$. Placing this
assertion within the context $\exists\twars.\chole$, we find that
$\exists\twars.\cxlet{\evid:\scheme{\tvars}{\co_1}\ttyp}{\co_2}$~\dlabel{l}
entails $\exists\twars.\cxlet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2}$,
which, by \dref{j} and \Rule{C-Ex*}, is equivalent to
$\cxlet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2}$~\dlabel{r}.
Conversely, by \dref{j}, \dref{det}, and Lemma~\ref{lemma-older-ok},
\dref{r} entails \dref{l}.

\demoreset\proofcase\Rule{C-LetSub}. Let
$\tvar\not\in\ftv{\ttyp,\co}$~\dlabel{x}. As an immediate consequence of
Lemma~\ref{lemma-subsume-env}, we find that $\ttyp\subtype\tvar \wedge
\cplet{\evid:\tvar}\co$ entails $\cplet{\evid:\ttyp}\co$. By congruence of
entailment, $\exists\tvar.(\ttyp\subtype\tvar \wedge \cplet{\evid:\tvar}\co)$
entails $\exists\tvar.\cplet{\evid:\ttyp}\co$, which, by \dref{x} and
\Rule{C-Ex*}, is $\cplet{\evid:\ttyp}\co$. Conversely, by \dref{x},
Lemma~\ref{lemma-collect} and \Rule{C-ExAnd}, $\cplet{\evid:\ttyp}\co$ is
equivalent to $\exists\tvar.(\tvar=\ttyp \wedge \cplet{\evid:\ttyp}\co)$, that
is, $\exists\tvar.(\ttyp\subtype\tvar \wedge \tvar\subtype\ttyp \wedge
\cplet{\evid:\ttyp}\co)$, which, by Lemma~\ref{lemma-subsume-env}, entails
$\exists\tvar.(\ttyp\subtype\tvar \wedge \cplet{\evid:\tvar}\co)$.

\demoreset\proofcase\Rule{C-Eq}. Let
$\disjoint\tvars{\ftv\env}$~\dlabel{x}. By \Rule{CM-Predicate}, the assertion
$\satdef\ga\env{\tvarc=\ttypc}$~\dlabel{h} is equivalent to
$\ga(\tvarc)=\ga(\ttypc)$, which in turn is equivalent to
$\ga\circ\subst\tvarc\ttypc=\ga$~\dlabel{co}. Assuming \dref{co} and applying
Lemma~\ref{lemma-sat-subst}, we find that the assertion
$\satdef\ga\env\co$~\dlabel{l} may be written
$\satdef\ga{\subst\tvarc\ttypc\env}{\subst\tvarc\ttypc\co}$, which by \dref{x}
is $\satdef\ga\env{\subst\tvarc\ttypc\co}$~\dlabel{r}. Thus, \dref{h} implies
that \dref{l} and \dref{r} are equivalent. By \Rule{CM-And}, this proves
that 
$\satdef\ga\env{(\tvarc=\ttypc\wedge\co)}$ and
$\satdef\ga\env{(\tvarc=\ttypc\wedge\subst\tvarc\ttypc\co)}$ are equivalent.
Lemma~\ref{lemma-gamma} allows discharging \dref{x} and yields the goal.
% C'est la forme FORTE du lemme~\ref{lemma-gamma} qui est employée.

\proofcase\Rule{C-Name}. By Lemma~\ref{lemma-collect} and by induction on the
length of the vectors $\tvarc$ and $\ttypc$.

\proofcase\Rule{C-NameEq}. By \Rule{C-Name}, \Rule{C-Dup}, \Rule{C-ExAnd},
and \Rule{C-Eq}.
\end{Proof}
\end{full}

Let us explain. \Rule{C-And} and \Rule{C-AndAnd} state that conjunction is
commutative and associative. \Rule{C-Dup} states that redundant conjuncts may
be freely added or removed, where a conjunct is redundant if and only if it is
entailed by another conjunct. Throughout \this, these three laws are often
used implicitly. \Rule{C-ExEx} and \Rule{C-Ex*} allow grouping consecutive
existential quantifiers and suppressing redundant ones, where a quantifier is
redundant if and only if the variables bound by it do not occur free within
its scope.  \Rule{C-ExAnd} allows conjunction and existential quantification
to commute, provided no capture occurs; it is known as a \emph{scope
extrusion} law. When the rule is oriented from left to right, its
side-condition may always be satisfied by suitable \aconv. \Rule{C-ExTrans}
states that it is equivalent for a type $\ttyp$ to be an instance of $\ts$ or
to be a supertype of some instance of $\ts$.  We note that the instances of
a monotype are its supertypes, that is, by Definition~\ref {def-ts-instance},
$\ccall{\ttyp'}{\ttyp}$ and $\ttyp'\subtype\ttyp$ are equivalent. As a result,
specializing \Rule{C-ExTrans} to the case where $\ts$ is a monotype, we find
that $\ttyp'\subtype\ttyp$ is equivalent to
$\exists\tzar.(\ttyp'\subtype\tzar\wedge\tzar\subtype\ttyp)$, for fresh
$\tzar$, a standard equivalence law. When oriented from left to right, it
becomes an interesting {simplification} law: in a chain of subtyping
constraints, an intermediate variable such as $\tzar$ may be suppressed,
provided it is {local}, as witnessed by the existential quantifier
$\exists\tzar$. \Rule{C-InId} states that, within the scope of the binding
$\evid:\ts$, every {free} occurrence of $\evid$ may be safely replaced
with $\ts$. The restriction to free occurrences stems from the side-condition
$\evid\not\in\dpv\ccon$. When the rule is oriented from left to right, its
other side-conditions, which require the context $\cxlet{\evid:\ts}\ccon$ not
to capture $\ts$'s free type variables or free program identifiers, may always
be satisfied by suitable \aconv. \Rule{C-In*} complements the previous rule by
allowing redundant \kwd{let} bindings to be simplified. We note that
\Rule{C-InId} and \Rule{C-In*} provide a simple procedure for eliminating
\kwd{let} forms. \Rule{C-InAnd} states that the \kwd{let} form commutes with
conjunction; \Rule{C-InAnd*} spells out a common particular
case. \Rule{C-InEx} states that it commutes with existential
quantification. When the rule is oriented from left to right, its
side-condition may always be satisfied by suitable \aconv. \Rule{C-LetLet}
states that \kwd{let} forms may commute, provided they bind distinct program
identifiers and provided no free program identifiers are captured in the
process. \Rule{C-LetAnd} allows the conjunct $\co_1$ to be moved outside of
the constrained type scheme $\scheme\tvars{\co_1\wedge\co_2}\ttyp$, provided
it does not involve any of the universally quantified type variables
$\tvars$. When oriented from left to right, the rule yields an important
simplification law: indeed, taking an instance of $\scheme\tvars{\co_2}\ttyp$
is less expensive than taking an instance of
$\scheme\tvars{\co_1\wedge\co_2}\ttyp$, since the latter involves creating a
copy of $\co_1$, while the former does not. \Rule{C-LetDup} allows pushing a
series of \kwd{let} bindings into a constrained type scheme, provided no
capture occurs in the process.  It is not used as a simplification law but as
a tool in some proofs.  \Rule{C-LetEx} states that it does not make any
difference for a set of type variables $\twars$ to be existentially quantified
inside a constrained type scheme or part of the type scheme's universal
quantifiers. Indeed, in either case, taking an instance of the type scheme
means producing a constraint where $\twars$ is existentially quantified.
\Rule{C-LetAll} states that it is equivalent for a set of type variables $\twars$ to be
part of a type scheme's universal quantifiers or existentially bound
{outside} the \kwd{let} form, {provided} these type variables are
determined. In other words, when a type variable is sufficiently constrained,
it does not matter whether it is polymorphic or monomorphic.
\iffull{\Rule{C-LetAll} provides a restricted converse of
Lemma~\ref{lemma-younger-entail}. }Together, \Rule{C-LetEx} and \Rule{C-LetAll}
allow, in some situations, hoisting existential quantifiers out of the
{left}-hand side of a \kwd{let} form.
%
\longpage
\begin{example}
\label{example-let-left-ex}
\demoreset \Rule{C-LetAll} would be invalid without the condition that
$\exists\tvars.\co_1$ determines $\twars$. Consider, for instance, the
constraint
$\cxlet{\evid:\dmscheme\twar{\twar\arw\twar}}{(\ccall\evid{\tint\arw\tint}
\wedge\ccall\evid{\tbool\arw\tbool})}$~\dlabel{c}, where $\tint$ and $\tbool$
are incompatible nullary type constructors. By \Rule{C-InId} and
\Rule{C-In*}, 
it is equivalent to
 $\dmscheme\twar {\twar\arw\twar}\subtype\tint\arw\tint
\wedge \dmscheme\twar{\twar\arw\twar}\subtype\tbool\arw\tbool$
which, by Definition~\ref {def-ts-instance}, means
 $\exists\twar.(\twar\arw\twar\subtype\tint\arw\tint)
\wedge\exists\twar.(\twar\arw\twar\subtype\tbool\arw\tbool)$, that is,
$\ctrue$. Now, if \Rule{C-LetAll} was valid without its side-condition, then
\dref{c} would also be equivalent to
$\exists\twar.\cxlet{\evid:\twar\arw\twar}{(\ccall\evid{\tint\arw\tint}
\wedge\ccall\evid{\tbool\arw\tbool})}$, which by \Rule{C-InId} and
\Rule{C-In*} is $\exists\twar.(\twar\arw\twar\subtype\tint\arw\tint
\wedge\twar\arw\twar\subtype\tbool\arw\tbool)$. By \Rule{C-Arrow} and
\Rule{C-ExTrans}, this is $\tint=\tbool$, that is, $\cfalse$. Thus,
the law is invalid in this case. It is easy to see why: when the type scheme
$\ts$ contains a $\forall\twar$ quantifier, every instance of $\ts$ receives
its own $\exists\twar$ quantifier, making $\twar$ a distinct (local) type
variable; but when $\twar$ is not universally quantified, all instances
of $\ts$ {share} references to a single (global) type variable $\twar$.
This corresponds to the intuition that, in the former case, $\ts$ is
{polymorphic} in $\twar$, while in the latter case, it is
{monomorphic} in $\twar$. \shortfull{It is possible to prove}{Lemma~\ref{lemma-younger-entail} states} that,
when deprived of its side-condition, \Rule{C-LetAll} is only an entailment
law, that is, its right-hand side entails its left-hand side.
%
Similarly, it is in general invalid to hoist an existential
quantifier out of the left-hand side of a \kwd{let} form. To see this, one may
study the (equivalent) constraint
$\cxlet{\evid:\scheme\tvar{\exists\twar.\tvar=\twar\arw\twar}\tvar}
{(\ccall\evid{\tint\arw\tint}\wedge\ccall\evid{\tbool\arw\tbool})}$.
%
Naturally, in the above examples, the side-condition ``$\ctrue$
determines $\twar$'' does {not} hold: by Definition~\ref{def-determines},
it is equivalent to ``two ground assignments that coincide outside $\twar$
must coincide on $\twar$ as well,'' which is false when
$\modelk\normalkind$ contains two distinct elements, such as $\tint$ and
$\tbool$ here.

\demoreset There are cases, however, where the side-condition does
hold. For instance, we later prove that $\exists\tvar.\twar=\tint$ determines
$\twar$; see Lemma~\ref{lemma-determines}. As a result, \Rule{C-LetAll} states
that $\cxlet{\evid:\scheme{\tvar\twar}{\twar=\tint}{\twar\arw\tvar}}
{\co}$~\dlabel{c1} is equivalent to
$\exists\twar.\cxlet{\evid:\scheme\tvar{\twar=\tint}{\twar\arw\tvar}}
\co$~\dlabel{c2}, provided $\twar\not\in\ftv\co$. The intuition is simple:
because $\twar$ is forced to assume the value $\tint$ by the equation
$\twar=\tint$, it makes no difference whether $\twar$ is or isn't universally
quantified. By \Rule{C-LetAnd}, \dref{c2} is equivalent to
$\exists\twar.(\twar=\tint\wedge\cxlet{\evid:\dmscheme\tvar{\twar\arw\tvar}}
\co)$~\dlabel{c3}. In an efficient constraint solver, simplifying \dref{c1}
into \dref{c3} {before} using \Rule{C-InId} to eliminate the \kwd{let}
form is worthwhile, since doing so obviates the need for copying the type
variable $\twar$ and the equation $\twar=\tint$ at every free occurrence of
$\evid$ inside $\co$.
\end{example}

\Rule{C-LetSub} is the analog of an environment strengthening lemma: roughly
speaking, it states that, if a constraint holds under the assumption that
$\evid$ has type $\tvar$, where $\tvar$ is some supertype of $\ttyp$, then it
also holds under the assumption that $\evid$ has type $\ttyp$. The last three
rules deal with the equality predicate. \Rule{C-Eq} states that it is valid to
replace equals with equals; note the absence of a side-condition. When
oriented from left to right, \Rule{C-Name} allows introducing fresh names
$\tvarc$ for the types $\ttypc$. As always, $\tvarc$ stands for a vector of
{distinct} type variables; $\ttypc$ stands for a vector of the same
length of types of appropriate kind. Of course, this makes sense only if the
definition is not circular, that is, if the type variables $\tvars$ do not
occur free within the terms $\ttyps$. When oriented from right to left,
\Rule{C-Name} may be viewed as a simplification law: it allows eliminating
type variables whose value has been determined.  \Rule{C-NameEq} is a
combination of \Rule{C-Eq} and \Rule{C-Name}. It shows that applying an
idempotent substitution to a constraint $\co$ amounts to placing $\co$ within
a certain context.%
\begin{full}
This immediately yields a proof of the following fact:
%
\begin{lemma}
\label{lemma-entail-subst}
$\co\entails\cp$ implies $\unifier(\co)\entails\unifier(\cp)$.
\end{lemma}
%
\begin{Proof}
\demoreset Because every substitution is the composition of a renaming and of
an idempotent substitution, and because entailment assertions are stable under
renamings, we may assume, \spdg, that $\unifier$ is idempotent. Thus,
$\unifier$ may be written $\subst\tvarc\ttypc$, where
$\disjoint\tvars{\ftv\ttyps}$~\dlabel{f}. By congruence of entailment,
$\co\entails\cp$ implies $\exists\tvars.(\tvarc=\ttypc\wedge\co) \entails
\exists\tvars.(\tvarc=\ttypc\wedge\cp)$~\dlabel{b}. By \dref{f} and
\Rule{C-NameEq}, \dref{b} may be read $\subst\tvarc\ttypc\co \entails
\subst\tvarc\ttypc\cp$.
\end{Proof}

It is important to stress that, because the effect of a type substitution may
be emulated using equations, conjunction, and existential quantification,
there is no need ever to employ type substitutions in the definition of a
constraint-based type system---it is possible, instead, to express every
concept in terms of constraints. In \this, we follow this route, and use type
substitutions only when dealing with the type system \dm, whose historical
formulation is substitution-based.
\end{full}

So far, we have considered \kwd{def} a primitive constraint form and defined
the \kwd{let} form in terms of \kwd{def}, conjunction, and existential
quantification. The motivation for this approach was to simplify the
\ifshort{(omitted) }proofs of several constraint equivalence laws. However,
in the remainder of \this, {we work with \kwd{let} forms exclusively and
never employ the \kwd{def} construct}. This offers us an extra property: every
constraint that contains a false subconstraint must be false.
%
\begin{lemma}
\label{lemma-false}
$\ccon[\cfalse]\logeq\cfalse$.
\end{lemma}
%
\begin{full}
\begin{Proof}
\demoreset It is straightforward to check that the constraints
$\cfalse\wedge\co$, $\exists\tvars.\cfalse$, and $\cplet{\evid:\ts}\cfalse$
are false. The constraint $\cxlet{\evid:\scheme\tvars\cfalse\ttyp}\co$ entails
$\exists\tvars.\cfalse$, so it is false as well. The result follows by
induction on the structure of $\ccon$. Note that the law is invalid in the
presence of \kwd{def} forms: for instance,
$\cplet{\evid:\scheme\tvar\cfalse\tvar}\ctrue$ is equivalent to $\ctrue$.
\end{Proof}
\end{full}

% --------------------------------------------------------------------------

\subsection*{Reasoning with Constraints in an Equality-Only Syntactic Model}
\label{section-equality-only}

We have given a number of equivalence laws that are valid with respect to any
interpretation of constraints, that is, within any model. However, an
important special case is that of \emph{equality-only syntactic models}.
Indeed, in that specific setting, our constraint-based type systems are in
close correspondence with \dm. In brief, we aim to prove that every
satisfiable constraint $\co$ such that $\fpv\co=\varnothing$ admits a
\emph{canonical solved form} and to show that this notion corresponds to the
standard concept of a \emph{most general unifier}. \iffull{We also establish a
few technical properties of most general unifiers. } These results are
exploited when we relate \hmx{} with Damas and Milner's system (p.~\pageref{section-hmeq-dm}).
% in \S\ref{section-hmeq-dm}.

Thus, let us now assume that constraints are interpreted in an equality-only
syntactic model. Let us further assume that, for every kind $\kind$, (i) there
are at least \emph{two} type constructors of image kind $\kind$ and (ii) for
every type constructor $\tycon$ of image kind $\kind$, there exists
$\gtyp\in\modelk\kind$ such that $\gtyp(\epsilon)=\tycon$. We refer to models
that violate (i) or (ii) as \emph{degenerate}; one may argue that such models
are of little interest. The assumption that the model is nondegenerate is 
\shortfull
{used in the proof of Theorem~\ref{theorem-dm-encodes-hm}}
%
% via lemma \ref {lemma-true-entails} and \ref {lemma-transl-inst}
%
{used in the proof of Lemmas~\ref{lemma-true-entails}
and~\ref{lemma-entailment-is-refinement}}.
Last, throughout the present subsection
% \S\ref{section-equality-only}, 
we manipulate only
constraints that have \emph{no free program identifiers}.

\begin{full}
Under these new assumptions, the interpretation of equality coincides with its
syntax: every equation that holds in the model is in fact a syntactic
truism. The converse, of course, holds in every model.
%
% This lemma in fact holds in any nondegenerate free tree model.
%
\begin{lemma}
\label{lemma-true-entails}
If $\ctrue\entails\ttyp=\ttyp'$ holds, then $\ttyp$ and $\ttyp'$ coincide.
\end{lemma}
%
\begin{Proof}
\demoreset Let $\ctrue\entails\ttyp=\ttyp'$~\dlabel{h}. The proof is by
induction on the structure of $\ttyp$ and $\ttyp'$.
%
First, assume neither of $\ttyp$ and $\ttyp'$ is a type variable. In a free
tree model, \dref{h} implies that $\ttyp$ and $\ttyp'$ have the same head
symbol, that is, they must be of the form $\tycon\,\ttypc$ and
$\tycon\,\ttypcp$, respectively, and implies
$\ctrue\entails\ttypc=\ttypcp$. By the induction hypothesis, this implies that
$\ttypc$ and $\ttypcp$ coincide. Hence, $\ttyp$ and $\ttyp'$ coincide as well.
%
Next, assume $\ttyp$ is a type variable and $\ttyp'$ is not. By requirements
(i) and (ii) above, there exists a ground assignment that maps $\ttyp$ to a
ground type whose head symbol is \emph{not} that of $\ttyp'$. This contradicts
\dref{h}, so this case cannot arise. The case where $\ttyp'$ is a type
variable and $\ttyp$ is not is symmetric.
%
Last, assume both $\ttyp$ and $\ttyp'$ are type variables. If they are
distinct, then, by requirements (i) and (ii) above, there exists a ground
assignment that maps each of them to a distinct ground type, which contradicts
\dref{h}. So, $\ttyp$ and $\ttyp'$ must coincide.
\end{Proof}

In a syntactic model, ground types are finite trees. As a result, cyclic
equations, such as $\tvar=\tint\arw\tvar$, are false.
%
\begin{lemma}
\label{lemma-cyclic}
$\tvar\in\ftv\ttyp$ and $\ttyp\not\in\tyvarset$ imply
$(\tvar=\ttyp)\logeq\cfalse$.
\end{lemma}
%
\begin{Proof}
\demoreset Let $\tvar\in\ftv\ttyp$~\dlabel{h} and
$\ttyp\not\in\tyvarset$~\dlabel{h2}. By way of contradiction, assume $\ga$
satisfies $\tvar=\ttyp$, that is, $\ga(\tvar)=\ga(\ttyp)$~\dlabel{c}. In a
free tree model, \dref{h} and \dref{h2} imply that $\ga(\tvar)$ is a
\emph{strict} subtree of $\ga(\ttyp)$. Together with \dref{c}, this implies
that $\ga(\tvar)$ is a strict subtree of itself. In a syntactic model, where
trees are inductively defined, this is impossible. Thus, $\tvar=\ttyp$ is
false.
\end{Proof}
\end{full}

A \emph{solved form} is a conjunction of equations, where the left-hand sides
are {distinct} type variables that do not appear in the right-hand sides,
possibly surrounded by a number of existential quantifiers. Our definition is
identical to Lassez, Maher, and Marriott's solved forms \citeyr{lassez-al-88}
and to Jouannaud and Kirchner's {tree} solved
forms \citeyr{jouannaud-kirchner-91}, except we allow for prenex existential
quantifiers, which are made necessary by our richer constraint language.
Jouannaud and Kirchner also define \emph{dag} solved forms, which may be
exponentially smaller. Because we define solved forms only for proof purposes,
we need not take performance into account at this point. The efficient
constraint solver presented in \S\ref{section-solver} does manipulate
graphs, rather than trees. Type scheme introduction and instantiation
constructs cannot appear within solved forms; indeed, provided the constraint
at hand has no free program identifiers, they can be expanded away. For this
reason, their presence in the constraint language has no impact on the results
contained in this section.
%
\begin{definition}
\label{def-solved-form}
A \emph{solved form} is of the form $\exists\twars.(\tvarc=\ttypc)$, where
$\disjoint\tvars{\ftv\ttyps}$.
\end{definition}

Solved forms offer a convenient way of reasoning about constraints because
every satisfiable constraint is equivalent to one. This property is
established by the following lemma.
%
\begin{lemma}
\label{lemma-solved-form}
% Let $\fpv\co=\varnothing$. (implicite)
Every constraint is equivalent to either a solved form or
$\cfalse$.
\end{lemma}
\begin{Proof}
\demoreset We first establish that every conjunction of equations is
equivalent to either a solved form or $\cfalse$. To do so, we present
Robinson's unification algorithm \citeyr{Robinson71} as a rewriting
system. The system's invariant is to operate on constraints of the form either
$\tvarc=\ttypc;\co$, where $\disjoint\tvars{\ftv{\ttyps,\co}}$ and the
semicolon is interpreted as a distinguished conjunction, or $\cfalse$.  We
identify equations in $\co$ up to commutativity.  The system is defined as
follows:
%
$$\begin{array}{rrcl}
\tvarc=\ttypc; & \tvar=\tvar\wedge\co
& \red &
\tvarc=\ttypc; \co \\
\tvarc=\ttypc; & \tycon\,\ttypc_1=\tycon\,\ttypc_2\wedge\co
& \red &
\tvarc=\ttypc; \ttypc_1=\ttypc_2\wedge\co \\
\tvarc=\ttypc; & \tycon_1\,\ttypc_1=\tycon_2\,\ttypc_2\wedge\co
& \red &
\cfalse \\
& & & \text{if $\tycon_1\not=\tycon_2$} \\
\tvarc=\ttypc; & \tvar=\ttyp\wedge\co
& \red &
\tvarc=\subst\tvar\ttyp\ttypc\wedge\tvar=\ttyp; \subst\tvar\ttyp\co \\
& & & \text{if $\tvar\not\in\ftv\ttyp$} \\
\tvarc=\ttypc; & \tvar=\ttyp\wedge\co
& \red &
\cfalse \\
& & & \text{if $\tvar\in\ftv\ttyp$ and $\ttyp\not\in\tyvarset$}
\end{array}$$
%
It is straightforward to check that the above invariant is indeed preserved by
the rewriting system. Let us check that constraint equivalence is also
preserved. For the first rule, this is immediate. For the second and third
rules, it follows from the fact that we have assumed a free tree model; for the
fourth rule, a consequence of \Rule{C-Eq}; for the last rule, a consequence of
Lemma~\ref{lemma-cyclic}. Furthermore, the system is terminating; this is
witnessed by an ordering where $\cfalse$ is the least element and where
constraints of the form $\tvarc=\ttypc;\co$ are ordered lexicographically,
first by the number of type variables that appear free within $\co$, second by
the size of $\co$. Last, a normal form for this rewriting system must be of
the form either $\tvarc=\ttypc; \ctrue$, where (by the invariant)
$\disjoint\tvars{\ftv\ttyps}$---that is, a solved form, or $\cfalse$.

Next, we show that the present lemma holds when $\co$ is built out of
equations, conjunction, and existential quantification. Orienting
\Rule{C-ExAnd} from left to right yields a terminating rewriting system that
preserves constraint equivalence. The normal form of $\co$ must be
$\exists\twars.\co'$, where $\co'$ is a conjunction of equations. By the
previous result, $\co'$ is equivalent to either a solved form or $\cfalse$.
Because solved forms are preserved by existential quantification and because
$\exists\twars.\cfalse$ is $\cfalse$, the same holds of $\co$.

Last, we establish the result in the general case. We assume
$\fpv\co=\varnothing$~\dlabel{nofpv}. Orienting \Rule{C-InId} and \Rule{C-In*}
from left to right yields a terminating rewriting system that preserves
constraint equivalence. The normal form $\co'$ of $\co$ cannot contain any
type scheme introduction forms; given \dref{nofpv}, it cannot contain any
instantiation forms either. Thus, $\co'$ is built out of equations,
conjunction, and existential quantification only. By the previous result, it
is equivalent to either a solved form or $\cfalse$, which implies that the
same holds of $\co$.
\end{Proof}

It is possible to impose further restrictions on solved forms. A solved form
$\exists\twars.(\tvarc=\ttypc)$ is \emph{canonical} if and only if its free
type variables are exactly $\tvars$. This is stated, in an equivalent way,
by the following definition.
%
\begin{definition}
\label{def-canonical-form}
A \emph{canonical solved form} is a constraint of the form
$\exists\twars.(\tvarc=\ttypc)$, where $\ftv\ttyps\subseteq\twars$ and
$\disjoint\tvars\twars$.
\end{definition}
%
\begin{lemma}
\label{lemma-canonical-form}
Every solved form is equivalent to a canonical solved form.
\end{lemma}
%
\begin{Proof}
\demoreset Consider the rewriting rule
%
$\exists\twars\tvar.(\tvar=\ttyp\wedge\tvarc=\ttypc) \red
  \exists\twars.(\tvarc=\ttypc)$.
%
Assume its left-hand side is a solved form. Then, its right-hand side is a
solved form as well. Furthermore, we have
$\tvar\not\in\ftv{\ttyp,\tvarc,\ttypc}$, which by \Rule{C-NameEq} implies
that the rule preserves constraint equivalence. The rule is terminating;
its normal forms are solved forms $\exists\twars.(\tvarc=\ttypc)$ such that
$\disjoint\tvars\twars$.

Next, consider the rewriting rule $\exists\twars.(\tvarc=\ttypc) \red
\exists\twars\twar.(\tvarc=\subst\tvar\twar\ttypc\wedge\tvar=\twar)$, with the
side-conditions $\tvar\in\ftv\ttyps\setminus\twars$~\dlabel{s1} and
$\twar\not\in\ftv{\twars,\tvars,\ttyps}$~\dlabel{s2}.  Assume its left-hand
side is a solved form. By \dref{s1} and \dref{s2}, we have
$\tvar\not\in\tvars$, $\tvar\not=\twar$, and $\twar\not\in\tvars$, which imply
that the right-hand side is a solved form as well. By \dref{s1}, \dref{s2},
and \Rule{C-NameEq}, the rule preserves constraint equivalence. The rule is
terminating; its normal forms are solved forms $\exists\twars.(\tvarc=\ttypc)$
such that $\ftv\ttyps\subseteq\twars$.
\end{Proof}
It is easy to describe the solutions of a canonical solved form: they are the
ground refinements of the substitution $\subst\tvarc\ttypc$.
\begin{short}
Hence, every canonical solved form is satisfiable.
\end{short}
%
\begin{full}
\begin{lemma}
\label{lemma-canonical-solutions}
A ground assignment $\ga$ satisfies a canonical solved form
$\exists\twars.(\tvarc=\ttypc)$ if and only if there exists a ground
assignment $\ga'$ such that $\ga(\tvarc)=\ga'(\ttypc)$. As a result, every
canonical solved form is satisfiable.
\end{lemma}
%
\begin{Proof}
Let $\exists\twars.(\tvarc=\ttypc)$ be a canonical solved form. By
\Rule{CM-Exists} and \Rule{CM-Predicate}, $\ga$ satisfies
$\exists\twars.(\tvarc=\ttypc)$ if and only if there exists $\gtypc$ such that
$\etend\ga\twarc\gtypc(\tvarc)=\etend\ga\twarc\gtypc(\ttypc)$. Thanks to the
hypotheses $\disjoint\tvars\twars$ and $\ftv\ttyps\subseteq\twars$, this is
equivalent to the existence of a ground assignment $\ga'$ such that
$\ga(\tvarc)=\ga'(\ttypc)$. Thus, for every ground assignment $\ga'$,
$\etend{\ga'}\tvarc{\ga'(\ttypc)}$ satisfies $\exists\twars.(\tvarc=\ttypc)$,
which proves that this constraint is satisfiable.
\end{Proof}
Together, Lemmas~\ref{lemma-canonical-form}
and~\ref{lemma-canonical-solutions} imply that every solved form is
satisfiable.
%
Our interest in canonical solved forms stems from the following fundamental
property, which provides a \emph{syntactic} characterization of entailment
between canonical solved forms: if $\exists\twars_1.(\tvarc=\ttypc_1)$ is
more specific than $\exists\twars_2.(\tvarc=\ttypc_2)$, in a logical sense,
then $\ttypc_1$ refines $\ttypc_2$, in a syntactic sense. The converse also
holds (can you prove it?), but is not needed here.
%
\begin{lemma}
\label{lemma-entailment-is-refinement}
If $\exists\twars_1.(\tvarc=\ttypc_1) \entails
\exists\twars_2.(\tvarc=\ttypc_2)$, where both sides are canonical solved
forms, then there exists a type substitution $\refiner$ such that
$\ttypc_1=\refiner(\ttypc_2)$.
\end{lemma}
%
\begin{Proof}
\demoreset Let $\exists\twars_1.(\tvarc=\ttypc_1) \entails
\exists\twars_2.(\tvarc=\ttypc_2)$~\dlabel{e}.
By Lemma~\ref{lemma-canonical-solutions}, \dref{e} may be read
$\forall\ga\quad(\exists\ga_1\quad\ga(\tvarc)=\ga_1(\ttypc_1))\Rightarrow
(\exists\ga_2\quad\ga(\tvarc)=\ga_2(\ttypc_1))$, that is,
$\forall\ga_1\exists\ga_2\quad\ga_1(\ttypc_1)=\ga_2(\ttypc_2)$~\dlabel{fact}.
If all elements of $\ttypc_2$ are variables, then $\subst{\ttypc_2}{\ttypc_1}$
determines a type substitution, even though $\ttypc_2$ may have repeated
elements. Indeed, using \dref{fact} and the fact that the model is
nondegenerate, it is not difficult to check that if two elements of $\ttypc_2$
coincide, then the corresponding elements of $\ttypc_1$ must coincide as
well. The general case, where $\ttypc_2$ contains arbitrary terms, follows by
induction on the size of $\ttypc_2$, again using the fact that the model is
nondegenerate.
\end{Proof}
As a corollary, we find that canonical solved forms are \emph{unique} up to
\aconv and up to \Rule{C-Ex*}, \emph{provided} the set $\tvars$ of their free
type variables is fixed.
%
\begin{lemma}
\label{lemma-mgu-unique}
If the canonical solved forms $\exists\twars_1.(\tvarc=\ttypc_1)$ and
$\exists\twars_2.(\tvarc=\ttypc_2)$ are equivalent, then there exists
a renaming $\rng$ such that $\ttypc_1=\rng(\ttypc_2)$.
\end{lemma}

Note that the fact that the canonical solved forms
$\exists\twars_1.(\tvarc_1=\ttypc_1)$ and
$\exists\twars_2.(\tvarc_2=\ttypc_2)$ are equivalent does \emph{not} imply
that $\tvars_1$ and $\tvars_2$ coincide. Consider, for example, the canonical
solved forms $\ctrue$ and $\exists\twar.(\tvar=\twar)$, which by
\Rule{C-NameEq} are equivalent. One might wish to further restrict canonical
solved forms by requiring $\tvars$ to be the set of \emph{essential} type
variables of the constraint $\exists\twars.(\tvarc=\ttypc)$, that is, the set
of the type variables that appear free in \emph{all} equivalent
constraints. However, as far our technical development is concerned, it seems
more convenient not to do so.  Instead, we show that it is possible to
explicitly restrict or extend $\tvars$ when needed
(Lemma~\ref{lemma-mgu-support}).
\end{full}

The following definition allows entertaining a dual view of canonical solved
forms, either as constraints or as idempotent type substitutions. The latter
view is commonly found in standard treatments of
unification \cite{lassez-al-88,jouannaud-kirchner-91} and in classic
presentations of \MLtype.
%
\begin{definition}
\label{def-mgu}
If $\subst\tvarc\ttypc$ is an idempotent substitution of domain $\tvars$, let
$\exists\subst\tvarc\ttypc$ denote the canonical solved form
$\exists\twars.(\tvarc=\ttypc)$, where $\twars=\ftv\ttyps$. An idempotent
substitution $\unifier$ is a \emph{most general unifier} of the constraint
$\co$ if and only if $\exists\unifier$ and $\co$ are equivalent.
\end{definition}

By definition, equivalent constraints admit the same most general unifiers.
Many properties of canonical solved forms may be reformulated in terms of most
general unifiers. By Lemmas~\ref{lemma-solved-form} and~\ref{lemma-canonical-form}, every satisfiable
constraint admits a most general unifier. 
\begin{full}
By Lemma~\ref{lemma-mgu-unique}, if
$\subst\tvarc{\ttypc_1}$ and $\subst\tvarc{\ttypc_2}$ are most general
unifiers of $\co$, then $\ttypc_1$ and $\ttypc_2$ coincide up to a renaming.
Conversely, if $\subst\tvarc\ttypc$ is a most general unifier of $\co$ and if
$\disjoint\tvars\rng$ holds, then $\subst\tvarc{\rng\ttypc}$ is also a most
general unifier of $\co$; indeed, these two substitutions correspond to
$\alpha$-equivalent canonical solved forms.

The following result relates the substitution $\unifier$ to the canonical
solved form $\exists\unifier$, stating that every ground refinement of the
former satisfies the latter.
%
\begin{lemma}
\label{lemma-mgu-self}
$\unifier(\exists\unifier)\logeq\ctrue$.
\end{lemma}
%
\begin{Proof}
\demoreset Let $\unifier=\subst\tvarc\ttypc$~\dlabel{t} be an idempotent
substitution of domain $\tvars$. Then, $\exists\unifier$ is the canonical
solved form $\exists\twars.(\tvarc=\ttypc)$, where $\twars=\ftv\ttyps$. By
Lemma~\ref{lemma-sat-subst},
$\ga\satisfies\unifier(\exists\unifier)$~\dlabel{ass} is equivalent to
$\ga\circ\unifier\satisfies\exists\unifier$, which, according to
Lemma~\ref{lemma-canonical-solutions}, is equivalent to
$\exists\ga'\quad(\ga\circ\unifier)(\tvarc)=\ga'(\ttypc)$. By \dref{t}, this
may be simplified to $\exists\ga'\quad\ga(\ttypc)=\ga'(\ttypc)$, which is
true.  We have proved that \dref{ass} holds for every ground assignment $\ga$,
which means $\unifier(\exists\unifier)\logeq\ctrue$.
\end{Proof}

The following lemma offers two technical results: the domain of a most general
unifier of $\co$ may be restricted so as to become a subset of $\ftv\co$; it
may also be extended to include arbitrary fresh variables. The next lemma is
a simple corollary.
%
\begin{lemma}
\label{lemma-mgu-support}
Let $\unifier$ be a most general unifier of $\co$. If
$\disjoint\tzars{\ftv\co}$, then $\corestrict\unifier\tzars$ is also a
most general unifier of $\co$.
%
If $\disjoint\tzars\unifier$, then there exists a most general unifier
of $\co$ that extends $\unifier$ and whose domain is $\Dom\unifier\cup\tzars$.
\end{lemma}
%
\begin{Proof}
Let $\unifier$ be a most general unifier of $\co$. $\unifier$ is of the form
$\subst\tvarc\ttypc$, where $\tvars=\Dom\unifier$, and $\co$ is equivalent to
the canonical solved form $\exists\twars.(\tvarc=\ttypc)$, where
$\twars=\ftv\ttyps$.

\demoreset First, let $\disjoint\tzars{\ftv\co}$~\dlabel{z}. Write
$\tvarc=\ttypc$ as $\tvarc_1=\ttypc_1\wedge\tvarc_2=\ttypc_2$, where
$\tvars_1=\tvars\setminus\tzars$ and $\tvars_2=\tvars\cap\tzars$. By \dref{z},
we have $\disjoint{\tvars_2}{\ftv\co}$~\dlabel{x2}. By
Lemma~\ref{lemma-canonical-solutions}, the solutions of $\co$ are the ground
assignments $\ga$ such that $\ga(\tvarc_1)=\ga'(\ttypc_1)$ and
$\ga(\tvarc_2)=\ga'(\ttypc_2)$ hold for some $\ga'$. Then, by \dref{x2} and
Lemma~\ref{lemma-sat-free}, the solutions of $\co$ are the ground assignments
$\ga$ such that $\ga(\tvarc_1)=\ga'(\ttypc_1)$ holds for some $\ga'$. In other
words, $\co$ is equivalent to the canonical solved form
$\exists\twars_1.(\tvarc_1=\ttypc_1)$, where $\twars_1=\ftv{\ttypc_1}$.
Furthermore, $\subst{\tvarc_1}{\ttypc_1}$ is precisely
$\corestrict\unifier\tzars$.

\demoreset Now, let $\disjoint\tzars\unifier$, that is,
$\disjoint\tzars{\tvars\twars}$~\dlabel{z}. Let $\rng$ be a renaming such that
$\disjoint{\rng\tzars}{\tvars\twars\tzars}$~\dlabel{rho}.  By \dref{rho} and
\Rule{C-NameEq}, $\exists\twars.(\tvarc=\ttypc)$ is equivalent to
$\exists\twars\rng\tzars.(\tvarc=\ttypc\wedge\tzarc=\rng\tzarc)$~\dlabel{csf}.
By \dref{z} and \dref{rho}, \dref{csf} is a canonical solved form. Thus, the
extension of $\unifier$ with $\subst\tzarc{\rng\tzarc}$ is a most general
unifier of $\co$.
\end{Proof}
%
\begin{lemma}
\label{lemma-mgu-intersection}
Let $\unifier_1$ and $\unifier_2$ be most general unifiers of $\co$. Let
$\tvars=\Dom{\unifier_1}\cap\Dom{\unifier_2}$. Then, $\unifier_1(\tvars)$ and
$\unifier_2(\tvars)$ coincide up to a renaming.
\end{lemma}
%
\begin{Proof}
$\unifier_1$ is a most general unifier of $\exists\unifier_2$, whose free type
variables are $\Dom{\unifier_2}$. By Lemma~\ref{lemma-mgu-support}, its
restriction to $\Dom{\unifier_1}\cap\Dom{\unifier_2}$ is also a most general
unifier of $\exists\unifier_2$; in other words, the restriction of
$\unifier_1$ to $\tvars$ is a most general unifier of $\co$. Similarly, the
restriction of $\unifier_2$ to $\tvars$ is a most general unifier of $\co$.
The result follows by Lemma~\ref{lemma-mgu-unique}.
\end{Proof}

Our last technical result relates the most general unifiers of $\co$ with the
most general unifiers of $\exists\tvar.\co$. It states that the former are
extensions of the latter. Furthermore, under a few freshness conditions,
\emph{every} most general unifier of $\exists\tvar.\co$ may be extended to
yield a most general unifier of $\co$.
%
% This property plays a key role in the translation of \hmeq typing judgments
% to \dm typing judgments in the next section.
%
\begin{lemma}
\label{lemma-mgu-exists}
If $\unifier$ is a most general unifier of $\co$, then
$\corestrict\unifier\tvar$ is a most general unifier of
$\exists\tvar.\co$.  Conversely, if $\unifier$ is a most general unifier of
$\exists\tvar.\co$ and $\disjoint\tvar\unifier$ and
$\ftv{\exists\tvar.\co}\subseteq\Dom\unifier$, then there exists
a type substitution $\unifier'$ such that $\unifier'$ extends
$\unifier$, $\unifier'$ is a most general unifier of $\co$,
and $\Dom{\unifier'}=\Dom\unifier\cup\tvar$.
\end{lemma}
%
\begin{Proof}
Let $\unifier$ be a most general unifier of $\co$. Then, $\co$ must be
equivalent to a canonical solved form $\exists\twars.(\tvarc=\ttypc)$, where
$\unifier$ is $\subst\tvarc\ttypc$. We now distinguish two cases.  First, if
$\tvar\not\in\tvars$, then $\co$ and $\exists\tvar.\co$ are equivalent, so
$\unifier$ is a most general unifier for $\exists\tvar.\co$. Furthermore,
given $\Dom\unifier=\tvars\not\ni\tvar$, $\corestrict\unifier\tvar$ is
$\unifier$ itself. Second, if $\tvar\in\tvars$, then $\exists\tvar.\co$ may be
written $\exists\twars.\exists\tvar.(\tvar=\ttyp\wedge\tvarcp=\ttypcp)$, which
by \Rule{C-NameEq} is equivalent to $\exists\twars.(\tvarcp=\ttypcp)$.  As a
result, the substitution $\subst\tvarcp\ttypcp$---which is precisely
$\corestrict\unifier\tvar$---is a most general unifier for
$\exists\tvar.\co$.

\demoreset Conversely, let $\unifier$ be a most general unifier of
$\exists\tvar.\co$. Let $\disjoint\tvar\unifier$~\dlabel{x} and
$\ftv{\exists\tvar.\co}\subseteq\Dom\unifier$~\dlabel{covered}. Because
$\unifier$ is idempotent, we have
$\disjoint{\Dom\unifier}{\Range\unifier}$~\dlabel{lo}. Together,
\dref{covered} and \dref{lo} imply
$\disjoint{\ftv{\exists\tvar.\co}}{\Range\unifier}$~\dlabel{rt}. Furthermore,
\dref{x} and \dref{rt} imply $\disjoint{\ftv\co}{\Range\unifier}$~\dlabel{hi}.
Because $\exists\tvar.\co$ is satisfiable, $\co$ is satisfiable as well, so it
admits a most general unifier $\unifier'$. By Lemma~\ref{lemma-mgu-support}
and by \aconv, it is possible to require
$\Dom\unifier\subseteq\Dom{\unifier'}$~\dlabel{inclus} and
$\disjoint{\Range{\unifier'}}{\Range\unifier}$~\dlabel{dr}.  Furthermore, by
\dref{lo}, \dref{hi}, and Lemma~\ref{lemma-mgu-support}, it is possible to
further require $\disjoint{\Dom{\unifier'}}{\Range\unifier}$~\dlabel{clean}
without compromising \dref{inclus} or \dref{dr}. % Ça c'est fort, non?
By the previous result, $\corestrict{\unifier'}\tvar$ must be a most
general unifier of $\exists\tvar.\co$. By Lemma~\ref{lemma-mgu-intersection},
$\unifier$ and $\corestrict{\unifier'}\tvar$ must coincide up to a
renaming on the intersection of their domains, which by \dref{inclus} and
\dref{x} is $\Dom\unifier$.  Thus, if $\unifier$ is $\subst\tvarc\ttypc$,
where $\tvars=\Dom\unifier$, then $\unifier'(\tvarc)$ is
$\subst\twarc\twarcpz\ttypc$, where $\subst\twarc\twarcpz$ is a bijection
between $\twars=\Range\unifier$ and $\twars'_0$, a subset of
$\twars'=\Range{\unifier'}$. Thus, the constraint $\co\logeq\exists\unifier'$
may be written as the canonical solved form
$\exists\twars'.(\tvarc=\subst\twarc\twarcpz\ttypc \wedge
\tvarcp=\ttypcp)$~\dlabel{csf}. By \dref{dr}, the substitution
$\subst\twarcpz\twarc$ defines a bijection on $\twars'$. Furthermore, by
\dref{clean}, its image---a subset of $\twars\twars'$---is fresh for the free
type variables of \dref{csf}---that is, $\tvars\tvars'$. These arguments show
that an $\alpha$-equivalent form of \dref{csf} is
$\exists\subst\twarcpz\twarc{\twars'}.(\tvarc=\ttypc \wedge
\tvarcp=\subst\twarcpz\twarc\ttypcp)$. This yields a most general unifier
$\unifier'$ of $\co$ that extends $\unifier$.
% ouf!

By \dref{x} and \dref{lo}, we have
$\disjoint{\Dom\unifier\cup\tvar}{\Range\unifier}$. This allows requiring,
modulo a renaming of the image of $\unifier'$,
$\disjoint{\Dom\unifier\cup\tvar}{\Range{\unifier'}}$~\dlabel{rp}.
Furthermore, by \dref{covered}, we have
$\ftv\co\subseteq\Dom\unifier\cup\tvar$~\dlabel{cd}. By \dref{rp}, \dref{cd},
and Lemma~\ref{lemma-mgu-support}, we may further require
$\Dom{\unifier'}=\Dom\unifier\cup\tvar$.
\end{Proof}
%
% Il faut noter qu'on ne peut pas exiger que le domaine de $\unifier'$ soit
% exactement \Dom\unifier\cup\{\tvar\}. Il peut devoir être plus grand si
% la contrainte \exists\tvar.\co a plus de variables libres que la
% contrainte équivalente \exists\unifier.
\end{full}

% --------------------------------------------------------------------------
% --------------------------------------------------------------------------

\index{constraints!for type inference|)}

\section{\hmx}
\label{section-hmx}

Constraint-based type systems appeared during the 1980s
  \cite{Mitchell84a, fuh-mishra-88} and were widely studied during the
  following decade \cite{curtis-90, AikenWimmers93, jones94qualified,
  smith-94, palsberg-efficient-object-95, TrifonovSmith96,
  faehndrich-phd-99, pottier-ic-01}. We now present one such system,
  baptized \hmx because it is a \emph{parameterized} extension of
  Hindley and Milner's type discipline; the meaning of the parameter
  $X$ was explained on page~\pageref{page-parameter-x}. Its original
  description is due to \fulllongcite{Sulzmann&97}. Since then, it has
  been completed in a number of works including \longcite{mueller-98},
  \fulllongcite{sulzmann-mueller-zenger-99}, \longcite{sulzmann-00},
  \longcite{pottier-hmx-01}, and
  \fulllongcite{skalka-pottier-tip-02}. Each of these presentations
  introduces minor variations. Here, we follow
  \fulllongcite{pottier-hmx-01}, which is itself inspired by
  \fulllongcite{sulzmann-mueller-zenger-99}.

% --------------------------------------------------------------------------

\subsection*{Definition}

Our presentation of \hmx relies on the constraint language introduced in
\S\ref{section-constraints}. Technically, our approach to constraints is less
abstract than that of \fulllongcite{Sulzmann&97}. We interpret constraints within a
model, give conjunction and existential quantification their standard meaning,
and derive a number of equivalence laws (\S\ref{section-constraints}). Odersky
\etal., on the other hand, do not explicitly rely on a logical interpretation;
instead, they axiomatize constraint equivalence, that is, they consider a
number of equivalence laws as axioms. Thus, they ensure that their high-level
proofs, such as type soundness and correctness and completeness of type
inference, are independent of the low-level details of the logical
interpretation of constraints. Their approach is also more general, since it
allows dealing with other logical interpretations, such as ``open-world''
interpretations, where constraints are interpreted not within a fixed model,
but within a {family} of extensions of a ``current'' model. In \this, we
have avoided this extra layer of abstraction and given fixed meaning to
constraints, making things somewhat simpler. However, the changes required to
adopt Odersky \etal.'s approach would not be extensive, since the forthcoming
proofs do indeed rely mostly on constraint equivalence laws, rather than on
low-level details of the logical interpretation of constraints.

Another slight departure from Odersky \etal.'s work lies in the fact that we
have enriched the constraint language with type scheme introduction and
instantiation forms, which were absent in the original presentation of \hmx.
To prevent this addition from affecting \hmx, we require the constraints
that appear in \hmx typing judgments to {have no free program
identifiers}.  Note that this does not prevent them from containing
\kwd{let} forms\iffull{; we shall in fact exploit this feature when
establishing an equivalence between
\hmx and the type system presented in \S\ref{section-pcb}, where the new
constraint forms are effectively used}.

\begin{TTCOMPONENT}{Typing rules for \hmx}{}
\ttlabel{HM(X)-1}

\infrule[hmx-Var]
  {\env(\evid)=\ts \andalso
   \co\entails\exists\ts}
  {\hmj\co\env\evid\ts}

\infrule[hmx-Abs]
  {\hmj\co{(\env;\evar:\ttyp)}\et{\ttyp'}}
  {\hmj\co\env{\efun\evar\et}{\ttyp\arw\ttyp'}}

\infrule[hmx-App]
  {\hmj\co\env{\et_1}{\ttyp\arw\ttyp'} \kern 1.8em
   \hmj\co\env{\et_2}\ttyp}
  {\hmj\co\env{\eapp{\et_1}{\et_2}}{\ttyp'}}

\infrule[hmx-Let]
  {\hmj\co\env{\et_1}\ts \kern 1.1em
   \hmj\co{(\env;\evar:\ts)}{\et_2}\ttyp}
  {\hmj\co\env{\elet\evar{\et_1}{\et_2}}\ttyp}

\infrule[hmx-Gen]
  {\hmj{\co\wedge\cp}\env\et\ttyp \andalso
   \disjoint\tvars{\ftv{\co,\env}}}
  {\hmj{\co\wedge\exists\tvars.\cp}\env\et{\scheme\tvars\cp\ttyp}}

\infrule[hmx-Inst]
  {\hmj\co\env\et{\scheme\tvars\cp\ttyp}}
  {\hmj{\co\wedge\cp}\env\et\ttyp}

\infrule[hmx-Sub]
  {\hmj\co\env\et\ttyp \andalso
   \co\entails\ttyp\subtype\ttyp'}
  {\hmj\co\env\et{\ttyp'}}

\infrule[hmx-Exists]
  {\hmj\co\env\et\ts \andalso
   \disjoint\tvars{\ftv{\env,\ts}}}
  {\hmj{\exists\tvars.\co}\env\et\ts}

% \extraspacehack{.07in}
\end{TTCOMPONENT}

The type system \hmx consists of a four-place {judgment} whose
parameters are a constraint $\co$, an environment $\env$, an expression $\et$,
and a type scheme $\ts$. A judgment is written $\hmj\co\env\et\ts$ and is
read: \emph{under the assumptions $\co$ and $\env$, the expression $\et$ has
type $\ts$}. One may view $\co$ as an assumption about the judgment's free
type variables and $\env$ as an assumption about $\et$'s free program
identifiers. Recall that $\env$ now contains {constrained}
type schemes, and that $\ts$ is a {constrained} type scheme.

We would like the validity of a typing judgment to depend not on the
{syntax}, but only on the {meaning} of its constraint assumption. We
enforce this point of view by considering judgments equal modulo equivalence
of their constraint assumptions. In other words, the typing judgments
$\hmj\co\env\et\ts$ and $\hmj\cp\env\et\ts$ are considered identical when
$\co\logeq\cp$ holds.
% As a result, it does not make sense to analyze the
% syntax of a judgment's constraint assumption.
%
% As a technical remark, let us note that another (more explicit, but rather
% more clumsy) way of achieving the same effect would be to augment the system
% with the following typing rule:
% \infrule[hmx-Equiv]
%   {\hmj\co\env\et\ttyp \andalso
%    \co\logeq\co'}
%   {\hmj{\co'}\env\et\ttyp}
%
A judgment is \emph{valid}, or \emph{holds}, if and only if it is derivable
via the rules given in Figure~\ref{fig:HM(X)-1}. Note that a valid
judgment may involve an arbitrary constraint. A (closed) program $\et$ is
\emph{well-typed} within the (closed) environment $\env$ if and only if a judgment of
the form $\hmj\co\env\et\ts$ holds for some {satisfiable} constraint
$\co$. One might wonder why we do not make the apparently stronger requirement
that $\co\wedge\exists\ts$ be satisfiable; however, by \shortfull{inspection of the
typing rules}{Lemma~\ref{lemma-hmx-invariant}}, the reader may check that,
if the above judgment is derivable, then $\co\entails\exists\ts$ holds,
hence the two requirements are equivalent.

Let us now explain the rules. Like \Rule{dm-Var}, \Rule{hmx-Var} looks up the
environment to determine the type scheme associated with the program
identifier~$\evid$. Its second premise plays a minor technical role: as noted
in the previous paragraph, its presence helps simplify the definition of
well-typedness. \Rule{hmx-Abs}, \Rule{hmx-App}, and \Rule{hmx-Let} are
identical to \Rule{dm-Abs}, \Rule{dm-App}, and \Rule{dm-Let}, respectively,
except that the assumption $\co$ is made available to every subderivation. We
recall that the type $\ttyp$ may be viewed as the type scheme
$\scheme\varnothing\ctrue\ttyp$ (Definitions~\ref{def-type-scheme}
and~\ref{def-constraints}). As a result, types form a subset of type schemes,
which implies that $\env;\evar:\ttyp$ is a well-formed environment and
$\hmj\co\env\et\ttyp$ a well-formed typing judgment. To understand
\Rule{hmx-Gen}, it is best to first consider the particular case where $\co$
is $\ctrue$. This yields the following, simpler rule:
%
\infrule[hmx-Gen']
  {\hmj\cp\env\et\ttyp \andalso
   \disjoint\tvars{\ftv\env}}
  {\hmj{\exists\tvars.\cp}\env\et{\scheme\tvars\cp\ttyp}}
%
The second premise is identical to that of \Rule{dm-Gen}: the type variables
that are generalized must not occur free within the environment. The
conclusion forms the type scheme $\scheme\tvars\cp\ttyp$, where the type
variables $\tvars$ have become universally quantified, but are still subject
to the constraint $\cp$. Note that the type variables that
occur free in $\cp$ may include not only $\tvars$, but also other type
variables, typically free in $\env$. \Rule{hmx-Gen} may be viewed as a
more liberal version of \Rule{hmx-Gen'}, whereby part of the current
constraint, namely $\co$, need not be copied if it does not concern
the type variables that are being generalized, namely $\tvars$. This
optimization is important in practice, because $\co$ may be
very large. An intuitive explanation for its correctness is given
by the constraint equivalence law \Rule{C-LetAnd}, which expresses
the same optimization in terms of \kwd{let} constraints. Because
\hmx does not use \kwd{let} constraints, the optimization is
hard-wired into the typing rule.
As a last technical remark, let us point out that replacing
$\co\wedge\exists\tvars.\cp$ with $\co\wedge\cp$ in \Rule{hmx-Gen}'s
conclusion would not affect the set of derivable judgments; this fact may be
established using \Rule{hmx-Exists} and Lemma~\ref{lemma-hmx-weakening}.
\Rule{hmx-Inst} allows taking an instance of a type scheme. The reader may be
surprised to find that, contrary to \Rule{dm-Inst}, it does not involve a type
substitution. Instead, the rule merely drops the universal quantifier, which
amounts to applying the identity substitution $\tvarc\mapsto\tvarc$.  One
should recall, however, that type schemes are considered equal modulo \aconv,
so it is possible to {rename} the type scheme's universal quantifiers
prior to using \Rule{hmx-Inst}. The reason why this provides sufficient
expressive power appears in Exercise~\ref {ex:hmx-Inst}\iffull { and in the
proof of Theorem~\ref{theorem-hmx-encodes-dm}} below. The constraint $\cp$
carried by the type scheme is recorded as part of 
the current constraint in \Rule{hmx-Inst}'s conclusion. The \emph{subsumption}
rule \Rule{hmx-Sub} allows a type $\ttyp$ to be replaced at any time with an
arbitrary supertype $\ttyp'$. Because both $\ttyp$ and $\ttyp'$ may have free
type variables, whether $\ttyp\subtype\ttyp'$ holds depends on the current
assumption $\co$, which is why the rule's second premise is an
{entailment} assertion. An operational explanation of \Rule{hmx-Sub} is
that it requires all uses of subsumption to be explicitly recorded in the
current constraint. Note that \Rule{hmx-Sub} remains a useful and
necessary rule even when subtyping is interpreted as equality: then, it allows
exploiting the type {equations} found in $\co$. Last, \Rule{hmx-Exists}
allows the type variables that occur only within the current constraint to
become existentially quantified.  As a result, these type variables no longer
occur free in the rule's conclusion; in other words, they have become
{local} to the subderivation rooted at the premise. One may prove that
the presence of \Rule{hmx-Exists} in the type system does not augment the set
of well-typed programs, but does augment the set of valid typing judgments;
it is a pleasant technical convenience. Indeed, because judgments are
considered equal modulo constraint equivalence, constraints may be
transparently {simplified} at any time. (By {simplifying} a
constraint, we mean replacing it with an equivalent constraint whose syntactic
representation is considered simpler.) Bearing this fact in mind, one finds
that an effect of rule \Rule{hmx-Exists} is to enable {more}
simplifications: because constraint equivalence is a congruence,
$\co\logeq\cp$ implies $\exists\tvars.\co\logeq\exists\tvars.\cp$, but the
converse does not hold in general. For instance, there is in general no way of
simplifying the judgment $\hmj{\tvar\subtype\twar\subtype\tzar}\env\et\ts$,
but if it is known that $\twar$ does not appear free in $\env$ or $\ts$, then
\Rule{hmx-Exists} allows deriving
$\hmj{\exists\twar.(\tvar\subtype\twar\subtype\tzar)}\env\et\ts$, which is the
same judgment as $\hmj{\tvar\subtype\tzar}\env\et\ts$.  Thus, an interesting
simplification has been enabled. Note that
$\tvar\subtype\twar\subtype\tzar\logeq\tvar\subtype\tzar$ does {not}
hold, while, according to \Rule{C-ExTrans},
$\exists\twar.(\tvar\subtype\twar\subtype\tzar)\logeq\tvar\subtype\tzar$ does.

\begin{full}
We now establish a few simple properties of the type system \hmx. Our first
lemma is a minor technical property. As noted earlier, this justifies our
simplified definition of well-typedness.
%
\begin{lemma}
\label{lemma-hmx-invariant}
$\hmj\co\env\et\ts$ implies $\co\entails\exists\ts$.
\end{lemma}
%
\begin{Proof}
The proof is by structural induction on a derivation of $\hmj\co\env\et\ts$.
In each proof case, we adopt the notations of Figure~\ref{fig:HM(X)-1}.

\proofcase\Rule{hmx-Var}. The goal is precisely the rule's second premise.

\proofcase\Rule{hmx-Abs}, \Rule{hmx-App}, \Rule{hmx-Let}, \Rule{hmx-Inst},
\Rule{hmx-Sub}. In these cases $\ts$ is in fact a type $\ttyp$, that is, a type
scheme of the form $\scheme\varnothing\ctrue\ttyp$. As a result, the goal is
$\co\entails\ctrue$, a tautology.

\proofcase\Rule{hmx-Gen}. The rule's conclusion is
$\hmj{\co\wedge\exists\tvars.\cp}\env\et{\scheme\tvars\cp\ttyp}$. The goal is
$\co\wedge\exists\tvars.\cp\entails\exists\tvars.\cp$, a tautology.

\demoreset\proofcase\Rule{hmx-Exists}. The rule's conclusion is
$\hmj{\exists\tvars.\co}\env\et\ts$. Its premises are
$\hmj\co\env\et\ts$~\dlabel{g} and
$\disjoint\tvars{\ftv{\env,\ts}}$~\dlabel{d}. Applying the induction
hypothesis to \dref{g} yields $\co\entails\exists\ts$~\dlabel{imp}. By
congruence of entailment, by \dref{d} and \Rule{C-Ex*}, \dref{imp} implies
$\exists\tvars.\co\entails\exists\ts$.
\end{Proof}
\end{full}


%% \FIXJNF{ -- added this command to help with next paragraph}
\newcommand{\citethm}[2]{\longcite{#2}, #1}

A pleasant property of \hmx is that {strengthening} a judgment's
constraint assumption (that is, {weakening} the judgment itself) preserves
its validity.  It is worth noting that in traditional 
presentations, which rely more heavily on type substitutions, the analog of
this result is a \emph{type substitution} lemma; see for instance \citethm{Lemma
2.7}{Tofte88:Thesis}; \citethm{Lemma 1}{Remy!mleth}; \citethm{Proposition
1.2}{leroy-phd-92}; and \citethm{Lemma 3.4}{skalka-pottier-tip-02}. Here, the lemma
further states that weakening a judgment does not alter the shape of its
derivation, a useful property when reasoning by induction on type derivations.
%
\begin{lemma}[Weakening]
\label{lemma-hmx-weakening}
If $\co'\entails\co$, then every derivation of $\hmj\co\env\et\ts$ may be
turned into a derivation of $\hmj{\co'}\env\et\ts$ with the same shape.
\end{lemma}
%
\begin{Proof}
The proof is by structural induction on a derivation of $\hmj\co\env\et\ts$.
In each proof case, we adopt the notations of Figure~\ref{fig:HM(X)-1}.

\demoreset\proofcase\Rule{hmx-Var}. The rule's conclusion is
$\hmj\co\env\evid\ts$. Its premises are $\env(\evid)=\ts$~\dlabel{un} and
$\co\entails\exists\ts$~\dlabel{deux}. By hypothesis, we have
$\co'\entails\co$~\dlabel{h}.
By transitivity of entailment,
\dref{h} and \dref{deux} imply
$\co'\entails\exists\ts$~\dlabel{deuxp}. By \Rule{hmx-Var}, \dref{un} and
\dref{deuxp} yield $\hmj{\co'}\env\evid\ts$.

\proofcases\Rule{hmx-Abs}, \Rule{hmx-App}, \Rule{hmx-Let}. By the induction
hypothesis and by \Rule{hmx-Abs}, \Rule{hmx-App}, or \Rule{hmx-Let},
respectively.

\demoreset\proofcase\Rule{hmx-Gen}. The rule's conclusion is
$\hmj{\co\wedge\exists\tvars.\cp}\env\et{\scheme\tvars\cp\ttyp}$. Its premises
are $\hmj{\co\wedge\cp}\env\et\ttyp$~\dlabel{g} and
$\disjoint\tvars{\ftv{\co,\env}}$~\dlabel{d}. By hypothesis, we have
$\co'\entails\co\wedge\exists\tvars.\cp$~\dlabel{e}. We may assume, \spdg,
$\disjoint\tvars{\ftv{\co'}}$~\dlabel{f}. Applying the induction
hypothesis to \dref{g} and to the entailment assertion
$\co'\wedge\co\wedge\cp\entails\co\wedge\cp$, we obtain
$\hmj{\co'\wedge\co\wedge\cp}\env\et\ttyp$~\dlabel{gp}.  By \Rule{hmx-Gen},
applied to \dref{gp}, \dref{d} and \dref{f}, we get
$\hmj{\co'\wedge\co\wedge\exists\tvars.\cp}\env\et
{\scheme\tvars\cp\ttyp}$~\dlabel{gq}. By \dref{e} and \Rule{C-Dup}, the
constraints $\co'\wedge\co\wedge\exists\tvars.\cp$ and $\co'$ are equivalent,
so \dref{gq} is the goal
$\hmj{\co'}\env\et{\scheme\tvars\cp\ttyp}$.

\demoreset\proofcase\Rule{hmx-Inst}. The rule's conclusion is
$\hmj{\co\wedge\cp}\env\et\ttyp$. Its premise is
$\hmj\co\env\et{\scheme\tvars\cp\ttyp}$~\dlabel{p}. By hypothesis, $\co'$
entails $\co\wedge\cp$~\dlabel{e}. Because \dref{e} implies $\co'\entails\co$,
the induction hypothesis may be applied to \dref{p}, yielding
$\hmj{\co'}\env\et{\scheme\tvars\cp\ttyp}$~\dlabel{q}. By \Rule{hmx-Inst}, we
obtain $\hmj{\co'\wedge\cp}\env\et\ttyp$~\dlabel{g}. Because \dref{e} implies
$\co'\logeq\co'\wedge\cp$, \dref{g} is the goal $\hmj{\co'}\env\et\ttyp$.

\demoreset\proofcase\Rule{hmx-Sub}. The rule's conclusion is
$\hmj\co\env\et{\ttyp'}$. Its premises are $\hmj\co\env\et\ttyp$~\dlabel{g}
and $\co\entails\ttyp\subtype\ttyp'$~\dlabel{d}. By hypothesis, we have
$\co'\entails\co$~\dlabel{e}. Applying the induction hypothesis to \dref{g}
and \dref{e} yields $\hmj{\co'}\env\et\ttyp$~\dlabel{gp}.  By transitivity of
entailment, \dref{e} and \dref{d} imply
$\co'\entails\ttyp\subtype\ttyp'$~\dlabel{dp}. By \Rule{hmx-Sub}, \dref{gp}
and \dref{dp} yield $\hmj{\co'}\env\et{\ttyp'}$.

\demoreset\proofcase\Rule{hmx-Exists}. The rule's conclusion is
$\hmj{\exists\tvars.\co}\env\et\ts$. Its premises are
$\hmj\co\env\et\ts$~\dlabel{g} and
$\disjoint\tvars{\ftv{\env,\ts}}$~\dlabel{d}.
By hypothesis, we have $\co'\entails\exists\tvars.\co$~\dlabel{e}.
We may assume, \spdg, $\disjoint\tvars{\ftv{\co'}}$~\dlabel{f}.
Applying the induction hypothesis to \dref{g} and to the
entailment assertion $\co'\wedge\co\entails\co$, we obtain
$\hmj{\co'\wedge\co}\env\et\ts$~\dlabel{gp}. By \Rule{hmx-Exists},
\dref{gp} and \dref{d} yield
$\hmj{\exists\tvars.(\co'\wedge\co)}\env\et\ts$~\dlabel{gq}.
By \dref{f} and \Rule{C-ExAnd}, the constraint $\exists\tvars.(\co'\wedge\co)$
is equivalent to $\co'\wedge\exists\tvars.\co$, which, by \dref{e} and
\Rule{C-Dup}, is equivalent to $\co'$. Thus, \dref{gq} is
the goal $\hmj{\co'}\env\et\ts$.
\end{Proof}
%
\begin{exercise}[\Recommended, \Easy]
\solref{hmx-Inst}
In some presentations of \hmx, \Rule{hmx-Inst} is replaced with the following
variant:
\infrule[hmx-Inst']
  {\hmj\co\env\et{\scheme\tvars\cp\ttyp} \andalso \co\entails\subst\tvarc\ttypc\cp}
  {\hmj\co\env\et{\subst\tvarc\ttypc\ttyp}}
Show that \Rule{hmx-Inst'} is admissible in our presentation of \hmx---that is,
if its premise is derivable according to the rules of Figure~\ref{fig:HM(X)-1},
then so is its conclusion.
Thus, the choice between \Rule{hmx-Inst} and \Rule{hmx-Inst'} is only stylistic:
it makes no difference in the system's expressive power. Because \Rule{hmx-Inst} is more
elementary, choosing it simplifies some proofs.
\end{exercise}
%
\begin{exercise}[\QuickCheck]
\solref{hmx-derivation}
Give a derivation of $\hmj\ctrue\noenv{\efun\evar\evar}{\tint\arw\tint}$.
Give a derivation of $\hmj\ctrue\noenv{\efun\evar\evar}{\dmscheme\tvar{\tvar\arw\tvar}}$.
Check that the former judgment also follows from the latter via \Rule{hmx-Inst'} (Exercise~\ref{ex:hmx-Inst}),
and determine which derivation of $\hmj\ctrue\noenv{\efun\evar\evar}{\tint\arw\tint}$ this path gives rise to.
\end{exercise}
%
We do not give a direct type soundness proof for \hmx. Instead, in the
forthcoming sections, we prove that well-typedness in \hmx is equivalent to
the satisfiability of a certain constraint and use that characterization as a
basis for our type soundness proof. A direct type soundness result, based on a
denotational semantics, may be found
in \fulllongcite{Sulzmann&97}. Another type soundness proof, which
follows Wright and Felleisen's syntactic
approach \citeyr{IC::WrightF1994}, appears
in \longcite{skalka-pottier-tip-02}. Last, a hybrid approach, which combines some
of the advantages of the previous two, is given in \longcite{pottier-hmx-01}.

% ----------------------------------------------------------------------

\subsection*{An Alternate Presentation of \hmx}

\begin{TTCOMPONENT}{An alternate presentation of \hmx}{}
\ttlabel{HM(X)-2}

\infrule[hmd-VarInst]
  {\env(\evid)=\scheme\tvars\cp\ttyp}
  {\hmj{\co\wedge\cp}\env\evid\ttyp}

\vspace{.13in}

\infrule[hmd-Abs]
  {\hmj\co{(\env;\evar:\ttyp)}\et{\ttyp'}}
  {\hmj\co\env{\efun\evar\et}{\ttyp\arw\ttyp'}}

\vspace{.13in}

\infrule[hmd-App]
  {\hmj\co\env{\et_1}{\ttyp\arw\ttyp'} \kern 1.9em
   \hmj\co\env{\et_2}\ttyp}
  {\hmj\co\env{\eapp{\et_1}{\et_2}}{\ttyp'}}

\infrule[hmd-LetGen]
  {\hmj{\co\wedge\cp}\env{\et_1}{\ttyp_1} \andalso
   \disjoint\tvars{\ftv{\co,\env}} \\
   \hmj{\co\wedge\exists\tvars.\cp}
       {(\env;\evar:\scheme\tvars\cp{\ttyp_1})}{\et_2}{\ttyp_2}}
  {\hmj{\co\wedge\exists\tvars.\cp}\env
       {\elet\evar{\et_1}{\et_2}}{\ttyp_2}}

\infrule[hmd-Sub]
  {\hmj\co\env\et\ttyp \andalso
   \co\entails\ttyp\subtype\ttyp'}
  {\hmj\co\env\et{\ttyp'}}

\infrule[hmd-Exists]
  {\hmj\co\env\et\ttyp \andalso
   \disjoint\tvars{\ftv{\env,\ttyp}}}
  {\hmj{\exists\tvars.\co}\env\et\ttyp}

% \extraspacehack{.07in}
\end{TTCOMPONENT}

The presentation of \hmx given in Figure~\ref{fig:HM(X)-1} has only four
syntax-directed rules out of eight. It is a good specification of the type
system, but it is far from an algorithmic description. As a first step towards
such a description, we provide an alternate presentation of \hmx, where
generalization is performed only at \kwlet expressions and instantiation takes
place only at references to program identifiers (Figure~\ref{fig:HM(X)-2}). This
presentation only has two non-syntax-directed rules, making it sometimes easier to
reason about. It
has the property that all judgments are of the form $\hmj\co\env\et\ttyp$,
rather than $\hmj\co\env\et\ts$. The following \shortfull{theorem
states}{theorems state} that the two presentations are indeed equivalent.
%
\begin{short}
\begin{theorem}
\label{theorem-hmd-correct-complete}
$\hmj\co\env\et\ttyp$ is derivable via the rules of
Figure~\ref{fig:HM(X)-2} if and only if it is a valid \hmx judgment.
\end{theorem}
\end{short}
%
\begin{full}
\begin{theorem}
\label{theorem-hmd-correct}
If $\hmj\co\env\et\ttyp$ is derivable via the rules of
Figure~\ref{fig:HM(X)-2}, then it is a valid \hmx judgment.
\end{theorem}
%
\begin{Proof}
It suffices to check that every rule in Figure~\ref{fig:HM(X)-2} is admissible
in \hmx. This is immediate in all cases but the following.

\demoreset\proofcase\Rule{hmd-VarInst}. Assume
$\env(\evid)=\scheme\tvars\cp\ttyp$~\dlabel{h}.  By
Lemma~\ref{lemma-exists-entail}, $\co\wedge\cp$ entails
$\exists\tvars.\cp$~\dlabel{d}. By \Rule{hmx-Var}, \dref{h} and \dref{d} imply
$\hmj{\co\wedge\cp}\env\evid{\scheme\tvars\cp\ttyp}$~\dlabel{k}.  By
\Rule{hmx-Inst}, \dref{k} implies $\hmj{\co\wedge\cp\wedge\cp}\env\evid\ttyp$,
which is the goal, because $\co\wedge\cp\wedge\cp$ is equivalent to
$\co\wedge\cp$.

\demoreset\proofcase\Rule{hmd-LetGen}. Assume
$\hmj{\co\wedge\cp}\env{\et_1}{\ttyp_1}$~\dlabel{g} and
$\disjoint\tvars{\ftv{\co,\env}}$~\dlabel{f} and
$\hmj{\co\wedge\exists\tvars.\cp}
{(\env;\evar:\scheme\tvars\cp{\ttyp_1})}{\et_2}{\ttyp_2}$~\dlabel{d}.  By
\Rule{hmx-Gen}, \dref{g} and \dref{f} imply
$\hmj{\co\wedge\exists\tvars.\cp}\env{\et_1}
{\scheme\tvars\cp{\ttyp_1}}$~\dlabel{h}. By \Rule{hmx-Let}, \dref{h} and
\dref{d} yield
$\hmj{\co\wedge\exists\tvars.\cp}\env{\elet\evar{\et_1}{\et_2}}{\ttyp_2}$.
\end{Proof}
%
\begin{theorem}
\label{theorem-hmd-complete}
If $\hmj\co\env\et\ttyp$ is a valid \hmx judgment, then it is derivable via
the rules of Figure~\ref{fig:HM(X)-2}.
\end{theorem}
%
\begin{Proof}
The proof is by induction on the \emph{weight} of a derivation of
$\hmj\co\env\et\ttyp$, where the weight of a derivation is the sum of the
weights of the rule instances that it involves, and the weight of a rule
instance is 2 if it is an instance of \Rule{hmx-Gen} or \Rule{hmx-Inst} and 1
otherwise.
%
The result is immediate in all cases but the following.  For the sake of
clarity, we write $\hmdj\co\env\et\ttyp$ for judgments which are derivable
via the rules of Figure~\ref{fig:HM(X)-2}.

\demoreset\proofcase\Rule{hmx-Var}. The rule's conclusion is
$\hmj\co\env\evid\ttyp$. Its first premise is $\env(\evid)=\ttyp$~\dlabel{f}.
(Its second premise is the tautology $\co\entails\ctrue$.) By
\Rule{hmd-VarInst}, \dref{f} implies $\hmdj\co\env\evid\ttyp$, which is the
goal.

\demoreset\proofcase\Rule{hmx-Let}. The rule's conclusion is
$\hmj\co\env{\elet\evar{\et_1}{\et_2}}\ttyp$~\dlabel{c}. Its premises are
$\hmj\co\env{\et_1}\ts$~\dlabel{g} and
$\hmj\co{(\env;\evar:\ts)}{\et_2}\ttyp$~\dlabel{d}. Applying the induction
hypothesis to \dref{d} yields
$\hmdj\co{(\env;\evar:\ts)}{\et_2}\ttyp$~\dlabel{dd}.
%
If $\ts$ is a monotype, then applying the induction hypothesis to \dref{g}
yields $\hmdj\co\env{\et_1}\ts$~\dlabel{dg}. By \Rule{hmd-LetGen},
instantiated with $\tvars=\varnothing$ and $\cp=\ctrue$, \dref{dg} and
\dref{dd} yield $\hmdj\co\env{\elet\evar{\et_1}{\et_2}}\ttyp$, which is the
goal.
%
If $\ts$ is not a monotype, then let us analyze the last rule in the
derivation of \dref{g}. It must be one of the following.

%
% This paragraph shows that an Inst;Gen sequence may be inserted
% at any point.
%
\proofsubcase\Rule{hmx-Var}. Then, $\et_1$ is a program identifier
$\evid$. Let us write $\ts=\scheme\tvars\cp{\ttyp_1}$, where
$\disjoint\tvars\ftv{\co,\env}$~\dlabel{djouf}. \Rule{hmx-Var}'s premises are
$\env(\evid)=\scheme\tvars\cp{\ttyp_1}$~\dlabel{un} and
$\co\entails\exists\tvars.\cp$~\dlabel{deux}. By \dref{un} and
\Rule{hmd-VarInst}, we have
$\hmdj{\co\wedge\cp}\env\evid{\ttyp_1}$~\dlabel{i}.  By \dref{deux}, $\co$ and
$\co\wedge\exists\tvars.\cp$ are equivalent. Thus, we may apply
\Rule{hmd-LetGen} to \dref{i}, \dref{djouf} and \dref{dd} and obtain
$\hmdj\co\env{\elet\evar{\et_1}{\et_2}}\ttyp$.

%
% This paragraph shows that Gen;(left) Let may be coalesced into LetGen.
%
\proofsubcase\Rule{hmx-Gen}. Then, we have
$\co\logeq\co_0\wedge\exists\tvars.\cp$ and $\ts=\scheme\tvars\cp{\ttyp_1}$.
\Rule{hmx-Gen}'s premises are
$\hmj{\co_0\wedge\cp}\env{\et_1}{\ttyp_1}$~\dlabel{u} and
$\disjoint\tvars{\ftv{\co_0,\env}}$~\dlabel{fu}. 
Applying the induction hypothesis to \dref{u} yields
$\hmdj{\co_0\wedge\cp}\env{\et_1}{\ttyp_1}$~\dlabel{pu}.
Applying \Rule{hmd-LetGen} to
\dref{pu}, \dref{fu} and \dref{dd} yields
$\hmdj\co\env{\elet\evar{\et_1}{\et_2}}\ttyp$.

%
% This paragraph shows that Exists;(left)Let may be turned into Let;Exists.
%
\proofsubcase\Rule{hmx-Exists}. Then, we have
$\co\logeq\exists\twars.\co_0$. \Rule{hmx-Exists}'s premises are
$\hmj{\co_0}\env{\et_1}\ts$~\dlabel{gl} and
$\disjoint\twars{\ftv{\env,\ts}}$~\dlabel{gr}. We may assume, \spdg,
$\disjoint\twars{\ftv\ttyp}$~\dlabel{dj}. By Lemmas~\ref{lemma-exists-entail}
and~\ref{lemma-hmx-weakening}, \dref{d} implies
$\hmj{\co_0}{(\env;\evar:\ts)}{\et_2}\ttyp$~\dlabel{dw}.  By applying
\Rule{hmx-Let} to \dref{gl} and \dref{dw}, we obtain a derivation of
$\hmj{\co_0}\env{\elet\evar{\et_1}{\et_2}}\ttyp$~\dlabel{x} that is lighter
than the derivation of \dref{c}. Thus, by the induction hypothesis, we have
$\hmdj{\co_0}\env{\elet\evar{\et_1}{\et_2}}\ttyp$~\dlabel{xd}.  By \dref{gr},
\dref{dj}, and \Rule{hmd-Exists}, \dref{xd} implies
$\hmdj\co\env{\elet\evar{\et_1}{\et_2}}\ttyp$.

\proofcase\Rule{hmx-Gen}. Because $\scheme\tvars\cp\ttyp$ is a monotype,
$\tvars$ must be empty and $\cp$ must be $\ctrue$. As a result, the rule's
premise coincides with its conclusion. The result follows by the induction
hypothesis.

\demoreset\proofcase\Rule{hmx-Inst}. The rule's conclusion is
$\hmj{\co\wedge\cp}\env\et\ttyp$~\dlabel{c}. Its premise is
$\hmj\co\env\et{\scheme\tvars\cp\ttyp}$~\dlabel{p}. If $\scheme\tvars\cp\ttyp$
is a monotype, then \dref{c} and \dref{p} coincide and the result is
immediate. If $\scheme\tvars\cp\ttyp$ is not a monotype, then let us analyze
the last rule in the derivation of \dref{p}. It must be one of the following.

%
% This paragraph coalesces Var;Inst into VarInst.
%
\proofsubcase\Rule{hmx-Var}. Then, $\et$ is a program identifier $\evid$. The
rule's first premise is $\env(\evid)=\scheme\tvars\cp\ttyp$~\dlabel{un}. By
\Rule{hmd-VarInst}, \dref{un} implies $\hmdj{\co\wedge\cp}\env\et\ttyp$.

%
% This paragraph shows that Gen;Inst may be turned into Sub;Exists, modulo
% the weakening lemma. (In other words, Gen;Inst may be turned into one
% application of the substitution lemma.)
% TEMPORARY à revoir, car la suppression de la séquence Gen;Inst est
% triviale à prouver si on utilise la règle hmx-Inst', qu'on a prouvée
% dans un exercice.
%
\proofsubcase\Rule{hmx-Gen}. Then, we have
$\co\logeq\co_0\wedge\exists\tvars.\cp$~\dlabel{eq}. \Rule{hmx-Gen}'s
premises are $\hmj{\co_0\wedge\rng\cp}\env\et{\rng\ttyp}$~\dlabel{pg} and
$\disjoint{\rng\tvars}{\ftv{\co_0,\env}}$~\dlabel{pd}, where $\rng$ is a
renaming that is fresh for $\ftv{\scheme\tvars\cp\ttyp}$.
We may assume, \spdg, $\disjoint{\rng\tvars}{\ftv{\cp,\ttyp}}$~\dlabel{fresh}.
By Lemma~\ref{lemma-hmx-weakening}, \dref{pg} implies
$\hmj{\co_0\wedge\rng\cp\wedge\tvarc=\rng\tvarc}
\env\et{\rng\ttyp}$~\dlabel{pgp}. By \Rule{C-Eq} and \Rule{hmx-Sub},
% TEMPORARY and because \rng is fresh for ...
\dref{pgp} implies
$\hmj{\co_0\wedge\cp\wedge\tvarc=\rng\tvarc}\env\et\ttyp$~\dlabel{pgq}.  
Applying \Rule{hmx-Exists} to  \dref{pgq},
\dref{pd}, and \dref{fresh} yields
$\hmj{\co_0\wedge\cp\wedge\exists\rng\tvars.(\tvarc=\rng\tvarc)}
\env\et\ttyp$, that is,
$\hmj{\co_0\wedge\cp}\env\et\ttyp$~\dlabel{pgr}. Furthermore,
the derivation of \dref{pgr} that we have just built is lighter
than that of \dref{c}. So, by the induction hypothesis, we have
$\hmdj{\co_0\wedge\cp}\env\et\ttyp$~\dlabel{pgs}. Because \dref{eq} implies
$\co\wedge\cp\logeq\co_0\wedge\cp$, \dref{pgs} is the goal
$\hmdj{\co\wedge\cp}\env\et\ttyp$.

%
% This paragraph shows that Inst;Exists may be turned into Exists;Inst.
%
\proofsubcase\Rule{hmx-Exists}. Then, we have $\co\logeq
\exists\twars.\co_0$. \Rule{hmx-Exists}'s premises are
$\hmj{\co_0}\env\et{\scheme\tvars\cp\ttyp}$~\dlabel{l} and
$\disjoint\twars{\ftv{\env,{\scheme\tvars\cp\ttyp}}}$~\dlabel{r}.  We may
assume, \spdg, $\disjoint\twars\tvars$~\dlabel{dj}.  Then, by \dref{r} and
\dref{dj}, we have $\disjoint\twars{\ftv{\env,\cp,\ttyp}}$~\dlabel{rp}. By
applying \Rule{hmx-Inst} to \dref{l}, we obtain a derivation of
$\hmj{\co_0\wedge\cp}\env\et\ttyp$~\dlabel{x} that is lighter than
the derivation of \dref{c}. Thus, by the induction hypothesis, we have
$\hmdj{\co_0\wedge\cp}\env\et\ttyp$~\dlabel{xd}. By \dref{rp},
\Rule{hmd-Exists}, and \Rule{C-ExAnd}, \dref{xd} implies
$\hmdj{\co\wedge\cp}\env\et\ttyp$.
\end{Proof}
\end{full}

\shortfull{This theorem shows}{Theorems~\ref{theorem-hmd-correct}
and~\ref{theorem-hmd-complete} show} that the rule sets of
Figures~\ref{fig:HM(X)-1} and~\ref{fig:HM(X)-2} derive the same monomorphic
judgments, that is, the same judgments of the form
$\hmj\co\env\et\ttyp$. The fact that judgments of the form
$\hmj\co\env\et\ts$, where $\ts$ is a not a monotype, cannot be derived using
the new rule set is a technical simplification, without deep significance.%
\begin{full}
The first two exercises below shed some light on this issue.

\begin{exercise}[\Easy]
Show that both rule sets lead to the same set of \emph{well-typed} programs.
\solref{hmx-hmd}
\end{exercise}

\begin{exercise}[\Easy]
Show that, if \Rule{hmx-Gen} is added to the rule set of
Figure~\ref{fig:HM(X)-2}, then both rule sets derive exactly the same
judgments.
\solref{hmd-gen}
\end{exercise}
\end{full}

\begin{exercise}[\Moderate, \nosolution]
Show that it is possible to simplify the presentation of Damas and Milner's
type system in an analogous manner. That is, define an alternate set of typing
rules for \dm, which allows deriving judgments of the form
$\dmj\env\et\ttyp$; then, show that this new rule set is equivalent to the
previous one, in the same sense as above. Which auxiliary properties of \dm
does your proof require? A solution is given
by \fulllongcite{Clement86:MiniML}.
\end{exercise}

% --------------------------------------------------------------------------
%
% Je démontre l'équivalence entre DM et HM(=) *après* avoir donné la seconde
% formulation de HM(X) car cela me permet d'utiliser hmd-VarInst au lieu de
% de hmx-Var et hmx-Inst. Cela simplifie un tout petit peu la grande preuve car
% cela permet de supposer que $\unifier$ est un mgu de $\co$, et non un
% unificateur quelconque. Cela permet d'éviter un lemme auxiliaire dans le
% cas de la règle hmx-Exists.

\subsection*{Relating \hmx with Damas and Milner's Type System}
\label{section-hmeq-dm}

\index{Damas--Milner type system!relation with \hmx|(}

In order to explain our interest in \hmx, we wish to show that it is more
general than Damas and Milner's type system. Since \hmx really is a
{family} of type systems, we must make this statement more
precise. First, every member of the \hmx family contains \dm. Conversely, \dm
contains \hmeq, the constraint-based type system obtained by specializing \hmx
to the setting of an equality-only syntactic model.

The first of these assertions is easy to prove because the mapping from \dm
judgments to \hmx judgments is essentially the identity: every valid \dm
judgment may be viewed as a valid \hmx judgment under the trivial assumption
$\ctrue$. This statement relies on the fact that the \dm type scheme
$\dmscheme\tvars\ttyp$ is identified with the constrained type scheme
$\scheme\tvars\ctrue\ttyp$, so \dm type schemes (respectively\ environments) form a
subset of \hmx type schemes (respectively\ environments). Its proof is easy and
relies on Exercise~\ref{ex:hmx-Inst}.

\begin{theorem}
\label{theorem-hmx-encodes-dm}
If $\dmj\env\et\dms$ holds in \dm, then $\hmj\ctrue\env\et\dms$ holds in \hmx.
\end{theorem}
%
\begin{Proof}
The proof is by structural induction on a derivation of $\dmj\env\et\dms$.
In each proof case, we adopt the notations of Figure~\ref{fig:Damas-Milner}.
% TEMPORARY mentionner qu'on utilise les règles originales de HM(X)?

\demoreset\proofcase\Rule{dm-Var}. The rule's conclusion is
$\dmj\env\evid\dms$. Its premise is $\env(\evid)=\dms$~\dlabel{p}.  By
definition and by \Rule{C-Ex*}, the constraint $\exists\dms$ is equivalent to
$\ctrue$. By applying \Rule{hmx-Var} to \dref{p} and to the assertion
$\ctrue\entails\ctrue$, we obtain $\hmj\ctrue\env\evid\dms$.

\proofcases\Rule{dm-Abs}, \Rule{dm-App}, \Rule{dm-Let}. By the induction
hypothesis and by \Rule{hmx-Abs}, \Rule{hmx-App} or \Rule{hmx-Let}, respectively.

\demoreset\proofcase\Rule{dm-Gen}. The rule's conclusion is
$\dmj\env\et{\dmscheme\tvars\ttyp}$. Its premises are
$\dmj\env\et\ttyp$~\dlabel{p} and $\disjoint\tvars{\ftv\env}$~\dlabel{h}.
Applying the induction hypothesis to~\dref{p} yields
$\hmj\ctrue\env\et\ttyp$~\dlabel{hp}. Furthermore, \dref{h} implies
$\disjoint\tvars{\ftv{\ctrue,\env}}$~\dlabel{hh}. By \Rule{hmx-Gen}, \dref{hp}
and \dref{hh} yield $\hmj\ctrue\env\et{\scheme\tvars\ctrue\ttyp}$.

\demoreset\proofcase\Rule{dm-Inst}. The rule's conclusion is
$\dmj\env\et{\subst\tvarc\ttypc\ttyp}$. Its premise is
$\dmj\env\et{\dmscheme\tvars\ttyp}$. Applying the induction hypothesis to it
yields $\hmj\ctrue\env\et{\scheme\tvars\ctrue\ttyp}$, which by
Exercise~\ref{ex:hmx-Inst} implies
$\hmj\ctrue\env\et{\subst\tvarc\ttypc\ttyp}$.
\end{Proof}

We are now interested in proving that \hmeq, as defined above, is contained
within \dm. To this end, we must translate every \hmeq judgment to a \dm
judgment. It turns out that this is possible if the original judgment's
constraint assumption is {satisfiable}. The translation relies on the
fact that the definition of \hmeq assumes an equality-only syntactic
model. Indeed, in that setting, every satisfiable constraint admits a most
general unifier (Definition~\ref{def-mgu}), whose properties we make essential
use of.

% TEMPORARY
% la preuve ne serait-elle pas plus simple sur une version de HM(X) en style
% substitutions (hmx-Inst')? cf. sketch de preuve de Chris Skalka dans l'article TOPLAS
% L'idée étant que dans cette formulation du système, on peut appliquer une
% substitution à une dérivation HM(X) sans en changer la structure (à vérifier);
% on peut alors appliquer les substitutions nécessaires de façon gloutonne, ce
% qui permet de travailler toujours avec une contrainte $\ctrue$ en hypothèse...
% et de ne traduire que des schémas $\scheme\tvars\cp\ttyp$ tels que $\exists\tvars.\cp$
% soit équivalente à $\ctrue$.

\begin{short}
Unfortunately, due to lack of space, we cannot give the details of this
translation, which are fairly involved. Let us merely say that, given a type
scheme $\ts$ and an idempotent type substitution $\unifier$ such that
$\ftv\ts\subseteq\Dom\unifier$ and $\exists\unifier\entails\exists\ts$ hold,
the translation of $\ts$ under $\unifier$ is a \dm type scheme, written
$\dmt\ts$.  Its meaning is intended to be the same as that of the \hmx type
scheme $\unifier(\ts)$. For instance, under the identity substitution, the
translation of the \hmx type scheme $\scheme{\tvar\twar}{\tvar=\twar\arw\twar}\tvar$
is the \dm type scheme $\dmscheme\tzar{\tzar\arw\tzar}$.
The translation is extended to environments in such a
way that $\dmt\env$ is defined when $\ftv\env\subseteq\Dom\unifier$ holds.
\end{short}%
\begin{full}
We begin by explaining how an \hmeq is translated into a \dm type scheme. 
In fact, we must not only translate a type scheme, but also apply a type
substitution to it. Instead of separating these steps, we perform both at
once, and parameterize the translation by a type substitution $\unifier$. (It
does not appear that separating them would help.) The definition of $\dmt\ts$
is somewhat involved: it is given in the statement of the following lemma,
whose proof establishes that the definition is indeed well-formed.
%
\begin{lemma}
\label{lemma-hmeq-to-dm}
\demoreset Consider a type scheme $\ts$ and an idempotent type substitution
$\unifier$ such that $\ftv\ts\subseteq\Dom\unifier$~\dlabel{sigdom} and
$\exists\unifier\entails\exists\ts$~\dlabel{usig}. Write
$\ts=\scheme\tvars\cp\ttyp$, where $\disjoint\tvars\unifier$~\dlabel{x}. Then,
there exists a type substitution $\unifier'$ such that $\unifier'$ extends
$\unifier$, $\Dom{\unifier'}$ is $\Dom\unifier\cup\tvars$, and $\unifier'$ is
a most general unifier of $\exists\unifier\wedge\cp$. Let
$\twars=\ftv{\unifier'(\tvars)}\setminus\Range\unifier$. Then, the
\emph{translation} of $\ts$ under $\unifier$, written $\dmt\ts$, is the \dm
type scheme $\dmscheme\twars{\unifier'(\ttyp)}$. This is a well-formed
definition. Furthermore, $\ftv{\dmt\ts}\subseteq \Range\unifier$ holds.
\end{lemma}
%
\begin{Proof}
By \dref{usig}, $\exists\unifier$ is equivalent to
$\exists\unifier\wedge\exists\ts$, which may be written
$\exists\unifier\wedge\exists\tvars.\cp$. By \dref{x} and \Rule{C-ExAnd}, this
is $\exists\tvars.(\exists\unifier\wedge\cp)$. Thus, because $\unifier$ is a
most general unifier of $\exists\unifier$, $\unifier$ is also a most general
unifier of $\exists\tvars.(\exists\unifier\wedge\cp)$~\dlabel{mgu}.
Furthermore, $\ftv{\exists\tvars.(\exists\unifier\wedge\cp)}$ is
$\ftv{\exists\unifier\wedge\exists\ts}$, which by definition of
$\exists\unifier$ and by \dref{sigdom} is a subset of
$\Dom\unifier$~\dlabel{foo}. By \dref{mgu}, \dref{x}, \dref{foo}, and
Lemma~\ref{lemma-mgu-exists}, there exists a type substitution $\unifier'$
such that $\unifier'$ extends $\unifier$~\dlabel{ext} and $\unifier'$ is a
most general unifier of $\exists\unifier\wedge\cp$~\dlabel{mgup} and
$\Dom{\unifier'}=\Dom\unifier\cup\tvars$~\dlabel{domu}.

Let us now define $\twars=\ftv{\unifier'(\tvars)}\setminus\Range\unifier$ and
$\dmt\ts=\dmscheme\twars{\unifier'(\ttyp)}$. By \dref{sigdom}, we have
$\ftv\ttyp\subseteq\tvars\cup\Dom\unifier$. Applying $\unifier'$ and
exploiting \dref{ext}, we find
$\ftv{\unifier'(\ttyp)}\subseteq\ftv{\unifier'(\tvars)}\cup\Range\unifier$,
which by definition of $\twars$ may be written
$\ftv{\unifier'(\ttyp)}\subseteq\twars\cup\Range\unifier$. Subtracting
$\twars$ on each side, we find
$\ftv{\dmt\ts}\subseteq\Range\unifier$~\dlabel{range}.

To show that the definition of $\dmt\ts$ is valid, there remains to show that
it does not depend on the choice of $\tvars$ or $\unifier'$. To prove the
former, it suffices to establish $\disjoint\tvars{\ftv{\dmt\ts}}$, which
indeed follows from \dref{x} and \dref{range}. As for the latter, because of
the constraints imposed by \dref{ext}, \dref{mgup}, and \dref{domu}, 
and by Lemma~\ref{lemma-mgu-intersection}, distinct
choices of $\unifier'$ may differ only by a renaming of
$\ftv{\unifier'(\tvars)}\setminus\Range\unifier$, that is, $\twars$. So, we
must check $\disjoint\twars{\ftv{\dmt\ts}}$, which holds by definition.
\end{Proof}

Note that if $\ts$ is in fact a type $\ttyp$, where
$\ftv\ttyp\subseteq\Dom\unifier$, then $\tvars$ is empty, so $\unifier'$ is
$\unifier$, $\twars$ is empty, and $\dmt{\ttyp}=\unifier(\ttyp)$. In other
words, the translation of a type under $\unifier$ is its image through
$\unifier$. More generally, the translation of an unconstrained type scheme
(that is, a type scheme whose constraint is $\ctrue$) is its image through
$\unifier$, as stated by the following exercise.
%
\begin{exercise}[\Easy, \nosolution]
Prove that $\dmt{\dmscheme\tvars\ttyp}$, when defined, is
$\unifier(\dmscheme\tvars\ttyp)$.
\end{exercise}

The translation becomes more than a mere type substitution when applied to
a nontrivial constrained type scheme. Some examples of this situation are
given below.
%
\begin{example}
Let $\ts=\scheme{\tvar\twar}{\tvar=\twar\arw\twar}\tvar$. Let $\unifier$ be
the identity substitution. The type scheme $\ts$ is closed and the constraint
$\exists\ts$ is equivalent to $\ctrue$, so $\dmt\ts$ is defined. We must find
a type substitution $\unifier'$ whose domain is $\tvar\twar$ and that is a
most general unifier of $\tvar=\twar\arw\twar$. All such substitutions are of
the form $\multisubst{\tvar\mapsto(\tzar\arw\tzar),\twar\mapsto\tzar}$, where
$\tzar$ is fresh. We have $\ftv{\unifier'(\tvar\twar)}=\tzar$, whence
$\dmt\ts=\dmscheme\tzar{\tzar\arw\tzar}$. Note that the choice of $\tzar$ does
not matter, since it is bound in $\dmt\ts$. Roughly speaking, the effect of
the translation was to replace the body $\tvar$ of the constrained type scheme
with its most general solution under the constraint
$\tvar=\twar\arw\twar$. 

Let $\ts=\scheme{\tvar\twar_1}{\tvar=\twar_1\arw\twar_2}\tvar$. Let
$\unifier=\subst{\twar_2}{\tzar_2}$. We have
$\ftv\ts=\twar_2\subseteq\Dom\unifier$. The constraint $\exists\ts$ is
equivalent to $\ctrue$, so $\dmt\ts$ is defined. We must find a type
substitution $\unifier'$ whose domain is $\tvar\twar_1\twar_2$ that extends
$\unifier$ and that is a most general unifier of $\tvar=\twar_1\arw\twar_2$.
All such substitutions are of the form
$\multisubst{\tvar\mapsto(\tzar_1\arw\tzar_2),\twar_1\mapsto\tzar_1,
  \twar_2\mapsto\tzar_2}$, where $\tzar_1$ is fresh. We have
$\ftv{\unifier'(\tvar\twar_1)}\setminus\Range\unifier=
\tzar_1\tzar_2\setminus\tzar_2=\tzar_1$, whence
$\dmt\ts=\dmscheme{\tzar_1}{\tzar_1\arw\tzar_2}$. The type variable $\tzar_2$
is \emph{not} universally quantified---even though it appears in the image of
$\tvar$, which \emph{was} universally quantified in $\ts$---because $\tzar_2$
is the image of $\twar_2$, which was free in $\ts$.
\end{example}

Before attacking the main theorem, let us establish a couple of technical
properties of the translation. First, $\dmt\ts$ is insensitive to the behavior
of $\unifier$ outside $\ftv\ts$, a natural property, since our informal intent
is for $\unifier$ to be applied to $\ts$.
%
\begin{lemma}
\label{lemma-transl-extend}
If $\unifier_1$ and $\unifier_2$ coincide on $\ftv\ts$, then
$\dmtrans\ts{\unifier_1}$ and $\dmtrans\ts{\unifier_2}$ are either both
undefined, or both defined and identical.
\end{lemma}
%
% TEMPORARY This proof is horrible. Can it be simplified?
\begin{Proof}
Let $\unifier_1$ and $\unifier_2$ coincide on $\ftv\ts$. Then, there exists a
type substitution $\unifier$ such that $\Dom\unifier=\ftv\ts$ and
$\unifier_i=\unifier\cup\subst{\tvarc_i}{\ttypc_i}$, for $i\in\{1,2\}$. Then,
$\exists\unifier$ is equivalent to $\exists\tvars_i.\exists\unifier_i$. It is
not difficult to prove that, as a result, $\exists\unifier\entails\exists\ts$
is equivalent to $\exists\unifier_i\entails\exists\ts$. Thus,
$\dmtrans\ts{\unifier_1}$ and $\dmtrans\ts{\unifier_2}$ are either both
undefined, or both defined.

\demoreset Let us assume that $\dmtrans\ts{\unifier_i}$ is defined, for
$i\in\{1,2\}$. Let $\ts=\scheme\tvars\cp\ttyp$, where
$\disjoint\tvars{\unifier_i}$. Then, $\dmt\ts$ is defined as well; let
$\unifier'$ be the auxiliary substitution involved in its definition.
$\unifier'$ is a most general unifier of $\exists\unifier\wedge\cp$, that is,
$\exists\tvars_i.\exists\unifier\wedge\cp$, which by \Rule{C-ExAnd} is
$\exists\tvars_i.(\exists\unifier\wedge\cp)$~\dlabel{hop}. We have
$\disjoint{\tvars_i}{\Dom\unifier\cup\tvars}=\Dom{\unifier'}$~\dlabel{un}.
Furthermore, we have $\disjoint{\tvars_i}{\Range\unifier}$, which allows
requiring, \spdg, $\disjoint{\tvars_i}{\Range{\unifier'}}$~\dlabel{deux}.  We
have $\ftv{\exists\tvars_i.(\exists\unifier_i\wedge\cp)}\subseteq
\Dom\unifier\cup\tvars=\Dom{\unifier'}$~\dlabel{trois}. By \dref{hop},
\dref{un}, \dref{deux}, \dref{trois}, and Lemma~\ref{lemma-mgu-exists}, there
exists a substitution $\unifier'_i$ such that $\unifier'_i$ extends
$\unifier'$ and $\unifier'_i$ is a most general unifier of
$\exists\unifier_i\wedge\cp$ and
$\Dom{\unifier'_i}=\Dom{\unifier'}\cup\tvars_i$. Thus, $\unifier'_i$ is a
suitable auxiliary substitution for use in the definition of
$\dmtrans\ts{\unifier_i}$. Furthermore, by \aconv of $\unifier'_i$, we may
require $\disjoint{\Range{\unifier'_i}\setminus\Range{\unifier'}}
{\ftv{\unifier'_i(\tvars)}}$, which may be written
$\disjoint{\Range{\unifier_i}\setminus\Range\unifier}
{\ftv{\unifier'_i(\tvars)}}$~\dlabel{sauve}. Besides,
$\unifier'_1$ and $\unifier'_2$ coincide on $\Dom{\unifier'}=\Dom\unifier\cup
\tvars$~\dlabel{coincide}. Using \dref{sauve} and \dref{coincide}, it is easy
to check that $\dmtrans\ts{\unifier_1}$ and $\dmtrans\ts{\unifier_2}$
coincide.
% Ouf! Qui aurait cru que ce serait si dur?
\end{Proof}

Second, if $\co\entails\ccall\ts{\ttyp'}$ holds, then the translations of
$\ts$ and $\ttyp'$ under a most general unifier of $\co$ are in Damas and
Milner's instance relation. One might say, roughly speaking, that the instance
relation is preserved by the translation.

\begin{lemma}
\label{lemma-transl-inst}
\demoreset Let $\ftv{\ts,\ttyp'}\subseteq\Dom\unifier$~\dlabel{sigdom} and
$\exists\unifier\entails\exists\ts$~\dlabel{usig}. Let
$\exists\unifier\entails\ccall\ts{\ttyp'}$~\dlabel{hminst}. Then,
$\unifier(\ttyp')$ is an instance of the \dm type scheme $\dmt\ts$.
\end{lemma}
%
\begin{Proof}
Write $\ts=\scheme\tvars\cp\ttyp$, where $\disjoint\tvars\unifier$~\dlabel{x}
and $\disjoint\tvars{\ftv{\ttyp'}}$~\dlabel{xt}. By \dref{sigdom},
\dref{usig}, and \dref{x}, one may define $\unifier'$, $\twars$, and $\dmt\ts$
exactly as in the statement of Lemma~\ref{lemma-hmeq-to-dm}. By \dref{xt} and
Definition~\ref{def-ts-instance}, \dref{hminst} is synonymous with
$\exists\unifier\entails\exists\tvars.(\cp\wedge\ttyp=\ttyp')$. Reasoning in
the same manner as in the first paragraph of the proof of
Lemma~\ref{lemma-hmeq-to-dm}, we find that there exists a type substitution
$\unifier''$ such that $\unifier''$ extends $\unifier$, $\Dom{\unifier''}$ is
$\Dom\unifier\cup\tvars$, and $\unifier''$ is a most general unifier of
$\exists\unifier\wedge\cp\wedge\ttyp=\ttyp'$.

We have $\Dom{\unifier'}=\Dom{\unifier''}$~\dlabel{samedom}. Furthermore,
$\unifier'$ is a most general unifier of $\exists\unifier\wedge\cp$, while
$\unifier''$ is a most general unifier of $\exists\unifier\wedge\cp\wedge
\ttyp=\ttyp'$, which implies
$\exists\unifier''\entails\exists\unifier'$~\dlabel{e}. By
Lemma~\ref{lemma-entailment-is-refinement}, $\unifier''$ refines $\unifier'$.
That is, there exists a type substitution $\refiner$ such that $\unifier''$ is
the restriction of $\refiner\circ\unifier'$ to
$\Dom\unifier\cup\tvars$~\dlabel{refine}. We may require
$\Dom\refiner\subseteq\Range\unifier\cup\ftv{\unifier'(\tvars)}$~\dlabel{dom}
without compromising \dref{refine}.

Consider $\tvar\in\Dom\unifier$. Because $\unifier''$ extends $\unifier$, we
have $\unifier''(\tvar)=\unifier(\tvar)$~\dlabel{un}. Furthermore, by
\dref{refine}, we have $\unifier''(\tvar)=(\refiner\circ\unifier')(\tvar)=
(\refiner\circ\unifier)(\tvar)$~\dlabel{deux}. Using \dref{un} and
\dref{deux}, we find $\unifier(\tvar)=\refiner(\unifier(\tvar))$. Because this
holds for every $\tvar\in\Dom\unifier$, $\refiner$ must be the identity over
$\Range\unifier$; that is,
$\disjoint{\Dom\refiner}{\Range\unifier}$~\dlabel{id} holds. Combining
\dref{dom} and \dref{id}, we find
$\Dom\refiner\subseteq\ftv{\unifier'(\tvars)}\setminus\Range\unifier$,
that is, $\Dom\refiner\subseteq\twars$~\dlabel{y}.

By construction of $\unifier''$, we have
$\exists\unifier''\entails\ttyp=\ttyp'$.  By Lemma~\ref{lemma-entail-subst},
this implies
$\unifier''(\exists\unifier'')\entails\unifier''(\ttyp)=\unifier''(\ttyp')$,
which by Lemma~\ref{lemma-mgu-self} may be read
$\ctrue\entails\unifier''(\ttyp)=\unifier''(\ttyp')$.  By
Lemma~\ref{lemma-true-entails}, $\unifier''(\ttyp)$ and $\unifier''(\ttyp')$
coincide. Because by~\dref{sigdom} $\ftv\ttyp$ is a subset of
$\Dom\unifier\cup\tvars$ and by 
\dref{refine}, the former may be written $\refiner(\unifier'(\ttyp))$.  By
\dref{sigdom} and because $\unifier''$ extends $\unifier$, the latter is
$\unifier(\ttyp')$. Thus, we have
$\refiner(\unifier'(\ttyp))=\unifier(\ttyp')$.
Together with \dref{y}, this establishes that $\unifier(\ttyp')$ is an
instance of $\dmscheme\twars{\unifier'(\ttyp)}$, that is, $\dmt\ts$.
\end{Proof}

We extend the translation to environments as follows. $\dmt\noenv$ is
$\noenv$. If $\exists\unifier\entails\exists\ts$ holds, then
$\dmt{\env;\evid:\ts}$ is $\dmt\env;\evid:\dmt\ts$, otherwise it is
$\dmt\env$. Notice that $\dmt \env$ contains fewer bindings than $\env$, which
ensures that bindings $x : \ts$ for which $\exists\unifier\entails\exists\ts$
does not hold will not be used in the translation. Note that
$\dmt\env$ is defined when $\ftv\env\subseteq\Dom\unifier$ holds.
\end{full}
We are now ready to state the main theorem. 
%
\begin{theorem}
\label{theorem-dm-encodes-hm}
Let $\hmj\co\env\et\ts$ hold in \hmeq. Let $\unifier$ be a most general
unifier of $\co$ such that $\ftv{\env,\ts}\subseteq\Dom\unifier$. Then,
$\dmj{\dmt\env}\et{\dmt\ts}$ holds in \dm.
\end{theorem}
%
Note that, by requiring
$\unifier$ to be a most general unifier of $\co$, we also require $\co$ to be
satisfiable. Judgments that carry an unsatisfiable constraint cannot be
translated.
%
\begin{Proof}
Let us first note that, by Lemma~\ref{lemma-hmx-invariant}, we have
$\co\entails\exists\ts$. This may be written
$\exists\unifier\entails\exists\ts$, which guarantees that $\dmt\ts$ is
defined.
%
The proof is by structural induction on an \hmeq typing derivation.
We assume that the derivation is expressed in terms of the rules of
Figure~\ref{fig:HM(X)-2}, but split \Rule{hmd-LetGen} into
\Rule{hmx-Let} and \Rule{hmx-Gen} for the sake of readability.

\demoreset\proofcase\Rule{hmd-VarInst}. The rule's conclusion is
$\hmj{\co\wedge\cp}\env\evid\ttyp$. By hypothesis, $\unifier$ is a most
general unifier of $\co\wedge\cp$~\dlabel{mgu}, and
$\ftv\ttyp\subseteq\Dom\unifier$~\dlabel{t} holds. The rule's premise is
$\env(\evid)=\ts$~\dlabel{p}, where $\ts$ stands for $\scheme\tvars\cp\ttyp$.
By \dref{mgu}, we have $\exists\unifier\logeq\co\wedge\cp\entails\cp\entails
\exists\tvars.\cp\logeq\exists\ts$~\dlabel{foo}. Furthermore, we have
$\ftv\ts\subseteq\ftv\env\subseteq\Dom\unifier$~\dlabel{bar}. These facts show
that $\dmt\ts$ is defined. Together with \dref{p}, this implies
$\dmt\env(\evid)=\dmt\ts$. By \Rule{dm-Var},
$\dmj{\dmt\env}\evid{\dmt\ts}$~\dlabel{q} follows. Now, by
Lemma~\ref{lemma-identity-instance}, we have $\cp\entails\ccall\ts\ttyp$,
which, combined with $\exists\unifier\entails\cp$, yields
$\exists\unifier\entails\ccall\ts\ttyp$~\dlabel{inst}. By \dref{inst},
\dref{foo}, \dref{bar}, \dref{t}, and Lemma~\ref{lemma-transl-inst}, we find
that $\unifier(\ttyp)$ is an instance of $\dmt\ts$. Thus, applying
\Rule{dm-Inst} to \dref{q} yields $\dmj{\dmt\env}\et{\unifier(\ttyp)}$.

\proofcase\Rule{hmd-Abs}. The rule's conclusion is
$\hmj\co\env{\efun\evar\et}{\ttyp\arw\ttyp'}$. Its premise is
$\hmj\co{(\env;\evar:\ttyp)}\et{\ttyp'}$.
Applying the induction hypothesis to it yields
$\dmj{\dmt\env;\evar:\unifier(\ttyp)}\et{\unifier(\ttyp')}$. By
\Rule{dm-Abs}, this implies
$\dmj{\dmt\env}{\efun\evar\et}{\unifier(\ttyp)\arw\unifier(\ttyp')}$, that is,
$\dmj{\dmt\env}{\efun\evar\et}{\unifier(\ttyp\arw\ttyp')}$.

\proofcase\Rule{hmd-App}. By an extension of $\Dom\unifier$ to include
$\ftv\ttyp$, by the induction hypothesis, and by \Rule{dm-App}.

\proofcase\Rule{hmx-Let}. By an extension of $\Dom\unifier$ to include
$\ftv\ts$, by the induction hypothesis, and by \Rule{dm-Let}.

\demoreset\proofcase\Rule{hmx-Gen}. The rule's conclusion is
$\hmj{\co\wedge\exists\ts}\env\et\ts$, where $\ts$ stands for
$\scheme\tvars\cp\ttyp$. By hypothesis, $\unifier$ is a most general unifier
of $\co\wedge\exists\ts$~\dlabel{mgu}, and
$\ftv{\env,\ts}\subseteq\Dom\unifier$~\dlabel{sigdom} holds. The rule's
premises are $\hmj{\co\wedge\cp}\env\et\ttyp$~\dlabel{p} and
$\disjoint\tvars{\ftv{\co,\env}}$~\dlabel{f}. We may further assume, \spdg,
$\disjoint\tvars\unifier$~\dlabel{g}. Given \dref{mgu}, \dref{sigdom}, and
\dref{g}, we may define $\unifier'$ and $\twars$ exactly as in
Lemma~\ref{lemma-hmeq-to-dm}.  Then, $\unifier'$ is a most general unifier of
$\exists\unifier\wedge\cp$, that is, $\co\wedge\cp$.  Furthermore,
$\Dom{\unifier'}$ is $\Dom\unifier\cup\tvars$, which by \dref{sigdom} is a
superset of $\ftv{\env,\ttyp}$.  Thus, the induction hypothesis applies to
$\unifier'$ and to \dref{p}, yielding
$\dmj{\dmtrans\env{\unifier'}}\et{\unifier'(\ttyp)}$. Because $\unifier'$
extends $\unifier$, by \dref{sigdom} and by Lemma~\ref{lemma-transl-extend},
this may be read $\dmj{\dmt\env}\et{\unifier'(\ttyp)}$~\dlabel{k}. According
to Lemma~\ref{lemma-hmeq-to-dm}, we have
$\ftv{\dmt\env}\subseteq\Range\unifier$, which by construction of $\twars$
implies $\disjoint\twars{\ftv{\dmt\env}}$~\dlabel{gen}. By \Rule{dm-Gen},
\dref{k} and \dref{gen} yield
$\dmj{\dmt\env}\et{\dmscheme\twars{\unifier'(\ttyp)}}$, that is,
$\dmj{\dmt\env}\et{\dmt\ts}$.

\demoreset\proofcase\Rule{hmd-Sub}. The rule's conclusion is
$\hmj\co\env\et{\ttyp'}$. By hypothesis, $\unifier$ is a most general unifier
of $\co$~\dlabel{mgu}, and
$\ftv{\env,\ttyp'}\subseteq\Dom\unifier$~\dlabel{sigdom} holds. The goal is
$\dmj{\dmt{\env}}{\et}{\unifier(\ttyp')}$~\dlabel{goal}. The rule's premises
are $\hmj\co\env\et\ttyp$~\dlabel{p} and $\co\entails\ttyp=\ttyp'$~\dlabel{e}.
We may assume, \spdg, $\disjoint{\ftv\ttyp}{\Range\unifier}$~\dlabel{ehoui}.
Then, by \dref{ehoui} and Lemma~\ref{lemma-mgu-support}, we may extend the
domain of $\unifier$, so as to achieve
$\ftv\ttyp\subseteq\Dom\unifier$~\dlabel{hop}, without compromising \dref{mgu}
or \dref{sigdom} or affecting the goal \dref{goal}.  By \dref{mgu},
\dref{sigdom}, and \dref{hop}, the induction hypothesis applies to \dref{p},
yielding $\dmj{\dmt{\env}}{\et}{\unifier(\ttyp)}$~\dlabel{q}.  Now, thanks to
\dref{mgu}, \dref{e} may be read $\exists\unifier\entails\ttyp=\ttyp'$, which
by Lemmas~\ref{lemma-entail-subst} and~\ref{lemma-mgu-self} implies
$\ctrue\entails\unifier(\ttyp)= \unifier(\ttyp')$. Then,
Lemma~\ref{lemma-true-entails} shows that $\unifier(\ttyp)$ and
$\unifier(\ttyp')$ coincide. As a result, \dref{q} is the goal
\dref{goal}.

\demoreset\proofcase\Rule{hmd-Exists}. The rule's conclusion is
$\hmj{\exists\tvars.\co}\env\et\ttyp$. By hypothesis, $\unifier$ is a most
general unifier of $\exists\tvars.\co$~\dlabel{mg}, and
$\ftv{\env,\ttyp}\subseteq\Dom\unifier$~\dlabel{sigdom} holds. The rule's
premises are $\hmj\co\env\et\ttyp$~\dlabel{p} and
$\disjoint\tvars{\ftv{\env,\ttyp}}$. We may assume, \spdg,
$\disjoint\tvars\unifier$~\dlabel{wlog}. As in the previous case, we may
extend the domain of $\unifier$ to guarantee
$\ftv{\exists\tvars.\co}\subseteq\Dom\unifier$~\dlabel{wlog2}. By
\dref{mg}, \dref{wlog}, \dref{wlog2}, and Lemma~\ref{lemma-mgu-exists}, there
exists a type substitution $\unifier'$ such that $\unifier'$ extends
$\unifier$~\dlabel{ext} and $\unifier'$ is a most general unifier of $\co$.
Applying the induction hypothesis to $\unifier'$ and to \dref{p} yields
$\dmj{\dmtrans\env{\unifier'}}\et{\unifier'(\ttyp)}$. By \dref{sigdom},
\dref{ext}, and Lemma~\ref{lemma-transl-extend}, this may be read
$\dmj{\dmt\env}\et{\unifier(\ttyp)}$.
\end{Proof}

Together, Theorems~\ref{theorem-hmx-encodes-dm}
and~\ref{theorem-dm-encodes-hm} yield a precise correspondence between \dm and
\hmeq: there exists a compositional translation from each to the other. In
other words, they may be viewed as two equivalent formulations of a single
type system. One might also say that \hmeq is a constraint-based formulation
of \dm. Furthermore, Theorem~\ref{theorem-hmx-encodes-dm} states that every
member of the \hmx family is an extension of \dm. This explains our double
interest in \hmx, as an alternate formulation of \dm, which we believe is more
pleasant for reasons already discussed, and as a more expressive framework.

\index{Damas--Milner type system!relation with \hmx|)}

% TEMPORARY pointer explicitement qu'on a également démontré qu'on pouvait
% imprimer tout schéma HM(=) sous forme d'un schéma DM (affichage)

% ---------------------------------------------------------------------
\begin{short}
\section{Constraint Generation}
\label{section-constraint-generation}

\index{constraints!generation|(}


% En version courte, cette section regroupe et condense les sections
% ``PCB(X)'' et ``Constraint generation.''

We now explain how to reduce type inference problems for \hmx to constraint
solving problems. A type inference problem consists of a type environment
$\env$, an expression $\et$, and a type $\ttyp$ of kind $\normalkind$. The
problem is to determine whether there exists a satisfiable constraint $\co$
such that $\hmj\co\env\et\ttyp$ holds. A constraint solving problem consists
of a constraint $\co$. The problem is to determine whether $\co$ is
satisfiable. To reduce a type inference problem $(\env,\et,\ttyp)$ to a
constraint solving problem, we must produce a constraint $\co$ that is both
\emph{sufficient} and \emph{necessary} for $\hmj\co\env\et\ttyp$ to
hold. Below, we explain how to compute such a constraint, which we write
$\calculehmx\env\et\ttyp$. We check that it is indeed \emph{sufficient} by
proving \soundhmx\env\et\ttyp. That is, the constraint
$\calculehmx\env\et\ttyp$ is specific enough to guarantee that $\et$ has type
$\ttyp$ under environment $\env$. We say that constraint generation is
\emph{sound}. We check that it is indeed \emph{necessary} by proving that, for
every constraint $\co$, the validity of $\hmj\co\env\et\ttyp$ implies
$\co\entails\calculehmx\env\et\ttyp$. That is, every constraint that
guarantees that $\et$ has type $\ttyp$ under environment $\env$ is at least as
specific as $\calculehmx\env\et\ttyp$. We say that constraint generation is
\emph{complete}. Together, these properties mean that
$\calculehmx\env\et\ttyp$ is the \emph{least specific} constraint that
guarantees that $\et$ has type $\ttyp$ under environment~$\env$.

We now see how to reduce a type inference problem to a constraint solving
problem. Indeed, if there exists a satisfiable constraint $\co$ such that
$\hmj\co\env\et\ttyp$ holds, then, by the completeness property,
$\co\entails\calculehmx\env\et\ttyp$ holds, so $\calculehmx\env\et\ttyp$ is
satisfiable. Conversely, by the soundness property, if
$\calculehmx\env\et\ttyp$ is satisfiable, then we have a satisfiable
constraint $\co$ such that $\hmj\co\env\et\ttyp$ holds. In other words, $\et$
is well-typed with type $\ttyp$ under environment $\env$ if and only if
$\calculehmx\env\et\ttyp$ is satisfiable.

The reader may be somewhat puzzled by the fact that our formulation of the
type inference problem requires an appropriate type $\ttyp$ to be known in
advance, whereas the very purpose of type inference seems to consist in
\emph{discovering} the type of $\et$! In other words, we have made $\ttyp$ an
\emph{input} of the constraint generation algorithm, instead of an
\emph{output}. Fortunately, this causes no loss of generality, because it is
possible to let $\ttyp$ be a type variable $\tvar$, chosen fresh for $\env$.
Then, the constraint produced by the algorithm will contain information about
$\tvar$. This is the point of the following exercise.
%
\begin{exercise}[\Recommended, \QuickCheck]
Let $\tvar\not\in\ftv\env$. Show that, if there exist a satisfiable constraint
$\co$ and a type $\ttyp$ such that $\hmj\co\env\et\ttyp$ holds, then there
exists a satisfiable constraint $\co'$ such that $\hmj{\co'}\env\et\tvar$
holds. Conclude that, given a closed environment $\env$ and an arbitrary type
variable $\tvar$, the term $\et$ is well-typed within $\env$ if and only if
$\calculehmx\env\et\tvar$ is satisfiable.
\solref{typability}
\end{exercise}
%
This shows that providing $\ttyp$ as an input to the constraint generation
procedure is not essential. We adopt this style because it is convenient. A
somewhat naive alternative would be to provide $\env$ and $\et$ only, and to
have the procedure return both a constraint $\co$ and a type
$\ttyp$ \cite{sulzmann-mueller-zenger-99}. It turns out that this does not
quite work, because $\co$ and $\ttyp$ may mention ``fresh'' variables, which
we must be able to quantify over, if we are to avoid an informal treatment of
``freshness.'' Thus, the true alternative is to provide $\env$ and $\et$ only
and to have the procedure return a \emph{type scheme} $\ts$ \cite{POPL::BourdoncleM1997,bonniot-02}.

\index{principal type schemes}%
The existence of a sound and complete constraint generation procedure is the
analog of the existence of \emph{principal type schemes} in classic
presentations of \MLtype \cite{DAMAS82}. Indeed, a principal type
scheme is least specific in the sense that all valid types are substitution
instances of it.  Here, the constraint $\calculehmx\env\et\ttyp$ is least
specific in the sense that all valid constraints entail it. More about
principal types and principal typings may be found in \longcite{Jim96} and
\longcite{wells-02}. 
\index{principal typings}%

How do we perform constraint generation? A standard
approach \cite{sulzmann-mueller-zenger-99, bonniot-02} is to define
$\calculehmx\env\et\ttyp$ by induction on the structure of $\et$. At every
\kwlet node, following \Rule{hmd-LetGen}, part of the current constraint,
namely $\cp$, is turned into a type scheme, namely $\scheme\tvars\cp\ttyp$,
which is used to extend the environment. Then, at every occurrence of the
program variable that was bound at this \kwlet node, following
\Rule{hmd-VarInst}, this type scheme is retrieved from the environment, and a
copy of $\cp$ is added back to the current constraint. If such an approach is
adopted, it is important to {simplify} the type scheme
$\scheme\tvars\cp\ttyp$ {before} it is stored in the environment, because
it would be inefficient to copy an unsimplified constraint. In other words,
in an efficient implementation of this standard approach, constraint
generation and constraint simplification cannot be separated.

{Type scheme introduction and elimination constraints}, which we
introduced in \S\ref{section-constraints} but did not use in the specification
of \hmx, are intended as a means of solving this problem. By extending our
vocabulary, we are able to achieve the desired separation between constraint
generation, on the one hand, and constraint solving and simplification, on the
other hand, without compromising efficiency. Indeed, by exploiting these new
constraint forms, we may define a constraint generation procedure whose time
and space complexity is linear, because it no longer involves copying
subconstraints back and forth between the environment and the constraint that
is being generated. (It is then up to the constraint solver to perform
simplification and copying, if and when necessary.) In fact, the environment
is suppressed altogether: we define $\calcule\et\ttyp$ by induction on the
structure of $\et$---notice the absence of the parameter $\env$. Then, the
constraint $\calculehmx\env\et\ttyp$ discussed above becomes syntactic sugar
for $\cxlet\env{\calcule\et\ttyp}$. We now employ the full constraint
language: the program identifiers that appear free in $\et$ may also appear
free in $\calcule\et\ttyp$, as part of instantiation constraints. They become
bound when $\calcule\et\ttyp$ is placed within the context $\cxlet\env\chole$.
A similar approach to constraint generation appears in \longcite{mueller-94}.

\begin{widefigure}
\TTtoprule
\vspace*{-2ex}
\begin{bnf}
    \calcule\evid\ttyp
\eq \ccall\evid\ttyp
\\
\calcule{\efun\evar\et}\ttyp
\eq \exists\tvar_1\tvar_2.(
      \clambda\evar{\tvar_1}{\calcule\et{\tvar_2}}
      \wedge
      \tvar_1\arw\tvar_2\subtype\ttyp 
) \\
\calcule{\eapp{\et_1}{\et_2}}\ttyp
\eq \exists\tvar_2.(
       \calcule{\et_1}{\tvar_2\arw\ttyp} 
       \wedge
       \calcule{\et_2}{\tvar_2}
) \\
\calcule{\elet\evar{\et_1}{\et_2}}\ttyp
\eq \cxlet{\evar:\scheme\tvar{\calcule{\et_1}\tvar}\tvar}{\calcule{\et_2}\ttyp}
\end{bnf}%
\vspace*{-3ex}
\TTbottomrule
\vspace*{-1ex}
\bcpcaption{genrules}{Constraint generation}
\end{widefigure}

The defining equations for $\calcule\et\ttyp$ appear in
Figure~\ref{fig:genrules}. We refer to them as the \emph{constraint generation
rules}. The definition is qu2te terse and certainly simpler than the
declarative specification of \hmx given in Figure~\ref{fig:HM(X)-1}; yet, we
prove below that the two are equivalent.

Before explaining the definition, we state the requirements that bear on the
type variables $\tvar_1$, $\tvar_2$, and $\tvar$, which appear bound in the
right-hand sides of the second, third, and fourth equations. These type
variables must have kind~$\normalkind$. They must be chosen distinct (that is,
$\tvar_1\not=\tvar_2$ in the second equation) and fresh for the objects that
appear on the left-hand side---that is, {the type variables that appear
bound in an equation's right-hand side must not occur free in the term and
type that appear in the equation's left-hand side.} Provided this restriction
is obeyed, different choices of $\tvar_1$, $\tvar_2$, and $\tvar$ lead to
$\alpha$-equivalent constraints---that is, to the same constraint, since we
identify objects up to \aconv---which guarantees that the above equations make
sense. Since expressions do not have free type variables, the
freshness requirement may be simplified to: type variables that appear bound
in an equation's right-hand side must not appear free in $\ttyp$. However,
this simplification would be rendered invalid by the introduction of open type
annotations within expressions. Note that we are able to state a
{precise} (as opposed to informal) freshness requirement. This is made
possible by the fact that $\calcule\et\ttyp$ has no free type variables other
than those of $\ttyp$, which in turn depends on our explicit use of
existential quantification to limit the scope of auxiliary variables.

Let us now review the four equations. The first equation may be read:
\emph{$\evid$ has type $\ttyp$ if and only if $\ttyp$ is an instance of the
type scheme associated with $\evid$.} Note that we no longer consult the type
scheme associated with $\evid$ in the environment---indeed, there is no
environment. Instead, we merely generate an instantiation constraint, where
$\evid$ appears free. (For this reason, every program identifier that occurs
free within $\et$ typically also occurs free within $\calcule\et\ttyp$.) This
constraint acquires its full meaning when it is later placed within a
context of the form $\cxlet{\evid:\ts}\chole$. This equation roughly
corresponds to \Rule{hmd-VarInst}. The second equation may be read:
\emph{$\efun\evar\et$ has type $\ttyp$ if and only if, for some $\tvar_1$ and
$\tvar_2$, (i) under the assumption that $\evar$ has type $\tvar_1$, $\et$ has
type $\tvar_2$, and (ii) $\ttyp$ is a supertype of $\tvar_1\arw\tvar_2$.}
Here, the types associated with $\evar$ and $\et$ must be fresh type
variables, namely $\tvar_1$ and $\tvar_2$, because we cannot in general guess
them. These type variables are \emph{bound} so as to guarantee that the
generated constraint is unique up to \aconv. They are \emph{existentially}
bound because we intend the constraint solver to discover their
value. Condition (i) is expressed by the subconstraint
$\clambda\evar{\tvar_1}{\calcule\et{\tvar_2}}$. This makes sense as follows.
The constraint $\calcule\et{\tvar_2}$ typically contains a number of instantiation
constraints bearing on $\evar$, of the form $\ccall\evar{\ttyp_i}$.  By
wrapping it within the context $\clambda\evar{\tvar_1}\chole$, we effectively
require every $\ttyp_i$ to be a supertype of $\tvar_1$. Note that
$\evar$ does not occur free in the constraint
$\clambda\evar{\tvar_1}{\calcule\et{\tvar_2}}$, which is necessary for
well-formedness of the definition, since it does not occur free in
$\efun\evar\et$. This equation roughly corresponds to \Rule{hmd-Exists},
\Rule{hmd-Abs}, and \Rule{hmd-Sub}. The third equation may be read:
\emph{$\eapp{\et_1}{\et_2}$ has type $\ttyp$ if and only if, for some
$\tvar_2$, $\et_1$ has type $\tvar_2\arw\ttyp$ and $\et_2$ has type
$\tvar_2$.} Here, the fresh type variable $\tvar_2$ stands for the unknown
type of $\et_2$. This equation roughly corresponds to \Rule{hmd-App}. The last
equation, which roughly corresponds to \Rule{hmd-LetGen}, may be read:
\emph{$\elet\evar{\et_1}{\et_2}$ has type $\ttyp$ if and only if, under the
assumption that $\evar$ has every type $\tvar$ such that
$\calcule{\et_1}{\tvar}$ holds, $\et_2$ has type $\ttyp$.} As in the case of
$\lambda$-abstractions, the instantiation constraints bearing on $\evar$ that
appear within $\calcule{\et_2}\ttyp$ are given a meaning via a \kwd{let}
prefix. The difference is that $\evar$ may now be assigned a type scheme, as
opposed to a monotype. An appropriate type scheme is built as follows. The
constraint $\calcule{\et_1}\tvar$ is the {least specific} constraint that
must be imposed on the fresh type variable $\tvar$ so as to make it a valid
type for $\et_1$. In other words, $\et_1$ has every type $\tvar$ such that
$\calcule{\et_1}\tvar$ holds, and none other. That is, the type scheme
$\scheme\tvar{\calcule{\et_1}\tvar}\tvar$, abbreviated $\ts$ in the following,
is a \emph{principal} type scheme for $\et_1$. It is interesting to note that
there is no question of {which} type variables to generalize. Indeed, by
construction, no type variables other than $\tvar$ may appear free in
$\calcule{\et_1}\tvar$, so we cannot generalize {more} variables.  On the
other hand, it is valid to generalize $\tvar$, since it does not appear free
anywhere else. This interesting simplification is inspired
by \fulllongcite{sulzmann-mueller-zenger-99}, where a similar technique is used.  Now,
what happens when $\calcule{\et_2}\ttyp$ is placed inside the context
$\cxlet{\evar:\ts}\chole$? When placed inside this context, an instantiation
constraint of the form $\ccall\evar{\ttyp'}$ acquires the meaning
$\ccall\ts{\ttyp'}$, which by definition of $\ts$ and by
Lemma~\ref{lemma-calcule-exists} (see below) is equivalent to
$\calcule{\et_1}{\ttyp'}$. Thus, the constraint produced by the fourth
equation simulates a textual expansion of the \kwlet construct, where every
occurrence of $\evar$ would be replaced with $\et_1$. Thanks to type scheme
introduction and instantiation constraints, however, this effect is achieved
without duplication of source code or constraints. In other words, constraint
generation has linear time and space complexity.
%
% TEMPORARY le baratin ci-dessus (version courte) contient quelques idées qui
% ne sont pas dans la version longue
%
\begin{exercise}[\QuickCheck, \nosolution]
Define the \emph{size} of an expression, of a type, and of a constraint,
viewed as abstract syntax trees. Check that the size of $\calcule\et\ttyp$ is
linear in the sum of the sizes of $\et$ and $\ttyp$.
\end{exercise}
%
\begin{exercise}[\Recommended, \QuickCheck\fullsolution]
\iffull{\solref{calcule-let}}
Compute and simplify, as best as you can, the constraint
$\calcule{\elet \efar {\efun \evar \evar} \eapp \efar
\efar}\ttyp$.
\end{exercise}

We now state several properties of constraint generation. We begin with
soundness, whose statement was explained above.
%
\begin{theorem}[Soundness]
\label{theorem/type-inference/soundness}
$\hmj{\cxlet\env{\calcule\et\ttyp}}\env\et\ttyp$.
\end{theorem}

The following lemmas are used in the proof of the completeness property and in
a number of other occasions. The first two state that $\calcule\et\ttyp$ is
\emph{covariant} with respect to $\ttyp$. Roughly speaking, this means that
enough subtyping constraints are generated to achieve completeness with
respect to \Rule{hmd-Sub}.
%
\begin{lemma}
\label{lemma-calcule-covariant}
$\calcule\et\ttyp \wedge \ttyp\subtype\ttyp' \entails
\calcule\et{\ttyp'}$.
\end{lemma}
%
\begin{lemma}
\label{lemma-calcule-exists}
$\tvar\not\in\ftv\ttyp$ implies
$\exists\tvar.(\calcule\et\tvar\wedge\tvar\subtype\ttyp) \logeq
\calcule\et\ttyp$.
\end{lemma}

The next lemma gives a simplified version of the second constraint generation
rule, in the specific case where the expected type is an arrow type. Thus,
fresh type variables need not be generated; one may directly use the arrow's
domain and codomain instead.
%
\begin{lemma}
\label{lemma-calcule-lambda}
$\calcule{\efun\evar\et}{\ttyp_1\arw\ttyp_2}$ is equivalent to
$\clambda\evar{\ttyp_1}{\calcule\et{\ttyp_2}}$.
\end{lemma}

We conclude with the completeness property. The theorem states that if, within
\hmx, $\et$ has type $\ttyp$ under assumptions $\co$ and $\env$, then $\co$
must be at least as specific as $\cxlet\env{\calcule\et\ttyp}$. The statement
requires $\co$ and $\env$ to have no free program identifiers, which is
natural, since they are part of an \hmx judgment. The hypothesis
$\co\entails\exists\env$ excludes the somewhat pathological situation where
$\env$ contains constraints not apparent in $\co$. This hypothesis vanishes
when $\env$ is the initial environment; see Definition~\ref{def-initial-env}.
%
\begin{theorem}[Completeness]
\label{theorem/type-inference/completeness}
Let $\co\entails\exists\env$. Assume $\fpv{\co,\env}=\varnothing$. If
$\hmj\co\env\et\ttyp$ holds in \hmx, then $\co$ entails
$\cxlet\env{\calcule\et\ttyp}$.
\end{theorem}

\end{short}

% ---------------------------------------------------------------------

\index{constraints!generation|)}

\begin{full}
\section{A Purely Constraint-Based Type System: \pcb}
\label{section-pcb}

In the previous section, we have presented \hmx, an elegant constraint-based
extension of Damas and Milner's type system. However, \hmx, as described
there, suffers from a drawback. A typing judgment involves both a constraint,
which represents an assumption about its free type variables, and an
environment, which represents an assumption about its free program
identifiers. At a \kwlet node, \Rule{hmd-LetGen} turns a part of the
current constraint, namely $\cp$, into a type scheme, namely
$\scheme\tvars\cp\ttyp$, and stores it into the environment. Then, at every
occurrence of the \kwlet-bound variable, \Rule{hmd-VarInst} retrieves this
type scheme from the environment and adds a copy of $\cp$ back to the current
constraint. If such an approach is adopted, it is important to \emph{simplify}
the type scheme $\scheme\tvars\cp\ttyp$ \emph{before} it is stored in the
environment, because it would be inefficient to copy an unsimplified
constraint. In other words, it appears that, in order to preserve efficiency,
constraint generation and constraint simplification cannot be separated.

Of course, in practice, it is not difficult to intermix these phases, so the
problem is not technical, but pedagogical. Indeed, we argued earlier that it
is natural and desirable to separate them. \emph{Type scheme introduction and
elimination constraints}, which we introduced in
\S\ref{section-constraints} but did not use in the specification of
\hmx, are intended as a means of solving this problem.  In the present
section, we exploit them to give a novel formulation of \hmx, which no longer
requires copying constraints back and forth between the environment and the
constraint assumption. In fact, the environment is suppressed altogether:
taking advantage of the new constraint forms, we encode information about
program identifiers within the constraint assumption.

% --------------------------------------------------------------------------

\subsection*{Presentation}

% TEMPORARY expliquer que/pourquoi ça s'appelle \pcb
% et trouver un meilleur nom

% TEMPORARY mieux expliquer pourquoi l'environnement ``disparaît'' -> il
% se retrouve intégré aux contraintes

We now employ the full constraint language
(\S\ref{section-constraints}). Typing judgments take the form
$\prj{\co}{\et}{\ttyp}$, where $\co$ may have free type variables \emph{and}
free program identifiers. The rules that allow deriving such judgments appear
in Figure~\ref{fig:pcb}. As before, we identify judgments up to constraint
equivalence.

\begin{TTCOMPONENT}{Typing rules for \pcb}{}
\ttlabel{pcb}

\infrule[\prjVar]
  {\co\entails\ccall\evid\ttyp}
  {\prj\co\evid\ttyp}

\infrule[\prjAbs]
  {\prj\co\et{\ttyp'}}
  {\prj{\clambda\evar\ttyp\co}{\efun\evar\et}{\ttyp\arw\ttyp'}}

\infrule[\prjApp]
  {\prj{\co_1}{\et_1}{\ttyp\arw\ttyp'} \andalso
   \prj{\co_2}{\et_2}\ttyp}
  {\prj{\co_1\wedge\co_2}{\eapp{\et_1}{\et_2}}{\ttyp'}}

\infrule[\prjLet]
  {\prj{\co_1}{\et_1}{\ttyp_1} \andalso
   \prj{\co_2}{\et_2}{\ttyp_2}}
  {\prj{\cxlet{\evar:\scheme\tyvarset{\co_1}{\ttyp_1}}{\co_2} \\}
       {\elet\evar{\et_1}{\et_2}}{\ttyp_2}}

\infrule[\prjSub]
  {\prj\co\et\ttyp}
  {\prj{\co\wedge\ttyp\subtype\ttyp'}\et{\ttyp'}}

\infrule[\prjIntro]
  {\prj\co\et\ttyp \andalso
   \disjoint\tvars{\ftv\ttyp}}
  {\prj{\exists\tvars.\co}\et\ttyp}

\extraspacehack{.07in}
\end{TTCOMPONENT}

Let us review the rules. \Rule{\prjVar} states that $\evid$ has type $\ttyp$
under any constraint that entails $\ccall\evid\ttyp$. Note that we no longer
consult the type scheme associated with $\evid$ in the environment---indeed,
there is no environment. Instead, we let the constraint assumption record the
fact that the type scheme should admit $\ttyp$ as one of its instances. Thus,
in a judgment $\prj\co\et\ttyp$, any program identifier that occurs free
within $\et$ typically also occurs free within $\co$. \Rule{\prjAbs} requires
the body $\et$ of a $\lambda$-abstraction to have type $\ttyp'$ under
assumption $\co$. Although no explicit assumption about $\evar$ appears in the
premise, $\co$ typically contains a number of instantiation constraints
bearing on $\evar$, of the form $\ccall\evar{\ttyp_i}$. In the rule's
conclusion, $\co$ is wrapped within the context $\clambda\evar\ttyp\chole$,
where $\ttyp$ is the type assigned to $\evar$. This effectively requires every
$\ttyp_i$ to denote a supertype of $\ttyp$, as desired. Note that
$\evar$ does not occur free in the constraint $\clambda\evar\ttyp\co$, which
is necessary for well-formedness of the definition, since it does not occur
free in $\efun\evar\et$. \Rule{\prjApp} exhibits a minor stylistic difference
with respect to \Rule{hmx-App}: its constraint assumption is split between its
premises. It is not difficult to prove that, when weakening holds (see
Lemma~\ref{lemma-pcb-weakening} below), this choice does not affect the set of
valid judgments. This new presentation encourages reading the rules in
Figure~\ref{fig:pcb} as the specification of an algorithm, which, given $\et$
and $\ttyp$, produces $\co$ such that $\prj\co\et\ttyp$ holds. In the case of
\Rule\prjApp, the algorithm invokes itself recursively for each of the two
subexpressions, yielding the constraints $\co_1$ and $\co_2$, then constructs
their conjunction. \Rule{\prjLet} is analogous to \Rule\prjAbs: by wrapping
$\co_2$ within a \kwd{let} prefix, it gives meaning to the instantiation
constraints bearing on $\evar$ within $\co_2$. The difference is that $\evar$
may now be assigned a type scheme, as opposed to a monotype. An appropriate
type scheme is built in the most straightforward manner from the constraint
$\co_1$ and the type $\ttyp_1$ that describe $\et_1$. All of the type
variables that appear free in the left-hand premise are generalized, hence the
notation $\scheme\tyvarset{\co_1}{\ttyp_1}$, which is a convenient shorthand
for $\scheme{\ftv{\co_1,\ttyp_1}}{\co_1}{\ttyp_1}$. The side-condition that
``type variables that occur free in the environment must not be generalized,''
which was present in \dm and \hmx, naturally disappears, since judgments no
longer involve an environment. \Rule{\prjSub} again exhibits a minor stylistic
difference with respect to \Rule{hmx-Sub}: the comments made about
\Rule{\prjApp} above apply here as well. \Rule{\prjIntro} is essentially
identical to \Rule{hmx-Exists}.

In the standard specification of \hmx, \Rule{hmd-Abs} and \Rule{hmd-LetGen}
accumulate information in the environment. Through the environment, this
information is made available to \Rule{hmd-VarInst}, which retrieves and
copies it. Here, instead, no information is explicitly transmitted. Where a
program identifier is bound, a type scheme introduction constraint is built;
where a program identifier is used, a type scheme instantiation constraint is
produced. The two are related only by our definition of the meaning of
constraints.

The reader may be puzzled by the fact that \Rule{\prjLet} allows \emph{all}
type variables that occur free in its left-hand premise to be generalized.
The following exercise sheds some light on this issue.
%
\begin{exercise}[\Recommended, \QuickCheck]
Build a type derivation for the expression
$\efun{\evar_1}{\elet{\evar_2}{\evar_1}{\evar_2}}$ within \pcb. Draw a
comparison with the solution of Exercise~\ref{ex:exdm}.
\solref{expcb}
\end{exercise}

The following lemma is an analog of Lemma~\ref{lemma-hmx-weakening}.
%
\begin{lemma}[Weakening]
\label{lemma-pcb-weakening}
If $\co'\entails\co$, then every derivation of $\prj\co\et\ttyp$ may be
turned into a derivation of $\prj{\co'}\et\ttyp$ with the same shape.
\end{lemma}
%
\begin{Proof}
The proof is by structural induction on a derivation of $\prj\co\et\ttyp$.
In each proof case, we adopt the notations of Figure~\ref{fig:pcb}.

\proofcase\Rule\prjVar. By transitivity of entailment.

\demoreset\proofcase\Rule\prjAbs. The rule's conclusion is
$\prj{\clambda\evar\ttyp\co}{\efun\evar\et}{\ttyp\arw\ttyp'}$~\dlabel{c}. By
hypothesis, we have $\co'\entails\clambda\evar\ttyp\co$~\dlabel{e}. We may
assume, \spdg, $\evar\not\in\fpv{\co'}$~\dlabel{f}. The rule's premise is
$\prj\co\et{\ttyp'}$~\dlabel{p}. Applying the induction hypothesis to \dref{p}
yields $\prj{\co\wedge\co'}\et{\ttyp'}$, which by \Rule{\prjAbs} implies
$\prj{\clambda\evar\ttyp{(\co\wedge\co')}}
{\efun\evar\et}{\ttyp\arw\ttyp'}$~\dlabel{d}. By \dref{f} and \Rule{C-InAnd*},
$\clambda\evar\ttyp{(\co\wedge\co')}$ is equivalent to
$(\clambda\evar\ttyp\co)\wedge\co'$, which by \dref{e} and \Rule{C-Dup} is
equivalent to $\co'$. Thus, \dref{d} is the goal
$\prj{\co'}{\efun\evar\et}{\ttyp\arw\ttyp'}$.

\proofcase\Rule\prjApp. By applying the induction hypothesis to each premise,
using the fact that $\co'\entails\co_1\wedge\co_2$ implies $\co'\entails\co_1$
and $\co'\entails\co_2$.

\proofcase\Rule\prjLet. Analogous to the case of \Rule\prjAbs. The induction
hypothesis is applied to the second premise only.

\proofcase\Rule\prjSub. Analogous to the case of \Rule\prjApp.

\proofcase\Rule\prjIntro. See the corresponding case in the proof of
Lemma~\ref{lemma-hmx-weakening}.
\end{Proof}

% --------------------------------------------------------------------------

\subsection*{Relating \pcb with \hmx}

Let us now provide evidence for our claim that \pcb is an alternate
presentation of \hmx. The next two theorems define an effective translation
from \hmx to \pcb and back.

The first theorem states that if, within \hmx, $\et$ has type $\ttyp$ under
assumptions $\co$ and $\env$, then, within \pcb, $\et$ also has type $\ttyp$,
under some assumption $\co'$. The relationship $\co\entails\cxlet\env{\co'}$
states that $\co$ entails the residual constraint obtained by confronting
$\env$, which provides information about the free program identifiers in
$\et$, with $\co'$, which contains instantiation constraints bearing on these
program identifiers. The statement requires $\co$ and $\env$ to have no free
program identifiers, which is natural, since they are part of an \hmx
judgment. The hypothesis $\co\entails\exists\env$ excludes the somewhat
pathological situation where $\env$ contains constraints not apparent in
$\co$. This hypothesis vanishes when $\env$ is the initial environment;
see Definition~\ref{def-initial-env}.
%
\begin{theorem}
\label{theorem-pcb-encodes-hmx}
Let $\co\entails\exists\env$. Assume $\fpv{\co,\env}=\varnothing$. If
$\hmj\co\env\et\ttyp$ holds in \hmx, then there exists a constraint $\co'$
such that $\prj{\co'}\et\ttyp$ holds in \pcb and $\co$ entails
$\cxlet\env{\co'}$.
\end{theorem}
%
\begin{Proof}
%
% Théorème prouvé le 06 juin 2002. Tapé le 31 janvier 2003.
%
The proof is by structural induction on a derivation of $\hmj\co\env\et\ttyp$.
In each proof case, we adopt the notations of Figure~\ref{fig:HM(X)-2}.

\demoreset\proofcase\Rule{hmd-VarInst}. The rule's conclusion is
$\hmj{\co\wedge\cp}\env\evid\ttyp$. By hypothesis, we have
$\co\wedge\cp\entails\exists\env$~\dlabel{hyp1} and
$\fpv{\co,\cp,\env}=\varnothing$~\dlabel{hyp2}. The rule's premise is
$\env(\evid)=\scheme\tvars\cp\ttyp$~\dlabel{gamma}. By \Rule{\prjVar}, we have
$\prj{\ccall\evid\ttyp}\evid\ttyp$, so there remains to establish
$\co\wedge\cp\entails\cxlet\env{\ccall\evid\ttyp}$~\dlabel{goal}. By
\dref{gamma}, \dref{hyp2}, and \Rule{C-InId}, the constraint
$\cxlet\env{\ccall\evid\ttyp}$ is equivalent to
$\cxlet\env{\ccall{\scheme\tvars\cp\ttyp}\ttyp}$, which, by \dref{hyp2} and
\Rule{C-In*}, is itself equivalent to
$\exists\env\wedge\ccall{\scheme\tvars\cp\ttyp}\ttyp$~\dlabel{d}. By
\dref{hyp1} and Lemma~\ref{lemma-identity-instance}, $\co\wedge\cp$ entails
\dref{d}. We have established \dref{goal}.

\demoreset\proofcase\Rule{hmd-Abs}. The rule's conclusion is
$\hmj\co\env{\efun\evar\et}{\ttyp\arw\ttyp'}$. Its premise is
$\hmj\co{(\env;\evar:\ttyp)}\et{\ttyp'}$~\dlabel{p}. The constraints
$\exists\env$ and $\exists(\env;\evar:\ttyp)$ are equivalent, so the induction
hypothesis applies to \dref{p} and yields a constraint $\co'$ such that
$\prj{\co'}\et{\ttyp'}$~\dlabel{j} and
$\co\entails\cxlet{\env;\evar:\ttyp}{\co'}$~\dlabel{e}.
Applying \Rule{\prjAbs} to \dref{j} yields
$\prj{\clambda\evar\ttyp{\co'}}{\efun\evar\et}{\ttyp\arw\ttyp'}$.
There remains to check that $\co$ entails
$\cxlet\env{\clambda\evar\ttyp{\co'}}$---but that is precisely \dref{e}.

\demoreset\proofcase\Rule{hmd-App}. The rule's conclusion is
$\hmj\co\env{\eapp{\et_1}{\et_2}}{\ttyp'}$. Its premises are
$\hmj\co\env{\et_1}{\ttyp\arw\ttyp'}$~\dlabel{l} and
$\hmj\co\env{\et_2}\ttyp$~\dlabel{r}. Applying the induction
hypothesis to \dref{l} and \dref{r}, we obtain constraints $\co'_1$ and
$\co'_2$ such that
$\prj{\co'_1}{\et_1}{\ttyp\arw\ttyp'}$~\dlabel{lj} and
$\prj{\co'_2}{\et_2}\ttyp$~\dlabel{rj} and
$\co\entails\cxlet\env{\co'_1}$~\dlabel{le} and
$\co\entails\cxlet\env{\co'_2}$~\dlabel{re}.
By \Rule\prjApp, \dref{lj} and \dref{rj} imply
$\prj{\co'_1\wedge\co'_2}{\eapp{\et_1}{\et_2}}{\ttyp'}$. Furthermore,
by \Rule{C-InAnd}, \dref{le} and \dref{re} yield
$\co\entails\cxlet\env{\co'_1\wedge\co'_2}$.

\demoreset\proofcase\Rule{hmd-LetGen}. The rule's conclusion is
$\hmj{\co\wedge\exists\tvars.\cp}\env {\elet\evar{\et_1}{\et_2}}{\ttyp_2}$.
By hypothesis, we have $\co\wedge\exists\tvars.\cp \entails
\exists\env$~\dlabel{hyp1} and $\fpv{\co,\cp,\env}=\varnothing$~\dlabel{hyp2}.
The rule's premises are $\hmj{\co\wedge\cp}\env{\et_1}{\ttyp_1}$~\dlabel{l}
and $\disjoint\tvars{\ftv{\co,\env}}$~\dlabel{free} and
$\hmj{\co\wedge\exists\tvars.\cp}{\env'}{\et_2}{\ttyp_2}$~\dlabel{r}, where
$\env'$ is $\env;\evar:\scheme\tvars\cp{\ttyp_1}$.  Applying the induction
hypothesis to \dref{l} yields a constraint $\co'_1$ such that
$\prj{\co'_1}{\et_1}{\ttyp_1}$~\dlabel{lj} and
$\co\wedge\cp\entails\cxlet\env{\co'_1}$~\dlabel{le}.  By \dref{hyp1},
\dref{hyp2}, and \Rule{C-In*}, we have $\co\wedge\exists\tvars.\cp \entails
\exists\env'$. Thus, the induction hypothesis applies to \dref{r} and yields a
constraint $\co'_2$ such that $\prj{\co'_2}{\et_2}{\ttyp_2}$~\dlabel{rj} and
$\co\wedge\exists\tvars.\cp\entails \cxlet{\env'}{\co'_2}$~\dlabel{re}.  By
\Rule\prjLet, \dref{lj} and \dref{rj} imply
$\prj{\cxlet{\evar:\scheme\tyvarset{\co'_1}{\ttyp_1}}{\co'_2}}
{\elet\evar{\et_1}{\et_2}}{\ttyp_2}$~\dlabel{j}.  By
Lemmas~\ref{lemma-younger-entail} and~\ref{lemma-pcb-weakening}, \dref{j}
yields $\prj{\cxlet{\evar:\scheme\tvars{\co'_1}{\ttyp_1}}{\co'_2}}
{\elet\evar{\et_1}{\et_2}}{\ttyp_2}$~\dlabel{k}, where the universal
quantification is over $\tvars$ only. There remains to establish that
$\co\wedge\exists\tvars.\cp$ entails
$\cxlet{\env;\evar:\scheme\tvars{\co'_1}{\ttyp_1}}{\co'_2}$~\dlabel{cg}. By
\dref{free}, \dref{hyp2}, and \Rule{C-LetDup}, the constraint \dref{cg} is
equivalent to
$\cxlet{\env;\evar:\scheme\tvars{\cxlet\env{\co'_1}}{\ttyp_1}}{\co'_2}$. By
\dref{le}, this constraint is entailed by
$\cxlet{\env;\evar:\scheme\tvars{\co\wedge\cp}{\ttyp_1}}{\co'_2}$, which by
\dref{free} and \Rule{C-LetAnd}, is equivalent to
$\co\wedge\cxlet{\env;\evar:\scheme\tvars\cp{\ttyp_1}}{\co'_2}$, that is,
$\co\wedge\cxlet{\env'}{\co'_2}$. By \dref{re}, this constraint is entailed by
$\co\wedge\exists\tvars.\cp$.

\demoreset\proofcase\Rule{hmd-Sub}. The rule's conclusion is
$\hmj\co\env\et{\ttyp'}$. Its premises are $\hmj\co\env\et\ttyp$~\dlabel{p}
and $\co\entails\ttyp\subtype\ttyp'$~\dlabel{s}. Applying the induction
hypothesis to \dref{p} yields a constraint $\co'$ such that
$\prj{\co'}\et\ttyp$~\dlabel{pj} and
$\co\entails\cxlet\env{\co'}$~\dlabel{pe}.
By \Rule\prjSub, \dref{pj} implies
$\prj{\co'\wedge\ttyp\subtype\ttyp'}\et{\ttyp'}$. There
remains to establish $\co\entails\cxlet\env{(\co'\wedge\ttyp\subtype\ttyp')}$,
which follows from \dref{pe} and \dref{s} by \Rule{C-InAnd*}.

\demoreset\proofcase\Rule{hmd-Exists}. The rule's conclusion is
$\hmj{\exists\tvars.\co}\env\et\ttyp$. Its premises are
$\hmj\co\env\et\ttyp$~\dlabel{p} and
$\disjoint\tvars{\ftv{\env,\ttyp}}$~\dlabel{f}. By hypothesis, we have
$\exists\tvars.\co\entails\exists\env$, which by
Lemma~\ref{lemma-exists-entail} implies
$\co\entails\exists\env$. Thus, the induction hypothesis applies to \dref{p}
and yields a constraint $\co'$ such that
$\prj{\co'}\et\ttyp$~\dlabel{pj}
and $\co\entails\cxlet\env{\co'}$~\dlabel{pe}.
By \Rule\prjIntro, \dref{pj} and \dref{f} imply
$\prj{\exists\tvars.\co'}\et\ttyp$.
There remains to establish
$\exists\tvars.\co\entails\cxlet\env{\exists\tvars.\co'}$.
By congruence of entailment, \dref{pe} implies
$\exists\tvars.\co\entails\exists\tvars.\cxlet\env{\co'}$.
The result follows by \dref{f} and \Rule{C-InEx}.
\end{Proof}

The second theorem states that if, within \pcb, $\et$ has type $\ttyp$ under
assumption $\co$, then, within \hmx, $\et$ also has type $\ttyp$, under
assumptions $\cxlet\env\co$ and $\env$. The idea is simple: the constraint
$\co$ represents a combined assumption about the initial judgment's free type
variables and free program identifiers. In \hmx, these two kinds of
assumptions must be maintained separately. So, we split them into a pair of an
environment $\env$, which may be chosen arbitrarily, provided it satisfies
$\fpv\co\subseteq\dpv\env$---that is, provided it defines all program
variables of interest, and the residual constraint $\cxlet\env\co$, which has
no free program identifiers, thus represents an assumption about the new
judgment's type variables only. Distinct choices of $\env$ give rise to
distinct \hmx judgments, which may be incomparable; this is related to the
fact that \MLtype does not have principal typings
\cite{Jim96}\index{principal typings}. Again, the
hypothesis $\fpv\env=\fpv{\cxlet\env\co}=\varnothing$ is natural, since we
wish $\env$ and $\cxlet\env\co$ to appear in an \hmx judgment.
%
% Note.
% Dans la mesure où les jugements sont identifiés modulo équivalence
% de contraintes, exiger fpi(C)=0 n'a de sens que si fpi est préservée
% par équivalence. Or cela est vrai pour les contraintes non équivalentes
% à false seulement (lemme~\ref{lemma-entail-fpv}). Dans l'énoncé ci-dessous,
% ça semble coller, puisqu'on fait justement cette hypothèse. Mais il y a
% peut-être là un problème auquel on n'a pas fait suffisamment attention
% ailleurs.
%
\begin{theorem}
\label{theorem-hmx-encodes-pcb}
Assume $\fpv\env=\fpv{\cxlet\env\co}=\varnothing$ and $\co\not\logeq\cfalse$.
If $\prj\co\et\ttyp$ holds
in \pcb, then $\hmj{\cxlet\env\co}\env\et\ttyp$ holds in \hmx.
\end{theorem}
%
\begin{Proof}
%
% Théorème prouvé le 06 juin 2002. Tapé le 31 janvier 2003.
%
The proof is by structural induction on a derivation of $\prj\co\et\ttyp$.
In each proof case, we adopt the notations of Figure~\ref{fig:pcb}.

By Lemma~\ref{lemma-false}, the hypothesis $\co\not\logeq\cfalse$ is preserved
whenever the induction hypothesis is invoked. It is explicitly used only in
case \Rule\prjVar, where it guarantees that the identifier at hand is bound in
$\env$.

\demoreset\proofcase\Rule\prjVar. The rule's conclusion is
$\prj\co\evid\ttyp$. Its premise is $\co\entails\ccall\evid\ttyp$~\dlabel{e}.
By Lemma~\ref{lemma-entail-fpv}, \dref{e} and the hypothesis
$\co\not\logeq\cfalse$ imply $\evid\in\fpv\co$.  Because $\cxlet\env\co$ has
no free program identifiers, this implies $\evid\in\dpv\env$, that is, the
environment $\env$ must define $\evid$. Let
$\env(\evid)=\scheme\tvars\cp{\ttyp'}$~\dlabel{gamma}, where
$\disjoint\tvars{\ftv{\env,\ttyp}}$~\dlabel{fresh}. By \dref{gamma},
\Rule{hmd-VarInst}, and \Rule{hmd-Sub}, we have
$\hmj{\cp\wedge\ttyp'\subtype\ttyp}\env\evid\ttyp$. By \dref{fresh} and
\Rule{hmd-Exists}, this implies
$\hmj{\exists\tvars.(\cp\wedge\ttyp'\subtype\ttyp)}\env\evid\ttyp$~\dlabel{g}.
Now, by \dref{fresh}, the constraint
$\exists\tvars.(\cp\wedge\ttyp'\subtype\ttyp)$ may be written
$\ccall{\scheme\tvars\cp{\ttyp'}}\ttyp$~\dlabel{foo}.  The hypothesis
$\fpv\env=\varnothing$ implies $\fpv\cp=\varnothing$~\dlabel{d}.  By \dref{d},
\Rule{C-InId} and \Rule{C-In*}, 
$\cxlet\env{\ccall\evid\ttyp}$ entails \dref{foo}. By \dref{e} and by congruence
of entailment, $\cxlet\env\co$ entails \dref{foo} as well. Thus, by Lemma~\ref{lemma-hmx-weakening},
the judgment \dref{g} implies
$\hmj{\cxlet\env\co}\env\evid\ttyp$.

\demoreset\proofcase\Rule\prjAbs. The rule's conclusion is
$\prj{\clambda\evar\ttyp\co}{\efun\evar\et}{\ttyp\arw\ttyp'}$. Its premise is
$\prj\co\et{\ttyp'}$~\dlabel{p}. Let $\env'$ stand for $\env;\evar:\ttyp$.
Applying the induction hypothesis to \dref{p} yields
$\hmj{\cxlet{\env'}\co}{\env'}\et{\ttyp'}$.
By \Rule{hmd-Abs}, this implies
$\hmj{\cxlet{\env'}\co}\env{\efun\evar\et}{\ttyp\arw\ttyp'}$.

\proofcase\Rule\prjApp. The rule's conclusion is
$\prj{\co_1\wedge\co_2}{\eapp{\et_1}{\et_2}}{\ttyp'}$. Its premises are
$\prj{\co_1}{\et_1}{\ttyp\arw\ttyp'}$ and
$\prj{\co_2}{\et_2}\ttyp$. Applying the induction hypothesis
yields respectively
$\hmj{\cxlet\env{\co_1}}\env{\et_1}{\ttyp\arw\ttyp'}$ and
$\hmj{\cxlet\env{\co_2}}\env{\et_2}\ttyp$, which by
Lemma~\ref{lemma-hmx-weakening}
and \Rule{hmd-App} imply
$\hmj{\cxlet\env{(\co_1\wedge\co_2)}}\env{\eapp{\et_1}{\et_2}}{\ttyp'}$.

\demoreset\proofcase\Rule\prjLet. The rule's conclusion is
$\prj{\cxlet{\evar:\scheme\tyvarset{\co_1}{\ttyp_1}}{\co_2}}
       {\elet\evar{\et_1}{\et_2}}{\ttyp_2}$. Its premises are
$\prj{\co_1}{\et_1}{\ttyp_1}$~\dlabel{l} and
$\prj{\co_2}{\et_2}{\ttyp_2}$~\dlabel{r}.
Let $\tvars$ stand for $\ftv{\co_1,\ttyp_1}$. We may require, \spdg,
$\disjoint\tvars{\ftv{\env,\co_2}}$~\dlabel{fresh}.
By hypothesis, we have $\fpv\env=\varnothing$~\dlabel{fenv}. We also have
$\fpv{\cxlet{\env;\evar:\scheme\tyvarset{\co_1}{\ttyp_1}}{\co_2}}=\varnothing$,
which implies $\fpv{\cxlet\env{\co_1}}=\varnothing$.
Thus, the induction hypothesis applies to \dref{l}
and yields $\hmj{\cxlet\env{\co_1}}\env{\et_1}{\ttyp_1}$~\dlabel{lj}.
Now, let $\ts$ stand for $\scheme\tvars{\cxlet\env{\co_1}}{\ttyp_1}$ and
$\env'$
stand for $\env;\evar:\ts$. We have $\fpv{\env'}=
\fpv{\cxlet{\env'}{\co_2}}=\varnothing$.
Thus, the induction hypothesis applies to \dref{r} and yields
$\hmj{\cxlet{\env'}{\co_2}}{\env'}{\et_2}{\ttyp_2}$~\dlabel{rj}.
Let us now weaken \dref{lj} and \dref{rj} so as to make them suitable
premises for \Rule{hmd-LetGen}. Applying Lemma~\ref{lemma-hmx-weakening}
to \dref{lj} yields $\hmj{(\cxlet{\env'}{\co_2})\wedge(\cxlet\env{\co_1})}
\env{\et_1}{\ttyp_1}$~\dlabel{lk}. Applying Lemma~\ref{lemma-hmx-weakening}
to \dref{rj} yields 
$\hmj{(\cxlet{\env'}{\co_2})\wedge\exists\tvars.(\cxlet\env{\co_1})}
{\env'}{\et_2}{\ttyp_2}$~\dlabel{rk}. Last, \dref{fresh} implies
$\disjoint\tvars{\ftv{\env,\cxlet{\env'}{\co_2}}}$~\dlabel{freshh}.
Applying \Rule{hmd-LetGen} to \dref{lk}, \dref{freshh} and \dref{rk},
we obtain $\hmj{(\cxlet{\env'}{\co_2})\wedge\exists\tvars.(\cxlet\env{\co_1})}
\env{\elet\evar{\et_1}{\et_2}}{\ttyp_2}$~\dlabel{g}.
Now, by \dref{fenv}, \dref{fresh}, and \Rule{C-LetDup},
$\cxlet{\env'}{\co_2}$ is equivalent to
$\cxlet{\env;\evar:\scheme\tvars{\co_1}{\ttyp_1}}{\co_2}$.
Using this fact, as well as \dref{fresh}, \Rule{C-InEx},
and \Rule{C-InAnd}, we find that the constraint
$(\cxlet{\env'}{\co_2})\wedge\exists\tvars.(\cxlet\env{\co_1})$ is
equivalent to
$\cxlet\env{(\cxlet{\evar:\scheme\tvars{\co_1}{\ttyp_1}}{\co_2}
\wedge\exists\tvars.\co_1)}$, which by definition of the \kwd{let} form,
is itself equivalent to
$\cxlet{\env;\evar:\scheme\tvars{\co_1}{\ttyp_1}}{\co_2}$. Last,
by definition of $\tvars$, this constraint is
$\cxlet{\env;\evar:\scheme\tyvarset{\co_1}{\ttyp_1}}{\co_2}$. Thus,
\dref{g} is the goal.

\demoreset\proofcase\Rule\prjSub. The rule's conclusion is
$\prj{\co\wedge\ttyp\subtype\ttyp'}\et{\ttyp'}$. Its premise is
$\prj\co\et\ttyp$~\dlabel{p}. Applying the induction hypothesis to \dref{p}
yields $\hmj{\cxlet\env\co}\env\et\ttyp$~\dlabel{pj}. By
Lemma~\ref{lemma-hmx-weakening} and \Rule{hmd-Sub}, \dref{pj} implies
$\hmj{(\cxlet\env\co)\wedge\ttyp\subtype\ttyp'}\env\et{\ttyp'}$, which by
\Rule{C-InAnd*} may be
written $\hmj{\cxlet\env{(\co\wedge\ttyp\subtype\ttyp')}}\env\et{\ttyp'}$.

\demoreset\proofcase\Rule\prjIntro. The rule's conclusion is
$\prj{\exists\tvars.\co}\et\ttyp$. Its premises are
$\prj\co\et\ttyp$~\dlabel{p} and $\disjoint\tvars{\ftv\ttyp}$~\dlabel{f}.  We
may further require, \spdg, $\disjoint\tvars{\ftv\env}$~\dlabel{g}. Applying
the induction hypothesis to \dref{p} yields
$\hmj{\cxlet\env\co}\env\et\ttyp$~\dlabel{pj}.  Applying \Rule{hmd-Exists} to
\dref{f}, \dref{g}, and \dref{pj}, we find
$\hmj{\exists\tvars.\cxlet\env\co}\env\et\ttyp$, which, by \dref{g} and
\Rule{C-InEx}, may be written
$\hmj{\cxlet\env{\exists\tvars.\co}}\env\et\ttyp$.
\end{Proof}

As a corollary, we find that, for closed programs, the type systems \hmx and
\pcb coincide. In particular, a program is well-typed with respect to one if
and only if it is well-typed with respect to the other. This supports the view
that \pcb is an alternate formulation of \hmx.
%
\begin{theorem}
Assume $\fpv\co=\varnothing$ and $\co\not\logeq\cfalse$. Then,
$\hmj\co\noenv\et\ttyp$ holds in \hmx if and only if $\prj\co\et\ttyp$ holds
in \pcb.
\end{theorem}
%
\begin{Proof}
First, let $\hmj\co\noenv\et\ttyp$ hold in \hmx. The assertion
$\co\entails\exists\noenv$ is a tautology, so, by
Theorem~\ref{theorem-pcb-encodes-hmx}, there exists a constraint $\co'$ such
that $\prj{\co'}\et\ttyp$ and $\co\entails\cxlet\noenv{\co'}$, that is,
$\co\entails\co'$.  By Lemma~\ref{lemma-pcb-weakening}, this implies
$\prj\co\et\ttyp$. The converse implication is a special case of
Theorem~\ref{theorem-hmx-encodes-pcb} where $\env$ is $\varnothing$.
\end{Proof}
%
% TEMPORARY souhaite-t-on généraliser cet énoncé au cas de l'environnement
% initial \ienv au lieu de l'environnement vide?

% --------------------------------------------------------------------------

\section{Constraint Generation}
\label{section-constraint-generation}

We now explain how to reduce type inference problems for \pcb to constraint
solving problems. A type inference problem consists of an expression $\et$ and
a type $\ttyp$ of kind $\normalkind$. The problem is to determine whether
$\et$ is well-typed with type $\ttyp$, that is, whether there exists a
satisfiable constraint $\co$ such that $\prj\co\et\ttyp$ holds. A constraint
solving problem consists of a constraint $\co$. The problem is to determine
whether $\co$ is satisfiable. To reduce a type inference problem $(\et,\ttyp)$
to a constraint solving problem, we must produce a constraint $\co$ that is
both \emph{sufficient} and \emph{necessary} for $\prj\co\et\ttyp$ to
hold. Below, we explain how to compute such a constraint, which we write
$\calcule\et\ttyp$. We check that it is indeed {sufficient} by proving
\sound\et\ttyp. That is, the constraint $\calcule\et\ttyp$ is specific enough
to guarantee that $\et$ has type $\ttyp$. We say that constraint generation is
\emph{sound}. We check that it is indeed {necessary} by proving that, for
every constraint $\co$, the validity of $\prj\co\et\ttyp$ implies
$\co\entails\calcule\et\ttyp$. That is, every constraint that guarantees that
$\et$ has type $\ttyp$ is at least as specific as $\calcule\et\ttyp$. We say
that constraint generation is \emph{complete}. Together, these properties mean
that $\calcule\et\ttyp$ is the {least specific} constraint that
guarantees that $\et$ has type $\ttyp$.

We now see how to reduce a type inference problem to a constraint solving
problem. Indeed, if there exists a satisfiable constraint $\co$ such that
$\prj\co\et\ttyp$ holds, then, by the completeness property,
$\co\entails\calcule\et\ttyp$ holds, so $\calcule\et\ttyp$ is satisfiable.
Conversely, by the soundness property, if $\calcule\et\ttyp$ is satisfiable,
then we have a satisfiable constraint $\co$ such that $\prj\co\et\ttyp$
holds. In other words, $\et$ is well-typed with type $\ttyp$ if and only if
$\calcule\et\ttyp$ is satisfiable.

The reader may be somewhat puzzled by the fact that our formulation of the
type inference problem requires an appropriate type $\ttyp$ to be known in
advance, whereas the very purpose of type inference seems to consist in
{discovering} the type of $\et$! In other words, we have made $\ttyp$ an
{input} of the constraint generation algorithm, instead of an
{output}. Fortunately, this causes no loss of generality, because it is
possible to let $\ttyp$ be a type variable $\tvar$. Then, the constraint
produced by the algorithm will contain information about $\tvar$. This is the
point of the following exercise.
%
\begin{exercise}[\Recommended, \QuickCheck]
Let $\tvar$ be an arbitrary type variable. Show that, if there exist a
satisfiable constraint $\co$ and a type $\ttyp$ such that $\prj\co\et\ttyp$
holds, then there exists a satisfiable constraint $\co'$ such that
$\prj{\co'}\et\tvar$ holds. Conclude that a closed term $\et$ is well-typed % TEMPORARY vis-à-vis de l'env. vide?
if and only if $\exists\tvar.\calcule\et\tvar \logeq \ctrue$ holds.
\solref{infervar}
\end{exercise}
%
This shows that providing $\ttyp$ as an input to the constraint generation
procedure is not essential. We adopt this style because it is convenient. A
somewhat naive alternative would be to provide $\et$ only, and to have the
procedure return both a constraint $\co$ and a type
$\ttyp$ \cite{sulzmann-mueller-zenger-99}. It turns out that this does not
quite work, because $\co$ and $\ttyp$ may mention ``fresh'' variables, which
we must be able to quantify over if we are to avoid an informal treatment of
freshness. Thus, the true alternative is to provide $\et$ only and to have
the procedure return a \emph{type scheme} $\ts$ \cite{POPL::BourdoncleM1997,bonniot-02}.

The existence of a sound and complete constraint generation procedure is the
analog of the existence of \emph{principal type schemes}\index{principal type schemes} in classic
presentations of \MLtype \cite{DAMAS82}. Indeed, a principal type
scheme is least specific in the sense that all valid types are substitution
instances of it.  Here, the constraint $\calcule\et\ttyp$ is least specific in
the sense that all valid constraints entail it. Earlier, we have established a
connection between constraint entailment and refinement of type substitutions,
in the specific case of equality constraints interpreted over a free algebra
of finite types; see Lemma~\ref{lemma-entailment-is-refinement}.

\begin{figure}
\begin{bnf}
    \calcule\evid\ttyp
\eq \ccall\evid\ttyp
\\
\calcule{\efun\evar\et}\ttyp
\eq \exists\tvar_1\tvar_2.(
      \clambda\evar{\tvar_1}{\calcule\et{\tvar_2}}
      \wedge
      \tvar_1\arw\tvar_2\subtype\ttyp 
) \\
\calcule{\eapp{\et_1}{\et_2}}\ttyp
\eq \exists\tvar_2.(
       \calcule{\et_1}{\tvar_2\arw\ttyp} 
       \wedge
       \calcule{\et_2}{\tvar_2}
) \\
\calcule{\elet\evar{\et_1}{\et_2}}\ttyp
\eq \cxlet{\evar:\scheme\tvar{\calcule{\et_1}\tvar}\tvar}{\calcule{\et_2}\ttyp}
\end{bnf}%
\bcpcaption{genrules}{Constraint generation}
\end{figure}

The constraint $\calcule\et\ttyp$ is defined in Figure~\ref{fig:genrules} by
induction on the structure of the expression $\et$. We refer to these defining
equations as the \emph{constraint generation rules}. The definition is quite
terse. It is perhaps even simpler than the declarative specification of \pcb
given in Figure~\ref{fig:pcb}; yet, we prove below that the two are
equivalent.

Before explaining the definition, we state the requirements that bear on the
type variables $\tvar_1$, $\tvar_2$, and $\tvar$, which appear bound in the
right-hand sides of the second, third, and fourth equations. These type
variables must have kind~$\normalkind$. They must be chosen distinct (that is,
$\tvar_1\not=\tvar_2$ in the second equation) and fresh for the objects that
appear on the left-hand side---that is, \emph{the type variables that appear
bound in an equation's right-hand side must not occur free in the term and
type that appear in the equation's left-hand side.} Provided this restriction
is obeyed, different choices of $\tvar_1$, $\tvar_2$, and $\tvar$ lead to
$\alpha$-equivalent constraints---that is, to the same constraint, since we
identify objects up to \aconv---which guarantees that the above equations make
sense. We note that, since expressions do not have free type variables, the
freshness requirement may be simplified to: type variables that appear bound
in an equation's right-hand side must not appear free in $\ttyp$. However,
this simplification is rendered invalid by the introduction of open type
annotations within expressions (page~\pageref{page-type-annotations}).
Note that we are able to state a \emph{precise} (as opposed to informal)
freshness requirement. This is made possible by the fact that
$\calcule\et\ttyp$ has no free type variables other than those of $\ttyp$,
which in turn depends on our explicit use of quantification.

Let us now review the four equations. The first one simply mirrors
\Rule\prjVar. The second one requires $\et$ to have type $\tvar_2$ under the
hypothesis that $\evar$ has type $\tvar_1$, and forms the arrow type
$\tvar_1\arw\tvar_2$; this corresponds to \Rule\prjAbs. Here, $\tvar_1$ and
$\tvar_2$ must be fresh type variables, because we cannot in general guess the
expected types of $\evar$ and $\et$. The expected type $\ttyp$ is required to
be a supertype of $\tvar_1\arw\tvar_2$; this corresponds to \Rule\prjSub. We
must bind the fresh type variables $\tvar_1$ and $\tvar_2$, so as to guarantee
that the generated constraint is unique up to \aconv. Furthermore, we must
bind them \emph{existentially}, because we intend the constraint solver to
choose some appropriate value for them. This is justified by
\Rule\prjIntro. The third equation uses the fresh type variable $\tvar_2$ to
stand for the unknown type of $\et_2$. The subexpression $\et_1$ is expected
to have type $\tvar_2\arw\ttyp$. This corresponds to \Rule\prjApp. The fourth
equation, which corresponds to \Rule\prjLet, is most interesting. It summons a
fresh type variable $\tvar$ and produces $\calcule{\et_1}\tvar$. This
constraint, whose sole free type variable is $\tvar$, is the \emph{least
specific} constraint that must be imposed on $\tvar$ so as to make it a valid
type for $\et_1$. As a result, the type scheme
$\scheme\tvar{\calcule{\et_1}\tvar}\tvar$, abbreviated $\ts$ in the following,
is a \emph{principal} type scheme for $\et_1$. There remains to place
$\calcule{\et_2}\ttyp$ inside the context $\cxlet{\evar:\ts}\chole$. Indeed,
when placed inside this context, an instantiation constraint of the form
$\ccall\evar{\ttyp'}$ acquires the meaning $\ccall\ts{\ttyp'}$, which by
definition of $\ts$ and by Lemma~\ref{lemma-calcule-exists} (see below) is
equivalent to $\calcule{\et_1}{\ttyp'}$. Thus, the constraint produced by the
fourth equation simulates a textual expansion of the \kwlet construct, where
every occurrence of $\evar$ would be replaced with $\et_1$. Thanks to type
scheme introduction and instantiation constraints, however, this effect is
achieved without duplication of source code or constraints. In other words,
constraint generation has linear time and space complexity.
%
\begin{exercise}[\QuickCheck, \nosolution]
Define the \emph{size} of an expression, of a type, and of a constraint,
viewed as abstract syntax trees. Check that the size of $\calcule\et\ttyp$ is
linear in the sum of the sizes of $\et$ and $\ttyp$.
\end{exercise}

We now establish several properties of constraint generation. We begin with
soundness, whose statement was explained above, and whose proof is
straightforward.
%
\begin{theorem}[Soundness]
\label{theorem/type-inference/soundness}
\sound\et\ttyp.
\end{theorem}
%
\begin{Proof}
By induction on the structure of $\et$.

\proofcase $\evid$. The goal $\prj{\ccall\evid\ttyp}\evid\ttyp$ follows from
\Rule\prjVar.

\proofcase $\efun\evar\et$. By the induction hypothesis, we have
\sound\et{\tvar_2}. By \Rule\prjAbs, this implies 
$\prj{\clambda\evar{\tvar_1}{\calcule\et{\tvar_2}}}{\efun\evar\et}
{\tvar_1\arw\tvar_2}$. By \Rule\prjSub, this implies
$\prj{\clambda\evar{\tvar_1}{\calcule\et{\tvar_2}}
      \wedge\tvar_1\arw\tvar_2\subtype\ttyp}
{\efun\evar\et}
\ttyp$. Lastly, because $\disjoint{\tvar_1\tvar_2}{\ftv\ttyp}$ holds,
\Rule{\prjIntro} applies and yields \sound{\efun\evar\et}\ttyp.

\proofcase $\eapp{\et_1}{\et_2}$. By the induction hypothesis, we have
\sound{\et_1}{\tvar_2\arw\ttyp} and
\sound{\et_2}{\tvar_2}. By \Rule\prjApp, this implies
$\prj{\calcule{\et_1}{\tvar_2\arw\ttyp}\wedge\calcule{\et_2}{\tvar_2}}
{\eapp{\et_1}{\et_2}}\ttyp$. Because $\tvar_2\not\in\ftv\ttyp$ holds,
\Rule{\prjIntro} applies and yields
\sound{\eapp{\et_1}{\et_2}}\ttyp.

\proofcase $\elet\evar{\et_1}{\et_2}$. By the induction hypothesis, we have
\sound{\et_1}{\tvar} and \sound{\et_2}\ttyp. By \Rule\prjLet, these imply
$\prj{\cxlet{\evar:\scheme\tyvarset{\calcule{\et_1}\tvar}\tvar}
{\calcule{\et_2}\ttyp}}{\elet\evar{\et_1}{\et_2}}\ttyp$. Because
$\ftv{\calcule{\et_1}\tvar}$ is $\tvar$, the universal quantification on
$\tyvarset$ really bears on $\tvar$ alone. We have proved
\sound{\elet\evar{\et_1}{\et_2}}\ttyp.
\end{Proof}

The following lemmas are used in the proof of the completeness property and in
a number of other occasions. The first two state that $\calcule\et\ttyp$ is
\emph{covariant} with respect to $\ttyp$. Roughly speaking, this means that
enough subtyping constraints are generated to achieve completeness with
respect to \Rule\prjSub.
%
\begin{lemma}
\label{lemma-calcule-covariant}
$\calcule\et\ttyp \wedge \ttyp\subtype\ttyp' \entails
\calcule\et{\ttyp'}$.
\end{lemma}
%
\begin{Proof}
By induction on the structure of $\et$.

\proofcase $\evid$. By Lemma~\ref{lemma-instance-covariant}, for every type
scheme $\ts$, $\ccall\ts\ttyp\wedge\ttyp\subtype\ttyp'$ entails
$\ccall\ts{\ttyp'}$. The goal
$\ccall\evid\ttyp\wedge\ttyp\subtype\ttyp'\entails\ccall\evid{\ttyp'}$ follows
from Lemma~\ref{lemma-instance-covariant}
by definition of entailment and by \Rule{CM-Instance}.

\demoreset\proofcase $\efun\evar\et$. Let
$\disjoint{\tvar_1\tvar_2}{\ftv{\ttyp,\ttyp'}}$~\dlabel{f}. Then, we have
$$\begin{array}{rlr}
& \exists\tvar_1\tvar_2.(
    \clambda\evar{\tvar_1}{\calcule\et{\tvar_2}} \wedge
    \tvar_1\arw\tvar_2\subtype\ttyp 
  ) \wedge \ttyp\subtype\ttyp' 
\\
\logeq & \exists\tvar_1\tvar_2.(
    \clambda\evar{\tvar_1}{\calcule\et{\tvar_2}} \wedge
    \tvar_1\arw\tvar_2\subtype\ttyp \wedge \ttyp\subtype\ttyp'
  ) 
& \dlabel{un} \\
\entails & \exists\tvar_1\tvar_2.(
    \clambda\evar{\tvar_1}{\calcule\et{\tvar_2}} \wedge
    \tvar_1\arw\tvar_2\subtype\ttyp'
  )
& \dlabel{deux}
\end{array}$$
where \dref{un} is by \dref{f} and \Rule{C-ExAnd}; \dref{deux} is
by transitivity of subtyping.

\demoreset\proofcase $\eapp{\et_1}{\et_2}$. Let
$\tvar_2\not\in\ftv{\ttyp'}$~\dlabel{f}. Then, we have
$$\begin{array}{rlr}
& \exists\tvar_2.(
    \calcule{\et_1}{\tvar_2\arw\ttyp} 
    \wedge \calcule{\et_2}{\tvar_2}
  ) \wedge \ttyp\subtype\ttyp' \\
\logeq & \exists\tvar_2.(
    \calcule{\et_1}{\tvar_2\arw\ttyp} \wedge \ttyp\subtype\ttyp'
    \wedge \calcule{\et_2}{\tvar_2}
  ) 
& \dlabel{un} \\
\logeq & \exists\tvar_2.(
    \calcule{\et_1}{\tvar_2\arw\ttyp}
    \wedge \tvar_2\arw\ttyp\subtype\tvar_2\arw\ttyp'
    \wedge \calcule{\et_2}{\tvar_2}
  ) 
& \dlabel{deux} \\
\entails & \exists\tvar_2.(
    \calcule{\et_1}{\tvar_2\arw\ttyp'}
    \wedge \calcule{\et_2}{\tvar_2}
  )
& \dlabel{trois}
\end{array}$$
where \dref{un} is by \dref{f} and \Rule{C-ExAnd}; \dref{deux} is by
\Rule{C-Arrow}; \dref{trois} is by the induction hypothesis.

\demoreset\proofcase $\elet\evar{\et_1}{\et_2}$. We have
$$\begin{array}{rlr}
& (\cxlet{\evar:\scheme\tvar{\calcule{\et_1}\tvar}\tvar}
  {\calcule{\et_2}\ttyp})
  \wedge \ttyp\subtype\ttyp' \\
\logeq & \cxlet{\evar:\scheme\tvar{\calcule{\et_1}\tvar}\tvar}
  ({\calcule{\et_2}\ttyp}
  \wedge \ttyp\subtype\ttyp') 
& \dlabel{un} \\
\entails & \cxlet{\evar:\scheme\tvar{\calcule{\et_1}\tvar}\tvar}
  {\calcule{\et_2}\ttyp'}
& \dlabel{deux}
\end{array}$$
where \dref{un} is by \Rule{C-InAnd*}; \dref{deux} is by the induction
hypothesis.
\end{Proof}
%
\begin{lemma}
\label{lemma-calcule-exists}
$\tvar\not\in\ftv\ttyp$ implies
$\exists\tvar.(\calcule\et\tvar\wedge\tvar\subtype\ttyp) \logeq
\calcule\et\ttyp$.
\end{lemma}
%
\begin{Proof}
\demoreset $\tvar\not\in\ftv\ttyp$ yields
$\tvar\not\in\ftv{\calcule\et\ttyp}$~\dlabel{g}. By
Lemma~\ref{lemma-calcule-covariant} and by congruence of entailment,
$\exists\tvar.(\calcule\et\tvar\wedge\tvar\subtype\ttyp)$ entails
$\exists\tvar.\calcule\et\ttyp$, which by \dref{g} and \Rule{C-Ex*} is
equivalent to $\calcule\et\ttyp$.
%
Conversely, by \dref{g} and \Rule{C-NameEq}, $\calcule\et\ttyp$ is equivalent
to $\exists\tvar.(\calcule\et\tvar\wedge\tvar=\ttyp)$, which entails
$\exists\tvar.(\calcule\et\tvar\wedge\tvar\subtype\ttyp)$.
\end{Proof}

The next lemma gives a simplified version of the second constraint generation
rule, in the specific case where the expected type is an arrow type. Then,
fresh type variables need not be generated; one may directly use the arrow's
domain and codomain instead.
%
\begin{lemma}
\label{lemma-calcule-lambda}
$\calcule{\efun\evar\et}{\ttyp_1\arw\ttyp_2}$ is equivalent to
$\clambda\evar{\ttyp_1}{\calcule\et{\ttyp_2}}$.
\end{lemma}
%
\begin{Proof}
\demoreset Let $\tvar_1\not=\tvar_2$ and
$\disjoint{\tvar_1\tvar_2}{\ftv{\ttyp_1,\ttyp_2}}$. We have
$$\begin{array}{rlr}
& \calcule{\efun\evar\et}{\ttyp_1\arw\ttyp_2} \\
\logeq & \exists\tvar_1\tvar_2.(
\clambda\evar{\tvar_1}{\calcule\et{\tvar_2}} \wedge
\tvar_1\arw\tvar_2 \subtype \ttyp_1\arw\ttyp_2
) & \dlabel{un} \\
\logeq & \exists\tvar_1.(
\ttyp_1\subtype\tvar_1 \wedge
\clambda\evar{\tvar_1}{\exists\tvar_2.(\calcule\et{\tvar_2} \wedge
\tvar_2\subtype\ttyp_2)}
) & \dlabel{deux} \\
\logeq & \exists\tvar_1.(
\ttyp_1\subtype\tvar_1 \wedge
\clambda\evar{\tvar_1}{\calcule\et{\ttyp_2}}
) & \dlabel{trois} \\
\logeq & 
\clambda\evar{\ttyp_1}{\calcule\et{\ttyp_2}}
& \dlabel{quatre} \\
\end{array}$$
where \dref{un} is by definition of constraint generation; \dref{deux} follows
from \Rule{C-Arrow}, \Rule{C-InAnd*}, \Rule{C-ExAnd} and
\Rule{C-InEx}; \dref{trois} is by Lemma~\ref{lemma-calcule-exists};
\dref{quatre} is by \Rule{C-LetSub}.
\end{Proof}

We conclude with the completeness property.
%
\begin{theorem}[Completeness]
\label {theorem/type-inference/completeness}
if $\prj\co\et\ttyp$, then $\co\entails\calcule\et\ttyp$.
\end{theorem}
%
\begin{Proof}
By induction on the derivation of $\prj\co\et\ttyp$.

\proofcase\Rule\prjVar. The rule's conclusion is $\prj\co\evid\ttyp$. Its
premise is $\co\entails\ccall\evid\ttyp$, which is also the goal.

\proofcase\Rule\prjAbs. The rule's conclusion is
$\prj{\clambda\evar\ttyp\co}{\efun\evar\et}{\ttyp\arw\ttyp'}$.  Its premise is
$\prj\co\et{\ttyp'}$. By the induction hypothesis, we have
$\co\entails\calcule\et{\ttyp'}$. By congruence of entailment, this implies
$\clambda\evar\ttyp\co\entails \clambda\evar\ttyp{\calcule\et{\ttyp'}}$,
which, by Lemma~\ref{lemma-calcule-lambda}, may be written
$\clambda\evar\ttyp\co\entails \calcule{\efun\evar\et}{\ttyp\arw\ttyp'}$.

\proofcase\Rule\prjApp. The rule's conclusion is
$\prj{\co_1\wedge\co_2}{\eapp{\et_1}{\et_2}}{\ttyp'}$. Its premises are
$\prj{\co_1}{\et_1}{\ttyp\arw\ttyp'}$ and $\prj{\co_2}{\et_2}\ttyp$. By the
induction hypothesis, we have $\co_1\entails\calcule{\et_1}{\ttyp\arw\ttyp'}$
and $\co_2\entails\calcule{\et_2}\ttyp$. Thus, $\co_1\wedge\co_2$ entails
$\calcule{\et_1}{\ttyp\arw\ttyp'}\wedge\calcule{\et_2}\ttyp$, which, by
\Rule{C-NameEq}, may be written
$\exists\tvar_2.(\tvar_2=\ttyp\wedge\calcule{\et_1}{\tvar_2\arw\ttyp'}\wedge
\calcule{\et_2}{\tvar_2})$, where $\tvar_2\not\in\ftv{\ttyp,\ttyp'}$.
Forgetting about the equation $\tvar_2=\ttyp$, we find that $\co_1\wedge\co_2$
entails $\exists\tvar_2.(\calcule{\et_1}{\tvar_2\arw\ttyp'}
\wedge\calcule{\et_2}{\tvar_2})$, which is precisely
$\calcule{\eapp{\et_1}{\et_2}}{\ttyp'}$.

\demoreset\proofcase\Rule\prjLet. The rule's conclusion is
$\prj{\cxlet{\evar:\scheme\tyvarset{\co_1}{\ttyp_1}}{\co_2}}
{\elet\evar{\et_1}{\et_2}}{\ttyp_2}$. Its premises are
$\prj{\co_1}{\et_1}{\ttyp_1}$ and $\prj{\co_2}{\et_2}{\ttyp_2}$. By the
induction hypothesis, we have $\co_1\entails\calcule{\et_1}{\ttyp_1}$ and
$\co_2\entails\calcule{\et_2}{\ttyp_2}$, which implies
$\cxlet{\evar:\scheme\tyvarset{\co_1}{\ttyp_1}}{\co_2} \entails
\cxlet{\evar:\scheme\tyvarset{\calcule{\et_1}{\ttyp_1}}{\ttyp_1}}
{\calcule{\et_2}{\ttyp_2}}$~\dlabel{un}.

Now, let us establish $\csubsume\ctrue
{\scheme\tvar{\calcule{\et_1}\tvar}\tvar}
{\scheme\tyvarset{\calcule{\et_1}{\ttyp_1}}{\ttyp_1}}$~\dlabel{deux}. By
definition, this requires proving
$\exists\tvars_1.(\calcule{\et_1}{\ttyp_1}\wedge\ttyp_1\subtype\tzar)
\entails
\exists\tvar.(\calcule{\et_1}\tvar\wedge\tvar\subtype\tzar)$~\dlabel{trois},
where $\tvars_1=\ftv{\ttyp_1}$ and $\tzar\not\in\tvar\tvars_1$~\dlabel{f}.
By Lemma~\ref{lemma-calcule-covariant}, \dref{f}, and \Rule{C-Ex*}, the
left-hand side of \dref{trois} entails $\calcule{\et_1}\tzar$. By
\dref{f} and Lemma~\ref{lemma-calcule-exists}, the right-hand side of
\dref{trois} is $\calcule{\et_1}\tzar$. Thus, \dref{trois} holds, and
so does \dref{deux}.

By \dref{deux} and Lemma~\ref{lemma-subsume-env}, we have
$\cxlet{\evar:\scheme\tyvarset{\calcule{\et_1}{\ttyp_1}}{\ttyp_1}}
{\calcule{\et_2}{\ttyp_2}} \entails
\cxlet{\evar:\scheme\tvar{\calcule{\et_1}\tvar}\tvar}
{\calcule{\et_2}{\ttyp_2}}
$~\dlabel{quatre}. By transitivity of entailment, \dref{un} and \dref{quatre}
yield
$\cxlet{\evar:\scheme\tyvarset{\co_1}{\ttyp_1}}{\co_2} \entails
\calcule{\elet\evar{\et_1}{\et_2}}{\ttyp_2}$.

\proofcase\Rule\prjSub. The rule's conclusion is
$\prj{\co\wedge\ttyp\subtype\ttyp'}\et{\ttyp'}$. Its premise is
$\prj\co\et\ttyp$. By the induction hypothesis, we have
$\co\entails\calcule\et\ttyp$, which implies
$\co\wedge\ttyp\subtype\ttyp'\entails
\calcule\et\ttyp\wedge\ttyp\subtype\ttyp'$.  By
lemma~\ref{lemma-calcule-covariant} and by transitivity of entailment, we
obtain $\co\wedge\ttyp\subtype\ttyp'\entails\calcule\et{\ttyp'}$.

\demoreset\proofcase\Rule\prjIntro. The rule's conclusion is
$\prj{\exists\tvars.\co}\et\ttyp$. Its premises are $\prj\co\et\ttyp$ and
$\disjoint\tvars{\ftv\ttyp}$~\dlabel{f}. By the induction hypothesis, we have
$\co\entails\calcule\et\ttyp$. By congruence of entailment, this implies
$\exists\tvars.\co\entails\exists\tvars.\calcule\et\ttyp$~\dlabel{e}.
Furthermore, \dref{f} implies
$\disjoint\tvars{\ftv{\calcule\et\ttyp}}$~\dlabel{g}. By \dref{g} and
\Rule{C-Ex*}, \dref{e} may be written
$\exists\tvars.\co\entails\calcule\et\ttyp$.
\end{Proof}

% It may be worthwhile to point out that the proofs of the soundness and
% completeness properties together provide a procedure for normalizing type
% derivations in \pcb.

\end{full}

% --------------------------------------------------------------------------

\section{Type Soundness}
\label{sec.soundness}

We are now ready to establish type soundness for our type system. The
statement that we wish to prove is sometimes known as \emph{Milner's slogan}:
``Well-typed programs do not go wrong'' \cite{Milner78}.  Below, we define
well-typedness in terms of our constraint generation rules, for the sake of
convenience, and establish type soundness with respect to that particular
definition. Theorems~\ref{theorem-hmx-encodes-dm}\iffull{,
\ref{theorem-hmx-encodes-pcb},}
and~\ref{theorem/type-inference/completeness} imply that type soundness also
holds when well-typedness is defined with respect to the typing judgments
of \dm
{\shortfull{ or \hmx}{, \hmx, or \pcb}}. We establish type soundness by
following Wright and 
Felleisen's so-called \emph{syntactic
approach} \citeyr{IC::WrightF1994}. The approach consists of isolating two
independent properties. \emph{Subject reduction}, whose exact statement will
be given below, implies that well-typedness is preserved by
reduction. \emph{Progress} states that no stuck configuration is well-typed.
It is immediate to check that, if both properties hold, then no well-typed
program can reduce to a stuck configuration.
%
Subject reduction itself depends on a key lemma, usually known as a (term)
\emph{substitution lemma}.
\begin{short}
Here is a version of this lemma, stated in terms of the constraint generation
rules.
\end{short}%
\begin{full}
Here are two versions of this lemma: the
former is stated in terms of \pcb judgments, while the latter is stated in
terms of the constraint generation rules.
%
\begin{lemma}[Substitution]
\label{lemma-substitution}
$\prj\co\et\ttyp$ and $\prj{\co_0}{\et_0}{\ttyp_0}$ imply
$\prj{\cxlet{\evar_0:\scheme{\tvars_0}{\co_0}{\ttyp_0}}\co}
     {\subst{\evar_0}{\et_0}\et}\ttyp$.
\end{lemma}
%
\begin{Proof}
\demoreset The proof is by structural induction on the derivation of
$\prj\co\et\ttyp$.  In each proof case, we adopt the notations of
Figure~\ref{fig:pcb}. We write $\ts_0$ for
$\scheme{\tvars_0}{\co_0}{\ttyp_0}$. We refer to the hypothesis
$\prj{\co_0}{\et_0}{\ttyp_0}$ as \dlabel{hyp0}. We assume, \spdg,
$\disjoint{\tvars_0}{\ftv{\co,\ttyp}}$~\dlabel{hyp1} and
$\evar_0\not\in\fpv{\ts_0}$~\dlabel{hyp2}.
% TEMPORARY Il faudrait un \demoreset hiérarchique pour renuméroter à
% partir de 4 dans chaque cas de preuve

% Tu peux le faire: 
% Step 1: \dglabel {aaa} rend la reference {aaa} global. 
% Step 2: \demoreset \setcounter{demoref}{3} 
% Step 3: \dlabel{b} alloue a` partir de 4 while \label{aaa} fait reference
% au numero de {aaa}. 


\newcommand{\cx}{\cxlet{\evar_0:\ts_0}}
\newcommand{\CXP}{\cxlet{\ts_0'}}
\newcommand{\Subst}{\subst{\evar_0}{\et_0}}

\proofcase\Rule\prjVar. The rule's conclusion is
$\prj\co\evid\ttyp$~\dlabel{c}. Its premise is
$\co\entails\ccall\evid\ttyp$~\dlabel{prem}. Two subcases arise.

\proofsubcase $\evid$ is $\evar_0$. Applying
\Rule\prjSub{} to \dref{hyp0} yields
$\prj{\co_0\wedge\ttyp_0\subtype\ttyp}{\et_0}\ttyp$. y \dref{hyp1} and
\Rule\prjIntro, this implies
$\prj{\exists\tvars_0.(\co_0\wedge\ttyp_0\subtype\ttyp)}
{\et_0}\ttyp$~\dlabel{g}. Furthermore, by \dref{hyp1} again, the constraint
$\exists\tvars_0.(\co_0\wedge\ttyp_0\subtype\ttyp)$ is
$\ccall{\ts_0}\ttyp$, which is equivalent to
$\cx{\ccall{\evar_0}\ttyp}$. As a result, \dref{g} may be written
$\prj{\cx{\ccall\evid\ttyp}}{\Subst\evid}\ttyp$~\dlabel{gh}. 

\proofsubcase $\evid$ isn't $\evar_0$. Then, $\Subst\evid$ is $\evid$. Thus,
\Rule{\prjVar} yields
$\prj{\exists\ts_0\wedge\ccall\evid\ttyp}{\Subst\evid}\ttyp$. By \Rule{C-In*},
this may be read $\prj{\cx{\ccall\evid\ttyp}}{\Subst\evid}\ttyp$, that is,
again \dref{gh}.

In either subcase, by \dref{prem}, by congruence of entailment, and by
Lemma~\ref{lemma-pcb-weakening}, \dref{gh} implies 
$\prj{\cx\co}{\Subst\et}\ttyp$.

\proofcase\Rule\prjAbs. The rule's conclusion is
$\prj{\clambda\evar\ttyp\co}{\efun\evar\et}{\ttyp\arw\ttyp'}$. Its premise is
$\prj\co\et{\ttyp'}$~\dlabel{p}. We may assume, \spdg, that $\evar$ is
distinct from ${\evar_0}$ and does not occur free within $\et_0$ or
$\ts_0$~\dlabel{fresh}.  Applying the induction hypothesis to \dref{p} yields
$\prj{\cx\co}{\Subst\et}{\ttyp'}$, which, by \Rule\prjAbs, implies
$\prj{\clambda\evar\ttyp{(\cxlet{\evar_0:\ts_0}\co)}}{\efun\evar{\Subst\et}}
{\ttyp\arw\ttyp'}$. By \dref{fresh} and \Rule{C-LetLet}, this may be written
$\prj{\cx{(\cxlet{\evar:\ttyp}\co)}}{\Subst{(\efun\evar\et)}}
{\ttyp\arw\ttyp'}$.

\proofcase\Rule\prjApp. By the induction hypothesis, by \Rule\prjApp, and by
\Rule{C-InAnd}.

\proofcase\Rule\prjLet. The rule's conclusion is
$\prj{\cxlet{\evar:\scheme{\tvars_1}{\co_1}{\ttyp_1}}{\co_2}}
{\elet\evar{\et_1}{\et_2}}{\ttyp_2}$, where $\tvars_1=\ftv{\co_1,\ttyp_1}$.
Its premises are $\prj{\co_1}{\et_1}{\ttyp_1}$~\dlabel{left} and
$\prj{\co_2}{\et_2}{\ttyp_2}$~\dlabel{right}.  We may assume, \spdg, that
$\evar$ is distinct from $\evar_0$ and does not occur free within $\et_0$ or
$\ts_0$~\dlabel{frais}. We may also assume, \spdg,
$\disjoint{\tvars_1}{\ftv{\ts_0}}$~\dlabel{gen}.  Applying the induction
hypothesis to \dref{left} and \dref{right} respectively yields
$\prj{\cx{\co_1}}{\Subst{\et_1}}{\ttyp_1}$~\dlabel{leftp} and
$\prj{\cx{\co_2}}{\Subst{\et_2}}{\ttyp_2}$~\dlabel{rightp}.
Applying \Rule{\prjLet} to \dref{leftp} and \dref{rightp} produces
$\prj{\cxlet{\evar:\scheme\tyvarset{\cx{\co_1}}{\ttyp_1}}{\cx{\co_2}}}
     {\Subst{(\elet\evar{\et_1}{\et_2})}}{\ttyp_2}$~\dlabel{goo}.
Now, we have
$$\begin{array}{rlr}
&
\cxlet{\evar_0:\ts_0;\evar:\scheme{\tvars_1}{\co_1}{\ttyp_1}}{\co_2} \\
\logeq & 
\cxlet{\evar_0:\ts_0;\evar:\scheme{\tvars_1}{\cx{\co_1}}{\ttyp_1}}{\co_2}
& \dlabel{un} \\
\logeq &
\cxlet{\evar:\scheme{\tvars_1}{\cx{\co_1}}{\ttyp_1};\evar_0:\ts_0}{\co_2}
& \dlabel{deux} \\
\entails &
\cxlet{\evar:\scheme{\tyvarset\,\,}{\cx{\co_1}}{\ttyp_1};\evar_0:\ts_0}{\co_2}
& \dlabel{trois}
\end{array}$$
where \dref{un} follows from \dref{gen}, \dref{hyp2}, and \Rule{C-LetDup}; 
\dref{deux} follows from \dref{frais} and \Rule{C-LetLet}; and \dref{trois}
is by Lemma~\ref{lemma-younger-entail}. Thus, applying
Lemma~\ref{lemma-pcb-weakening} to \dref{goo} yields
$\prj{\cxlet{\evar_0:\ts_0;\evar:\scheme{\tvars_1}{\co_1}{\ttyp_1}}{\co_2}}
{\Subst{(\elet\evar{\et_1}{\et_2})}}{\ttyp_2}$.

\proofcase\Rule\prjSub. By the induction hypothesis, by
\Rule\prjSub, and by \Rule{C-InAnd*}.

\proofcase\Rule\prjIntro. The rule's conclusion is
$\prj{\exists\tvars.\co}\et\ttyp$. Its premises are
$\prj\co\et\ttyp$~\dlabel{l} and $\disjoint\tvars{\ftv\ttyp}$~\dlabel{r}.
We may assume, \spdg, $\disjoint\tvars{\ftv{\ts_0}}$~\dlabel{frit}.
Applying the induction hypothesis to \dref{l} yields
$\prj{\cx\co}{\Subst\et}\ttyp$, which, by \dref{r} and \Rule\prjIntro,
implies $\prj{\exists\tvars.\cx\co}{\Subst\et}\ttyp$~\dlabel{gaa}.
By \dref{frit} and \Rule{C-InEx}, \dref{gaa} is
$\prj{\cx{\exists\tvars.\co}}{\Subst\et}\ttyp$.
\end{Proof}
\end{full}
%
\begin{lemma}
\label{lemma-substitution-inference}
$\cxlet{\evar:\scheme\tvars{\calcule{\et_2}{\ttyp_2}}{\ttyp_2}}
{\calcule{\et_1}{\ttyp_1}}$ entails
$\calcule{\subst\evar{\et_2}{\et_1}}{\ttyp_1}$.
\end{lemma}
%
\begin{Proof}
This is an immediate consequence of
Theorem~\ref{theorem/type-inference/soundness},
Lemma~\ref{lemma-substitution}, and
Theorem~\ref{theorem/type-inference/completeness}.
\end{Proof}

Before going on, let us give a few definitions and formulate several
requirements. First, we must define an {initial environment} $\ienv$,
which assigns a type scheme to every constant. A couple of requirements must
be established to ensure that $\ienv$ is consistent with the semantics of constants,
as specified by $\reduces[\delta]$. Second, we must extend constraint
generation and well-typedness to {configurations}, as opposed to
programs, since reduction operates on configurations. Last, we must formulate
a {restriction} to tame the interaction between side effects and
\letpoly, which is unsound if unrestricted.
%
\begin{definition}
\label{def-initial-env}
Let $\ienv$ be an environment whose domain is the set of constants $\econset$.
We require $\ftv\ienv=\varnothing$, $\fpv\ienv=\varnothing$, and
$\exists\ienv\logeq\ctrue$. We refer to $\ienv$
as the \emph{initial} typing environment.
\indexsymfull{\ienv}{typing environment (initial)}
\end{definition}
%
\begin{full}
Since we require $\ienv$ to have no free type or program variables, the
constraint $\exists\ienv$ must be equivalent to either $\ctrue$ or
$\cfalse$. If it is $\cfalse$, then there exists a constant $\econ$ that no
well-typed program may use. We consider this a degenerate case, and avoid it
by requiring $\exists\ienv\logeq\ctrue$.
\end{full}
%
\begin{definition}
\label{def-typing-stores}
\label{def-tref}
Let $\tcref$ be an isolated, invariant type constructor of {\tcsignature}
$\normalkind \kindarrow \normalkind$.
A \emdef{store type} $\eS$ is a finite mapping from memory locations to types.
We write $\tref\eS$ for the environment that maps every $\eloc\in\Dom\eS$ to
$\tref{\eS(\eloc)}$.
Assuming $\Dom\es$ and $\Dom\eS$ coincide, the constraint $\calcule\es\eS$ is
defined as the conjunction of the constraints
$\calcule{\es(\eloc)}{\eS(\eloc)}$, where $\eloc$ ranges over $\Dom{\es}$.
Under the same assumption, the constraint $\calcule{\et/\es}{\ttyp/\eS}$ is
defined as $\calcule\et\ttyp \wedge \calcule\es\eS$.  A closed configuration
$\cconf\et\es$ is \emph{well-typed} if and only if there exist a type $\ttyp$ and a
store type $\eS$ such that $\Dom\es=\Dom\eS$ and the constraint $\cxlet{\ienv;
  \tref\eS}{\calcule{\et/\es}{\ttyp/\eS}}$ is satisfiable.
\end{definition}

The type $\tref\ttyp$ is the type of references (that is, memory locations)
that store data of type $\ttyp$~(\TAPLCHAPTER{13}). It must be \emph{invariant} in its
parameter, reflecting the fact that references may be both {read} and
{written}.

A store is a complex object: it may contain values that indirectly refer to
each other via memory locations. In fact, it is a representation of the graph
formed by objects and pointers in memory, which may contain cycles. We rely on
store types to deal with such cycles. In the definition of well-typedness, the
store type $\eS$ imposes a constraint on the contents of the store---the value
$\es(\eloc)$ must have type $\eS(\eloc)$---but also plays the role of a
hypothesis: by placing the constraint $\calcule{\et/\es}{\ttyp/\eS}$ within
the context $\cxlet{\tref\eS}\chole$, we give meaning to free occurrences of
memory locations within $\calcule{\et/\es}{\ttyp/\eS}$, and stipulate that it
is valid to assume that $\eloc$ has type $\eS(\eloc)$. In other words, we
essentially view the store as a large, mutually recursive binding of locations
to values.
%
The context $\cxlet\ienv\chole$ gives meaning to occurrences of constants
within $\calcule{\et/\es}{\ttyp/\eS}$.

\begin{full}
\begin{remark}
A reference of type $\tref\ttyp$ may be viewed as an object with \emph{set}
and \emph{get} methods, that is, as an object of type
$(\ttyp\arw\tunit)\times(\tunit\arw\ttyp)$, where $\tunit$ is the type of the
unit constant $\eunit$. Notice that, in this encoding, $\ttyp$ appears both as
the domain and as the codomain of an arrow type: this informally explains why
$\tcref$ must be invariant.

Note that it is also possible to replace $\tcref$ with a \emph{binary}
type constructor \kwd{biref}. The type $\tbiref{\ttyp_i}{\ttyp_o}$ is the type
of references to which one may write data of type $\ttyp_i$ and out of which
one may read data of type $\ttyp_o$. A reference of type
$\tbiref{\ttyp_i}{\ttyp_o}$ may be viewed as an object of type
$(\ttyp_i\arw\tunit)\times(\tunit\arw\ttyp_o)$.  Notice that, in this
encoding, $\ttyp_i$ appears only as the domain of an arrow type, while
$\ttyp_o$ appears only as the codomain of an arrow type. This informally
justifies why it is sound for \kwd{biref} to be contravariant with respect to
its first parameter and covariant in its second parameter, just like the arrow
type constructor. Using \kwd{biref} instead of $\tcref$ makes the type system
strictly more expressive. Furthermore, in type systems equipped with
subtyping, the absence of type constructors with invariant parameters may
simplify the design of constraint solvers and simplifiers. However, this
approach has a pragmatic drawback: it leads to larger and more complex
types. The idea, which seems due to \longcite{Reynolds88}, has been
studied in other settings, such as process and object
calculi \cite{PierceSangiorgi95,bugliesi-pericas-02}.
\end{remark}
\end{full}

We now define a relation between configurations that plays a key role in the
statement of the subject reduction property. The point of subject reduction is
to guarantee that well-typedness is preserved by reduction. However, such a
simple statement is too weak to be amenable to inductive proof. Thus, for the
purposes of the proof, we must be more specific. To begin, let us consider the
simpler case of a pure semantics, that is, a semantics without stores. Then,
we must state that if an expression $\et$ has type $\ttyp$ under a certain
constraint, then its reduct $\et'$ has type $\ttyp$ under the same constraint.
In terms of generated constraints, this statement becomes:
$\cxlet\ienv{\calcule\et\ttyp}$ entails $\cxlet\ienv{\calcule{\et'}\ttyp}$.
Let us now return to the general case, where a store is present.  The
statement of well-typedness for a configuration $\et/\es$ now involves a store
type $\eS$ whose domain is that of $\es$. So, the statement of well-typedness
for its reduct $\et'/\es'$ must involve a store type $\eS'$ whose domain is
that of $\es'$, which is larger if allocation occurred. The types of existing
memory locations must not change: we must request that $\eS$ and $\eS'$ agree
on $\Dom\eS$, that is, $\eS'$ must extend $\eS$.  Furthermore, the types
assigned to new memory locations in $\Dom{\eS'}\setminus\Dom\eS$ might involve
new type variables, that is, variables that do not appear free in $\eS$ or
$\ttyp$. We must allow these variables to be hidden---that is, existentially
quantified---otherwise the entailment assertion cannot hold. These
considerations lead us to the following definition:
%
\begin{definition}
\label{def-sr-inclusion}
$\et/\es \sr \et'/\es'$ holds if and only if, for every type
$\ttyp$ and for every store type $\eS$ such that $\Dom\es=\Dom\eS$, there
exist a set of type variables $\twars$ and a store type $\eS'$ such that
$\disjoint\twars{\ftv{\ttyp,\eS}}$ and $\ftv{\eS'}\subseteq\twars\cup\ftv\eS$
and $\Dom{\eS'}=\Dom{\es'}$ and $\eS'$ extends $\eS$ and
$$\begin{tabular*}{\displaywidth}{.C.R<{{}},L.r.}&
& \cxlet{\ienv; \tref{\eS\phantom'}}
        {\calcule{\et\phantom'/\es\phantom'}{\ttyp/\eS\phantom'}} \\
& \entails \exists\twars.
& \cxlet{\ienv; \tref{\eS'}}{\calcule{\et'/\es'}{\ttyp/\eS'}}.
\end{tabular*}$$
The relation $\sr$ is intended to express a connection between a configuration
and its reduct. Thus, subject reduction may be stated as:
$(\topreduces)\subseteq(\sr)$, that is, $\sr$ is indeed a conservative
description of reduction.
\end{definition}
%
% On peut choisir \twars en dehors de tout ensemble de variables fixé, \spdg.
% Cela sert dans la preuve de subject reduction, mais je ne crois pas utile
% de le dire ici.

We have introduced an initial environment $\ienv$ and used it in the
definition of well-typedness, but we haven't yet ensured that the type schemes
assigned to constants are an adequate description of their semantics. We now
formulate two requirements that relate $\ienv$ with $\reduces[\delta]$. They
are specializations of the subject reduction and progress properties to
configurations that involve an application of a constant. They represent proof
obligations that must be discharged when concrete definitions of $\econset$,
$\reduces[\delta]$, and $\ienv$ are given.
%
\begin{definition}
\label{def-requirements}
We require (i) $(\reduces[\delta]) \subseteq (\sr)$; and (ii) if the
configuration $\eapp[3] \econ {\ev_1} \ldots {\ev_k}/\es$ (where $k\geq 0$) is
well-typed, then either it is reducible, or $\eapp[3] \econ {\ev_1} \ldots
{\ev_k}$ is a value.
\end{definition}

The last point that remains to be settled before proving type soundness
is the interaction between side effects and \letpoly. The following
example illustrates the problem:
$$
\elet\erar{\eref{\efun\evar\evar}}{
  \elet\wildpat{(\eassign\erar{\efun\evar{(\evar\mathrel{\hat+}\hat1)}})}
        {\eapp{\ederef\erar}\etrue}
}
$$
This expression reduces to $\etrue\mathbin{\hat+}\hat1$, so it must not be
well-typed. Yet, if natural type schemes are assigned to $\eref$, $\ederef$,
and $\eassign{}{}$ (see Example~\ref{ex:sr-references}), then it {is}
well-typed with respect to the rules given so far, because $\erar$ receives
the polymorphic type scheme $\dmscheme\tvar{\tref{(\tvar\arw\tvar)}}$, which
allows writing a function of type $\tint\arw\tint$ into $\erar$ and reading it
back with type $\tbool\arw\tbool$.  The problem is that \letpoly simulates a
textual duplication of the \kwlet-bound expression $\eref{\efun\evar\evar}$,
while the semantics first reduces it to a value $\eloc$, causing a new binding
$\store\eloc{\efun\evar\evar}$ to appear in the store, then duplicates the
address $\eloc$. The new store binding is not duplicated: both copies of
$\eloc$ refer to the same memory cell. For this reason, generalization is
unsound in this case, and must be restricted. Many authors have attempted to
come up with a sound type system that accepts {all} pure programs and
remains flexible enough in the presence of side
effects \cite{Tofte88:Thesis,leroy-phd-92}. These proposals are often complex, which
is why they have been abandoned in favor of an extremely simple
{syntactic} restriction, known as the \emph{value
restriction} \cite{Wright95:simple}.
%
\begin{definition}
\label{def-value-restriction}
A program satisfies the \emphindex{value restriction} if and only if all
subexpressions of the form $\elet\evar{\et_1}{\et_2}$ are in fact of the form
$\elet\evar{\ev_1}{\et_2}$. In the following, we assume that either all
constants have pure semantics, or all programs satisfy the value restriction.
\end{definition}

Put slightly differently, the value restriction states that only values may be
generalized. This eliminates the problem altogether, since duplicating values
does not affect a program's semantics.  Note that any program that does not
satisfy the value restriction can be turned into one that does and has the
same semantics: it suffices to change $\elet\evar{\et_1}{\et_2}$ into
$\eapp{(\efun\evar{\et_2})}{\et_1}$ when $\et_1$ is not a value. Of course,
such a transformation may cause the program to become ill-typed. In other
words, the value restriction causes some perfectly safe programs to be
rejected. In particular, in its above form, it prevents generalizing
applications of the form $\eapp[3]\econ{\ev_1}\ldots{\ev_k}$, where $\econ$ is
a destructor of arity $k$. This is excessive, because many destructors have
pure semantics; only a few, such as $\eref$, allocate new mutable
storage. Furthermore, we use pure destructors to encode numerous language
features (\S\ref{section-calc-lang}). Fortunately, it is easy to relax the
restriction to allow generalizing not only values, but also a more general
class of \emph{nonexpansive} expressions, whose syntax guarantees that such
expressions cannot allocate new mutable storage (that is, {expand} the
domain of the store). The term {nonexpansive} was coined by
\longcite{Tofte88:Thesis}. Nonexpansive expressions may include applications of
the form $\eapp[3]\econ{\et_1}\ldots{\et_k}$, where $\econ$ is a pure
destructor of arity $k$ and $\et_1,\ldots,\et_k$ are nonexpansive. Experience
shows that this slightly relaxed restriction is acceptable in practice.
%
Some limitations remain: for instance, \emph{constructor functions} (that is,
functions that do not allocate mutable storage and build a value) are regarded
as ordinary functions, so their applications are considered potentially
expansive, even though a naked constructor application would be a value and
thus considered nonexpansive. For instance, in the expression $\elet \efar
{\eapp \econ \ev} {\elet \evar {\eapp \efar {\ew}} \et}$, where $\econ$ is a
constructor of arity~$2$, the partial application $\eapp \econ \ev$, to which
the name $\efar$ is bound, is a constructor function (of arity~$1$). The
program variable $\evar$ cannot receive a polymorphic type scheme, because
$\eapp \efar \ew$ is not a value, even though it has the same semantic meaning
as $\eapp[2]\econ\ev\ew$, which is a value.
%
A recent improvement to the value restriction \cite{garrigue-03} provides a
partial remedy. 
%
Technically, the effect of the value restriction (as stated in
Definition~\ref{def-value-restriction}) is summarized by the following result.
%
\begin{lemma}
\label{lemma-value-restriction}
Under the value restriction, the production $\ec ::= \elet\evar\ec\et$ may be
suppressed from the grammar of evaluation contexts (Figure~\ref{fig:MLsyntax})
without altering the operational semantics.
\end{lemma}
%
\begin{Proof}
First, the value restriction is preserved by reduction. Indeed, none of the
reduction rules creates new \kwlet expressions. Furthermore, applying a
substitution of the form $\subst\evar\ev$ to a value must yield a value, so
existing \kwlet expressions---which, by assumption, satisfy the
restriction---still satisfy it when some of their free program variables are
substituted away.

Second, by Lemma~\ref{lemma-value-irreducible-top}, values are irreducible.
Thus, under the value restriction, the left-hand side of every \kwlet form is
irreducible. As a result, suppressing the production $\ec ::=
\elet\evar\ec\et$ does not alter the semantics.
\end{Proof}

We are finished with definitions and requirements. Let us now turn to the type
soundness results.
%
\begin{theorem}[Subject reduction]
\label{subject-reduction}
$(\topreduces) \subseteq (\sr)$.
\end{theorem}
%
\begin{Proof}
Because $\reduces$ and $\topreduces$ are the smallest relations that satisfy
the rules of Figure~\ref{fig:MLsemantics}, it suffices to prove that $\sr$
satisfies these rules as well.
%
We note that if, for every type $\ttyp$, $\calcule{\et}{\ttyp} \entails
\calcule{\et'}{\ttyp}$ holds, then $\et/\es \sr \et'/\es$ holds.
(Take $\twars=\varnothing$ and $\eS'=\eS$ and use the fact that entailment is
a congruence to check that the conditions of Definition~\ref{def-sr-inclusion}
are met.) We make use of this fact in cases \Rule{R-Beta} and \Rule{R-Let}
below.

\demoreset\proofcase\Rule{R-Beta}.
% On peut se passer du lemme~\ref{lemma-calcule-lambda} dans ce cas si on le
% souhaite, mais il est plus simple de l'utiliser, si on en dispose.
%
We have
$$
\begin{array}{rlr}
& \calcule {\eapp {(\efun \evar \et)} \ev} \ttyp \\
\logeq &
\exists\tvar.
(
  \calcule{\efun\evar\et}{\tvar\arw\ttyp} \wedge
  \calcule\ev\tvar
) & \qquad\dlabel{un} \\
\logeq &
\exists\tvar.
(
  \clambda\evar\tvar{\calcule\et\ttyp} \wedge
  \calcule\ev\tvar
) & \dlabel{deux} \\
\logeq&
\exists\tvar.
  \cxlet{\evar:\scheme\varnothing{\calcule\ev\tvar}\tvar}{\calcule\et\ttyp}
& \dlabel{quatre}
\\\entails&
\calcule{\subst\evar\ev\et}\ttyp
& \dlabel{cinq}
\end{array}
$$
where \dref{un} is by definition of constraint generation;
\dref{deux} is by Lemma~\ref{lemma-calcule-lambda}; \dref{quatre} is by
\Rule{C-LetAnd}; \dref{cinq} is by Lemma~\ref{lemma-substitution-inference} and
\Rule{C-Ex*}.

\demoreset\proofcase\Rule{R-Let}.
We have
$$
\begin{array}{rlr}
& \calcule{\elet\evar\ev\et}\ttyp \\
= &
\cxlet{\evar:\scheme\tvar{\calcule\ev\tvar}\tvar}
      {\calcule\et\ttyp}
& \qquad\dlabel{un}
\\\entails&
\calcule{\subst\evar\ev\et}\ttyp
& \dlabel{cinq}
\end{array}
$$
where \dref{un} is by definition of constraint generation and
\dref{cinq} is by Lemma~\ref{lemma-substitution-inference}.

\proofcase\Rule{R-Delta}. This case is exactly requirement (i) in
Definition~\ref{def-requirements}. 

\demoreset\proofcase\Rule{R-Extend}.
Our hypotheses are $\et/\es \sr \et'/\es'$~\dlabel{h} and
$\disjoint{\Dom{\es''}}{\Dom{\es'}}$~\dlabel{disj}
and $\disjoint{\Range{\es''}}{\Dom{\es'\setminus\es}}$~\dlabel{hop}.
Because $\Dom\es$ must be a subset of
$\Dom{\es'}$, it is also disjoint with $\Dom{\es''}$. Our goal is
$\et/\es\es'' \sr \et'/\es'\es''$~\dlabel{goal}. Thus, let
us introduce a type $\ttyp$ and a store type of domain $\Dom{\es\es''}$,
or (equivalently) two store types $\eS$ and $\eS''$ whose domains are
respectively $\Dom\es$ and $\Dom{\es''}$.
By \dref{h}, there exist type variables $\twars$ and a store type
$\eS'$ such that $\disjoint\twars{\ftv{\ttyp,\eS}}$~\dlabel{frais} and
$\ftv{\eS'}\subseteq\twars\cup\ftv\eS$ and 
$\Dom{\eS'}=\Dom{\es'}$ and $\eS'$ extends $\eS$~\dlabel{etend} and
$\cxlet{\ienv; \tref\eS}{\calcule{\et/\es}{\ttyp/\eS}}
\entails \exists\twars.
\cxlet{\ienv; \tref{\eS'}}{\calcule{\et'/\es'}{\ttyp/\eS'}}$.
We may further require, \spdg,
$\disjoint\twars{\ftv{\eS''}}$~\dlabel{fresh}.
Let us now add the conjunct
$\cxlet{\ienv;\tref\eS}{\calcule{\es''}{\eS''}}$ to each side
of this entailment assertion. On the left-hand side, by \Rule{C-InAnd}
and by Definition~\ref{def-typing-stores}, we obtain
$\cxlet{\ienv;\tref\eS}{\calcule{\et/\es\es''}{\ttyp/\eS\eS''}}$~\dlabel{left}.
On the right-hand side, by \dref{frais}, \dref{fresh}, \Rule{C-ExAnd}, and
\Rule{C-InAnd}, we obtain $\exists\twars.
\cxlet\ienv{(\cxlet{\tref{\eS'}}{\calcule{\et'/\es'}{\ttyp/\eS'}} \wedge
\cxlet{\tref\eS}{\calcule{\es''}{\eS''}})}$~\dlabel{right}. Now, recall
that $\eS'$ extends $\eS$~\dref{etend} and, furthermore, \dref{hop} implies
$\disjoint{\fpv{\calcule{\es''}{\eS''}}}{\dpv{\eS'\setminus\eS}}$~\dlabel{hip}.
By \dref{hip}, \Rule{C-InAnd*}, and \Rule{C-InAnd}, \dref{right} is
equivalent to $\exists\twars.\cxlet{\ienv;\tref{\eS'}}
{(\calcule{\et'/\es'}{\ttyp/\eS'}\wedge \calcule{\es''}{\eS''})}$, that is,
$\exists\twars.\cxlet{\ienv;\tref{\eS'}}
{\calcule{\et'/\es'\es''}{\ttyp/\eS'\eS''}}$~\dlabel{rightp}.
Thus, we have established that \dref{left} entails \dref{rightp}.
Let us now place this entailment assertion within the constraint context
$\cxlet{\tref{\eS''}}\ehole$. On the left-hand side, because
$\fpv{\ienv,\eS,\eS''}=\varnothing$ and
$\dpv{\eS''}\cap\dpv{\ienv,\eS}\subseteq\Dom{\es''}\cap(\econset\cup\Dom\es)
=\varnothing$, \Rule{C-LetLet} applies, yielding
$\cxlet{\ienv;\tref{\eS\eS''}}{\calcule{\et/\es\es''}{\ttyp/\eS\eS''}}$~\dlabel{leftq}.
On the right-hand side, by \dref{fresh}, \Rule{C-InEx}, and by
analogous reasoning, we obtain
$\exists\twars.\cxlet{\ienv;\tref{\eS'\eS''}}
{\calcule{\et'/\es'\es''}{\ttyp/\eS'\eS''}}$~\dlabel{rightq}.
Thus, \dref{leftq} entails \dref{rightq}.
Given \dref{frais}, \dref{fresh}, given $\ftv{\eS'\eS''}\subseteq\twars\cup
\ftv{\eS\eS''}$, and given that $\eS'\eS''$ extends
$\eS\eS''$, this establishes the goal \dref{goal}.

\proofcase\Rule{R-Context}. The hypothesis is $\et/\es \sr \et'/\es'$. The
goal is $\ecin\ec\et/\es \sr \ecin\ec{\et'}/\es'$. Because $\topreduces$
relates closed configurations only, we may assume that the configuration
$\ecin\ec\et/\es$ is closed, so the memory locations that appear free within
$\ec$ are members of $\Dom\es$. Let us now reason by induction on the
structure of $\ec$.

\proofsubcase $\ec=\ehole$. The hypothesis and the goal coincide.

\demoreset\proofsubcase $\ec=\eapp{\ec_1}{\et_1}$. The induction hypothesis is
$\ec_1[\et]/\es \sr \ec_1[\et']/\es'$~\dlabel{hr}. Let us introduce a
type $\ttyp$ and a store type $\eS$ such that $\Dom\eS=\Dom\es$.  Consider the
constraint
$\cxlet{\ienv;\tref\eS}{\calcule{\ecin\ec\et/\es}{\ttyp/\eS}}$~\dlabel{c}. By
definition of constraint generation, \Rule{C-ExAnd}, \Rule{C-InEx}, and
\Rule{C-InAnd},
it is equivalent to
$$\exists\tvar.(
\cxlet{\ienv;\tref\eS}{\calcule{\ecin{\ec_1}\et/\es}{\tvar\arw\ttyp/\eS}}
\wedge
\cxlet{\ienv;\tref\eS}{\calcule{\et_1}\tvar}
)~\dlabel{d}$$
where $\tvar\not\in\ftv{\ttyp,\eS}$~\dlabel{x}.
By \dref{hr}, there exist type variables $\twars$ and a store type $\eS'$ such
that $\disjoint\twars{\ftv{\tvar,\ttyp,\eS}}$~\dlabel{fresh} and
$\ftv{\eS'}\subseteq\twars\cup\ftv\eS$~\dlabel{in} and
$\Dom{\eS'}=\Dom{\es'}$ and $\eS'$ extends $\eS$
and \dref{d} entails
$$
\exists\tvar.(
\exists\twars.\cxlet{\ienv;\tref{\eS'}}
                     {\calcule{\ecin{\ec_1}{\et'}/\es'}{\tvar\arw\ttyp/\eS'}}
\wedge
\cxlet{\ienv;\tref\eS}{\calcule{\et_1}\tvar}
)~\dlabel{e}.
$$
We pointed out earlier that the memory locations that appear free in $\et_1$
are members of $\Dom\eS$, which implies
$\cxlet{\tref\eS}{\calcule{\et_1}\tvar} \logeq
\cxlet{\tref{\eS'}}{\calcule{\et_1}\tvar}$~\dlabel{eq}.
By \dref{fresh}, \Rule{C-ExAnd}, \dref{eq}, \Rule{C-InAnd}, and by
definition of constraint generation, we find
that \dref{e} is equivalent to
$$
\exists\tvar\twars.
\cxlet{\ienv;\tref{\eS'}}
      {(\calcule{\ecin{\ec_1}{\et'}}{\tvar\arw\ttyp}
        \wedge\calcule{\et_1}\tvar
        \wedge\calcule{\es'}{\eS'})}~\dlabel{f}.
$$
\dref{x}, \dref{fresh} and \dref{in} imply $\tvar\not\in\ftv{\eS'}$. Thus,
by \Rule{C-InEx} and \Rule{C-ExAnd}, \dref{f} may be written
$$
\exists\twars.
\cxlet{\ienv;\tref{\eS'}}
      {(\exists\tvar.(\calcule{\ecin{\ec_1}{\et'}}{\tvar\arw\ttyp}
        \wedge\calcule{\et_1}\tvar)
        \wedge\calcule{\es'}{\eS'})},
$$
which, by definition of constraint generation, is
$$\exists\twars.\cxlet{\ienv;\tref{\eS'}}
{\calcule{\ecin\ec{\et'}/\es'}{\ttyp/\eS'}}~\dlabel{g}.$$
Thus, we have proved that \dref{c} entails \dref{g}.
By Definition~\ref{def-sr-inclusion}, this  establishes
$\ec[\et]/\es \sr \ec[\et']/\es'$.

\proofsubcase $\ec=\eapp\ev{\ec_1}$. Analogous to the previous subcase.

\demoreset\proofsubcase $\ec=\elet\evar{\ec_1}{\et_1}$.  The induction
hypothesis is $\ec_1[\et]/\es \sr \ec_1[\et']/\es'$~\dlabel{hr}. This subcase
is particularly interesting, because it is where \letpoly and side effects
interact. In the previous two subcases, we relied on the fact that the
$\exists\twars$ quantifier, which hides the types of the memory cells created
by the reduction step, \emph{commutes} with the connectives $\exists$ and
$\wedge$ introduced by application contexts. However, it does not in general
(left-)commute with the \kwd{let} connective
(Example~\ref{example-let-left-ex}). Fortunately, under the value restriction,
this subcase \emph{never arises} (Lemma~\ref{lemma-value-restriction}). By
Definition~\ref{def-value-restriction}, this subcase may arise only if all
constants have pure semantics, which implies $\es=\es'=\nostore$. Then, we
have
%
$$\begin{array}{rlr}
  & \cxlet\ienv{\calcule{\ecin\ec\et}\ttyp} \\
= & \cxlet{\ienv;\evar:\scheme\tvar{\calcule{\ecin{\ec_1}\et}\tvar}\tvar}
          {\calcule{\et_1}\ttyp} & \dlabel{deux} \\
\logeq &
\cxlet{\ienv; \evar:
       \scheme\tvar{\cxlet\ienv{\calcule{\ecin{\ec_1}\et}\tvar}}\tvar}
      {\calcule{\et_1}\ttyp} & \dlabel{trois} \\
\entails &
\cxlet{\ienv; \evar:
       \scheme\tvar{\cxlet\ienv{\calcule{\ecin{\ec_1}{\et'}}\tvar}}\tvar}
      {\calcule{\et_1}\ttyp} & \dlabel{quatre} \\
\logeq &
\cxlet\ienv{\calcule{\ecin\ec{\et'}}\ttyp} & \dlabel{cinq}
\end{array}$$
%
where \dref{deux} is by definition of constraint generation; \dref{trois}
follows from $\ftv\ienv=\fpv\ienv=\varnothing$ and \Rule{C-LetDup};
\dref{quatre} follows from \dref{hr}, specialized to the case of a pure
semantics; and \dref{cinq} is obtained by performing these steps in
reverse.
\end{Proof}
%
\begin{full}
\begin{exercise}[\Recommended, \Moderate]
\label{ex/monoref}
Try to carry out the last subcase of the above proof in the case of an impure
semantics and in the absence of the value restriction. Find out why it fails.
Show that it succeeds if $\twars$ is assumed to be empty. Use this fact to
prove that generalization is still safe when restricted to
\emph{nonexpansive} expressions, provided (i) evaluating a nonexpansive
expression cannot cause new memory cells to be allocated, (ii) nonexpansive
expressions are stable by substitution of values for variables, and (iii)
nonexpansive expressions are preserved by reduction.  \solref{monoref}
\end{exercise}
\end{full}

\begin{comment}
\def \etn {{\kwtt b}}
\def \ecn {{\cal B}}
\def \etnv {{\vec\etn}}
\begin{remark}
In Exercise~\ref {ex/monoref} we proposed to let nonexpansive expressions
rather than just values be polymorphic.  Here we propose to further allow to
let expansive expressions to be partially polymorphic.

Nonexpansive expressions $\etn$ can be defined follows.
A constant is said \emph{nonexpansive} if either it is a constructor
or destructors whose reduction (associate $\delta$-rules) is purely
functional. An outermost application $\eapp[3]{\et_0}{\et_1}\ldots{\et_k}$
is said \emph{nonexpansive} if $\et_0$ is a constant $\econ$ that is either
partially applied (\ie. $k < \arity\econ$); it is expansive otherwise, 
that is, if $\et_0$ is a variable or an expansive constant $\econ$ of arity
$k$ or a constant of arity strictly lower than $k$. A term $\et$ is
\emph{nonexpansive} if all expansive applications occur under an
abstraction. All applications of a term can be annotated as expansive
$\flat$ or nonexpansive $\natural$
%
Note that evaluation of nonexpansive expressions are purely functional.
Conversely, expressions that are purely function are not necessarily
nonexpansive since expansiveness is only a syntactic approximation.  For
instance, $\elet \evar {\et} {\et'}$ is purely functional if $\et$ loops and
is expansive if $\et'$ is.

$$
\def \eq{{\;}={\;}&}
\begin{tabular}{R:L}
    {\calcule\evid\ttyp}
\eq \ccall\evid\ttyp / \emptyset
\\
\calcule{\efun\evar\et}\ttyp
\eq \exists\tvar_1\tvar_2.(
      \clambda\evar{\tvar_1}{\calcule\et{\tvar_2}}
      \wedge
      \tvar_1\arw\tvar_2\subtype\ttyp 
) / \emptyset \\
\calcule{(\eapp{\et_1}{\et_2})^\natural}\ttyp
\eq \exists\tvar_2.(
  \elet {C_1/\evarc_1} {\calcule{\et_1}{\tvar_2\arw\ttyp}} 
  {\\&\qquad\;
\elet {C_2/\evarc_2} {\calcule{\et_2}{\tvar_2}}
    {\\&\qquad\;
      C_1 \wedge C_2 / \evarc_1, \evarc_2}
}
) \\
\calcule{(\eapp{\et_1}{\et_2})^\flat}\ttyp
\eq \exists\tvar_2.(
  \elet {C_1/\evarc_1} {\calcule{\et_1}{\tvar_2\arw\ttyp}} 
  {\\&\qquad\;
\elet {C_2/\evarc_2} {\calcule{\et_2}{\tvar_2}}
    {\\&\qquad\;
      C_1 \wedge C_2 / \evarc_1, \evarc_2, \ftv{\ttyp}^\flat}
}
) \\
\calcule{\elet\evar{\et_1}{\et_2}}\ttyp ^\twars
\eq
 \cxlet{\evar:\scheme\tvar{\calcule{\et_1}\tvar}\tvar}{\calcule{\et_2}\ttyp}
\end{tabular}
$$


Let $\ecn$ denote contexts of defined by the following grammar
$$
\ecn ::=
\ehole 
\mid \etn
\mid\eapp \ecn \et \mid \eapp \etn \ecn  
\mid \elet \evar  \ecn \et
\mid \elet \evar \etn \ecn
$$
and such that all applications but those in $\et$ are nonexpansive. 

A program $\et$ that is expansive can always be written 
as $\ecin \ecn \et$ where $\et$ is a let expression or an expansive
application. 
\def \rw {\Longrightarrow}
Let the relation $\rw$ be defined as follows
$$
\begin{tabular*}{\linewidth}{c.R,L.l,}&
\ecin \ecn {\elet \evar \et {\et'}} 
\rw&
\elet \evar \et \ecin \ecn {\etn'}
& $\et$ expansive
\\&
\ecin \ecn {\et} 
\rw&
\elet \evar \et \evar
&
$\et$ non variable.
\\
\end{tabular*}
$$
\vfill

$\ecin\ecn{\evar_1, \ldots, \evar_n}$ is nonexpansive and all holes
are exposed, \ie. not under a $\lambda$-abstraction.
%
An arbitrary expression $\et$ can always be decomposed as 
$\ecin \ecn{\et_1,\ldots,\et_k}$ where $\et_i$'s are expansive 
applications. That is, each $\et_i$ is of the form $\eapp[3] {\econ_i} 
{\et_i^1} \ldots {\et_i^1}$ and $\et_i^j$ can in turn be decomposed. 

Let extend $\calcule {\elet \evar \et {\et'}}{\ttyp}$ for expansive
expressions $\et=\ecin \ecn {\et_1, \ldots, \et_k}$ be recursively
defined as: 
$$
\begin{tabular}{RL}
% &\calcule {\elet \evar \et
% {\et'}}{\ttyp}
% \\= &
\calcule {\elet {\evar_1} {\et_1}  
{\ldots \elet {\evar_k} {\et_k} 
{\elet \evar {\ecin \ecn{\evar_1, \ldots, \evar_k}}}}
{\et'}}{\ttyp}
\end{tabular}
$$
where $\disjoint {\evar_i} {\evar, \fpv \ecn, \et_1, \ldots \et_k}$.
\end{remark}
\begin{exercise}[\Recommended, \Challenging]
Show that the above typing rule is sound.
Show that 
$\calcule {\elet \evar {\ecin \ecn{\et_1, \ldots, \et_k}}{\et'}}{\ttyp}$ can
also be written:  
$$
\begin{tabular}{RL}
% \\= &
% \cxlet
%  {\evar_1 : 
%    \cexists {\tvar_1} {\calcule {\et_1}{\tvar_1}}}  
%       {\ldots \\&
%        \cxlet
%           {\evar_k : \cexists {\tvar_k} {\calcule {\et_1}{\tvar_k}}}
% {\calcule
%   {\elet \evar {\ecin \ecn{\evar_1, \ldots, \evar_k}}
%   {\et'}}
% {\ttyp}}}
% \\= &
% \cexists {\tvar_1, \ldots \tvar_k}
% {\\&\qquad \cxlet
%  {\evar_1 : \calcule {\et_1}{\tvar_1}}
%       {\ldots \\&\qquad
%        \cxlet
%           {\evar_k : \calcule {\et_1}{\tvar_k}}
% {\cxlet
%   {\evar: 
%     \scheme {\tvar}
%       {\calcule {\ecin \ecn{\evar_1, \ldots, \evar_k}} {\tvar}}}
%   {\calcule{\et'}{\ttyp}}
% }}}
%\\= &
&
\cexists {\tvar_1, \ldots \tvar_k}
{\cxlet
  {\evar: 
    \scheme {\tvar}
      {\calcule {\ecin \ecn{\evar_1,\ldots,\evar_k}^{\tvar_1,\ldots\tvar_k}}
        {\tvar}}}
  {\calcule{\et'}{\ttyp}}}
\end{tabular}
$$
where 
$\calcule {\ecin \ecn{\evar_1, \ldots,
\evar_k}^{\tvar_1,\ldots\tvar_k}}{\tvar}$ 
is
$$
\begin{tabular}{RL}
\wedge
\begin{cases}
\calcule {\et_1}{\tvar_1} \wedge \ldots \wedge \calcule {\et_k}{\tvar_k}
\\
\cxlet {\evar_1 : \tvar_1} 
  {\ldots
     \cxlet {\evar_k : \calcule {\et_1}{\tvar_k}}
        {\calcule {\ecin \ecn{\evar_1, \ldots, \evar_k}} {\tvar}}}
\\
\end{cases}
\end{tabular}
$$ 

\solref{bestref}
\end{exercise}

\begin{solution}{bestref}
The soundness follow
\end{solution}

\end{comment}



Subject reduction ensures that well-typedness is preserved by reduction.
%
\begin{corollary}
\label{lemma-subject-reduction}
Let $\cconf\et\es \topreduces \cconf{\et'}{\es'}$. If $\cconf\et\es$ is well-typed, then so is
$\cconf{\et'}{\es'}$.
\end{corollary}
%
\begin{Proof}
\demoreset Assume $\cconf\et\es \topreduces \cconf{\et'}{\es'}$~\dlabel{red} and $\cconf\et\es$
is well-typed~\dlabel{wt}. By \dref{wt} and
Definition~\ref{def-typing-stores}, there exist a type $\ttyp$ and a store
type $\eS$ such that $\Dom\es=\Dom\eS$ and the constraint
$\cxlet{\ienv;\tref\eS}{\calcule{\cconf\et\es}{\ttyp/\eS}}$~\dlabel{c} is
satisfiable. By Theorem~\ref{subject-reduction} and
Definition~\ref{def-sr-inclusion}, \dref{red} implies that there exist a set
of type variables $\twars$ and a store type $\eS'$ such that
$\Dom{\eS'}=\Dom{\es'}$~\dlabel{s} and the constraint \dref{c} entails
$\exists\twars.  \cxlet{\ienv;
\tref{\eS'}}{\calcule{\cconf{\et'}{\es'}}{\ttyp/\eS'}}$~\dlabel{d}.  Because \dref{c}
is satisfiable, so is \dref{d}, which implies that $\cxlet{\ienv;
\tref{\eS'}}{\calcule{\cconf{\et'}{\es'}}{\ttyp/\eS'}}$ is satisfiable~\dlabel{sat}.
By \dref{s} and \dref{sat} and Definition~\ref{def-typing-stores}, $\cconf{\et'}{\es'}$
is well-typed.
\end{Proof}

Let us now \shortfull{state}{establish} the progress property.
%
\begin{full}
\begin{lemma}
\label{lemma-wt}
If $\cconf{\eapp{\et_1}{\et_2}}\es$ is well-typed, then $\cconf{\et_1}\es$ and $\cconf{\et_2}\es$ are
well-typed. If $\cconf{\elet\evar{\et_1}{\et_2}}\es$ is well-typed, then $\cconf{\et_1}\es$
is well-typed.
\end{lemma}
%
\begin{Proof}
\demoreset
Let us prove the second statement; the proof of the first one is analogous.
Because $\cconf{\elet\evar{\et_1}{\et_2}}\es$ is
well-typed, there exist a type $\ttyp$ and a store type $\eS$ such that
$\Dom\es=\Dom\eS$ and the constraint
$\cxlet{\ienv;\tref\eS}
{\calcule{\cconf{\elet\evar{\et_1}{\et_2}}\es}{\ttyp/\eS}}$~\dlabel{c} is
satisfiable. By definition of constraint generation, \dref{c} is
$\cxlet{\ienv;\tref\eS}{((
\cxlet{\evar:\scheme\tvar{\calcule{\et_1}\tvar}\tvar}{\calcule{\et_2}\ttyp}
)\wedge
\calcule\es\eS
)}$~\dlabel{d},
where $\tvar\not\in\ftv{\ttyp,\eS}$~\dlabel{fresh}.
By definition of \kwd{let}, \dref{d} entails
$\cxlet{\ienv;\tref\eS}{((
\exists\tvar.\calcule{\et_1}\tvar
)\wedge
\calcule\es\eS
)}$, which, by \dref{fresh}, \Rule{C-ExAnd} and \Rule{C-InEx}, is
equivalent to
$\exists\tvar.\cxlet{\ienv;\tref\eS}
{\calcule{\cconf{\et_1}\es}{\tvar/\eS}}$~\dlabel{f}. Thus,
\dref{f} is satisfiable, so $\cxlet{\ienv;\tref\eS}
{\calcule{\cconf{\et_1}\es}{\tvar/\eS}}$ is satisfiable as well,
and $\cconf{\et_1}\es$ is well-typed.
\end{Proof}
\end{full}
%
\begin{theorem}[Progress]
\label{theorem-progress}
If $\cconf\et\es$ is well-typed, then either it is reducible, or $\et$ is a value.
\end{theorem}
%
\begin{Proof}
The proof is by induction on the structure of $\et$.

\proofcase $\et=\evar$. Well-typed configurations are closed: this
case cannot occur.

\proofcase $\et=\eloc$. $\et$ is a value.

\proofcase $\et=\econ$. By requirement (ii) of
Definition~\ref{def-requirements}.

\proofcase $\et=\efun\evar{\et_1}$. $\et$ is a value.

\proofcase $\et=\eapp{\et_1}{\et_2}$. By Lemma~\ref{lemma-wt}, $\cconf{\et_1}{\es}$ is
well-typed. By the induction hypothesis, either it is reducible, or $\et_1$ is
a value. If the former, by \Rule{R-Context} and because every context of the
form $\eapp\ec{\et_2}$ is an evaluation context, the configuration $\cconf\et\es$
is reducible as well. Thus, let us assume $\et_1$ is a value. By
Lemma~\ref{lemma-wt}, $\cconf{\et_2}\es$ is well-typed. By the induction hypothesis,
either it is reducible, or $\et_2$ is a value. If the former, by
\Rule{R-Context} and because every context of the form
$\eapp{\et_1}\ec$---where $\et_1$ is a value---is an evaluation context, the
configuration $\cconf\et\es$ is reducible as well. Thus, let us assume $\et_2$ is a
value. Let us now reason by cases on the structure of $\et_1$.

\proofsubcase $\et_1=\evar$. Again, this subcase cannot occur.

\proofsubcase $\et_1=\eloc$. Because $\cconf\et\es$ is well-typed, a constraint of
the form $\cxlet{\ienv;\tref\eS}{(\exists\tvar.(\ccall\eloc{\tvar\arw\ttyp}
  \wedge\calcule{\et_2}\tvar)\wedge\calcule\es\eS)}$ must be satisfiable.
Furthermore, because $\cconf\et\es$ is closed, $\eloc$ is a member of $\Dom\eS$.
These imply that the constraint
$\tref{\eS(\eloc)}\subtype\tvar\arw\ttyp$ is satisfiable. Because the type
constructors $\tcref$ and $\arw$ are incompatible, this
is a contradiction. So, this subcase cannot occur.

\proofsubcase $\et_1=\efun\evar{\et'_1}$. By \Rule{R-Beta}, $\cconf\et\es$ is
reducible.

\proofsubcase $\et_1=\eapp[3]\econ{\ev_1}\ldots{\ev_k}$. Then, $\et$ is of the
form $\eapp[3]\econ{\ev_1}\ldots{\ev_{k+1}}$. The result follows by
requirement (ii) of Definition~\ref{def-requirements}.

\proofcase $\et=\elet\evar{\et_1}{\et_2}$. By Lemma~\ref{lemma-wt},
$\cconf{\et_1}\es$ is well-typed. By the induction hypothesis, either $\cconf{\et_1}\es$ is
reducible, or $\et_1$ is a value. If the former, by \Rule{R-Context} and
because every context of the form $\elet\evar\ec{\et_2}$ is an evaluation
context, the configuration $\cconf\et\es$ is reducible as well. If the latter, then
$\cconf\et\es$ is reducible by \Rule{R-Let}.
\end{Proof}

We may now conclude:
%
\begin{theorem}[Type Soundness]
\label{theorem-type-soundness}
Well-typed source programs do not go wrong.~\endpf
\end{theorem}
%
\begin{Proof}
We say that a source program $\et$ is well-typed if and only if the
configuration $\et/\nostore$ is well-typed, that is, if and only if
$\exists\tvar.\cxlet\ienv{\calcule\et\tvar}\logeq\ctrue$ holds. By
Lemma~\ref{lemma-subject-reduction}, all reducts of $\et/\nostore$ are
well-typed. By Theorem~\ref{theorem-progress}, none is stuck.
\end{Proof}

Recall that this result holds only if the requirements of
Definition~\ref{def-requirements} are met. In other words, some proof
obligations remain to be discharged when concrete definitions of $\econset$,
$\reduces[\delta]$, and $\ienv$ are given. This is illustrated by several
examples in \S\ref{section-calc-lang} and \S\ref{section-rows}.

\begin{full}
Of course, the type system is, in general, incomplete: that is, there exist
source programs that do not go wrong, yet are (unfortunately) ill-typed.  More
precisely, let us assume that there exists at least one ill-typed source
program $\et_0$. Then, it is easy to reduce the problem of determining whether
a program terminates to the problem of determining whether a program goes
wrong: indeed, $\et$ terminates if and only if $\et;\et_0$ goes wrong. Because
the $\lambda$-calculus is Turing complete, its termination problem is
undecidable, so determining whether a program goes wrong must be undecidable
as well. However, assuming that constraint satisfiability is decidable,
well-typedness in \hmx is decidable. Thus, the type system must be incomplete.
%
Note that this argument is valid not only for \hmx, but for any Turing
complete programming language equipped with a decidable type system. Of
course, the hypothesis that some programs \emph{do} go wrong is important:
otherwise, a trivial type system, where every program is well-typed, is both
sound and complete. This is illustrated by the next exercise.
%
\begin{exercise}[\QuickCheck]
\solref{all-terms-typable} Show that, when the set of constants $\econset$ is
empty (that is, when the programming language is the pure $\lambda$-calculus)
and when types are interpreted in a regular tree model (that is, when
recursive types exist), every closed source program is well-typed.
\end{exercise}
\end{full}

% --------------------------------------------------------------------------

\section{Constraint Solving}
\label{section-solver}

\index{constraints!solving|(}

We have introduced a parameterized constraint language, given
equivalence laws describing the interaction between its logical
connectives, and exploited them to prove theorems about type inference
and type soundness, which are valid independently of the nature of
primitive constraints---the so-called predicate applications. However,
there would be little point in proposing a parameterized constraint
solver, because much of the difficulty of designing an efficient
constraint solver  lies precisely in the treatment of primitive
constraints and in its interaction with \letpoly. In
this section, we focus on constraint solving in the setting of an
\emph{equality-only free tree model}. Thus, the constraint solver
developed here allows performing type inference for \hmeq (that is,
for Damas and Milner's type system) and for its extension with
recursive types. Of course, some of its mechanisms may be useful in
other settings. The program analysis and type inference literature
abounds with constraint-based systems of all kinds; a short list of
papers that put particular emphasis on constraint solving is
\fulllongcite{LICS::AikenW1992}, \fulllongcite{Henglein93},
\fulllongcite{mueller-niehren-podelski-ines-97},
\fulllongcite{faehndrich-phd-99}, \fulllongcite{melski-reps-00},
\fulllongcite{mueller-feature-01}, \fulllongcite{pottier-ic-01},
\fulllongcite{nielson-02}, McAllester \citeyr{mcallester-02,mcallester-03},
and \fulllongcite{simonet-solver-03}. 
% NOTE Cette liste est incomplète et arbitraire.

We begin with a rule-based presentation of a standard, efficient first-order
unification algorithm. This yields a constraint solver for a subset of the
constraint language, deprived of type scheme introduction and instantiation
forms. On top of it, we build a full constraint solver, which corresponds to
the code that accompanies \this.
\begin{full}
Then, we discuss another constraint solver, which uses a different strategy to
eliminate type scheme introduction and instantiation constraints. We conclude
with a brief discussion of type errors.
\end{full}

\subsection*{Unification}
\label{section-unification}

\index{unification|(}

Unification is the process of solving equations between terms. It was
first introduced by \longcite{Robinson71}, but his original algorithm
could be very inefficient. Efficient algorithms, which perform
unification in quasi-linear time, were independently proposed by
Martelli and Montanari \citeyr{Martelli-Montanari-76,Martelli-Montanari-82} and by
Huet~(\citeyear[Chapter 5]{HuetThesis}). Both algorithms rely on a
data structure that efficiently solves the \emph{union-find} problem
\cite {tarjan-75}. Martelli and Montanari's algorithm performs
unification in topological (top-down) order, and is thus restricted to
the acyclic case, that is, to the case where equations are interpreted
in a syntactic model. In this specific case, unification may actually
be performed in truly linear time \cite{Paterson-Wegman-78}. On the
other hand, Huet's algorithm is able to deal with cyclic
structures. The acyclicity check is postponed until the very end of
the solving process if equations are interpreted within a syntactic
model, or omitted altogether if working within a regular tree
model. Except for the final acyclicity check, Huet's algorithm is
incremental. Furthermore, it is simple;  we present a version of
it here. \longcite{knight-89} and \longcite{Baader-Siekmann-1993} also
describe Huet's algorithm, and provide further historical background
and references.

Following \longcite{jouannaud-kirchner-91}, we specify
the algorithm as a (nondeterministic) system of constraint rewriting rules. As
suggested above, it is almost the same for {finite} and {regular}
tree models; only one rule, which implements the \index{occurs check}\emph{\occurs}, must be
removed in the latter case. In other words, the algorithm works with
{possibly cyclic} data structures and does not rely in an essential way
on the \occurs. In order to more closely reflect the behavior of the actual
algorithm, and in particular the union-find data structure, we modify the
syntax of constraints by replacing equations with
{multi-equations}---equations involving an arbitrary number of types, as
opposed to exactly two. 
%
\begin{definition}
\label{def-meq}
Let there be, for every kind $\kind$ and for every $n\geq1$, a predicate
$=^n_\kind$, of {\psignature} $\kind^n\kindarrow\cdot$, whose interpretation
is ($n$-ary) equality. The predicate constraint
$=^n_\kind\,\ttyp_1\,\ldots\,\ttyp_n$ is written $\ttyp_1=\ldots=\ttyp_n$, and
called a \emph{multi-equation}. We consider the constraint $\ctrue$ as a
multi-equation of length~$0$ and let $\epsilon$ range over all
multi-equations. In the following, we identify multi-equations up to
permutations of their members, so a multi-equation $\meq$ of kind $\kind$ may
be viewed as a finite \emph{multiset} of types of kind $\kind$. We write
$\meq=\meq'$ for the multi-equation obtained by concatenating $\meq$ and
$\meq'$.
\end{definition}

Thus, we are interested in the following subset of the constraint language:
%
$$\mc ::= \ctrue \mid \cfalse \mid \meq \mid \mc\wedge\mc \mid
          \exists\tvars.\mc$$
%
Equations are replaced with multi-equations; no other predicates are
available. Type scheme introduction and instantiation forms are absent.
%
\begin{definition}
\label{def-normal-meq}
A multi-equation is \emph{\standard}\indexfull{standard multi-equation}
if and only if its variable members are
distinct and it has at most one nonvariable member. A constraint $\mc$ is
\emph{\standard} if and only if every multi-equation inside $\mc$ is
{\standard} and every variable that occurs (free or bound) in $\mc$ is a
member of at most one multi-equation inside $\mc$.
(Note that \emphfull{to be a member of $\meq$} implies, but is not equivalent
to, \emphfull{to occur free in $\meq$}.)
\end{definition}

A union-find algorithm maintains equivalence classes (that is, disjoint sets)
of variables, and associates with each class a \emph{descriptor}, which in
our case is either absent or a nonvariable term. Thus, a \emph{\standard}
constraint represents a state of the union-find algorithm. A constraint that
is {not} {\standard} may be viewed as a superposition of a state of the
union-find algorithm, on the one hand, and of control information, on the
other hand. For instance, a multi-equation of the form $\meq=\ttyp_1=\ttyp_2$,
where $\meq$ is made up of distinct variables and $\ttyp_1$ and $\ttyp_2$ are
nonvariable terms, may be viewed, roughly speaking, as the equivalence class
$\meq=\ttyp_1$, together with a pending request to solve $\ttyp_1=\ttyp_2$ and
to update the class's descriptor accordingly.  Because multi-equations encode
both state and control, our specification of the unification algorithm remains
rather abstract. It would be possible to give a lower-level description, where
state ({\standard} conjunctions of multi-equations) and control (pending
binary equations) are distinguished.

% TEMPORARY on pourrait faire construire cette description de bas niveau
% en exercice. Cf. mon sujet d'examen de DEA 2003.

\begin{definition}
\label{def-cyclic}
Let $\mc$ be a conjunction of multi-equations. $\twar$ is \emph{dominated} by
$\tvar$ with respect to $\mc$ (written: $\twar\isdominated\mc\tvar$) if and
only if $\mc$ contains a conjunct of the form $\tvar=\tycon\,\ttypc=\meq$,
where $\twar\in\ftv\ttyps$. $\mc$ is \emph{cyclic} if and only if the graph of
$\isdominated\mc$ exhibits a cycle.
\end{definition}

The specification of the unification algorithm consists of a set of constraint
rewriting rules, given in Figure~\ref{fig:unif-trs}. Rewriting is performed
modulo \aconv, modulo permutations of the members of a multi-equation, modulo
commutativity and associativity of conjunction, and under an arbitrary
context. The specification is nondeterministic: several rule instances may be
simultaneously applicable.

\begin{widefigure}
\TTtoprule
\vspace*{-1ex}
$$
\begin{regles}
% Extrusion de l'existentiel.
\regle{S-ExAnd}
  {(\exists\tvars.\mc_1) \wedge \mc_2}
  {\exists\tvars.(\mc_1 \wedge \mc_2)}
  {\disjoint\tvars{\ftv{\mc_2}}}

% Fusion des multi-équations.
\regle{S-Fuse}
  {\tvar=\meq \wedge \tvar=\meq'}
  {\tvar=\meq=\meq'}
  {}

% Élimination des variables redondantes.
\regle{S-Stutter}
  {\tvar=\tvar=\meq}
  {\tvar=\meq}
  {}

% Décomposition structurelle, sur des petits termes seulement.
\regle{S-Decompose}
  {\tycon\,\tvarc=\tycon\,\ttypc=\meq}
  {\tvarc=\ttypc \wedge
   \tycon\,\tvarc=\meq}
  {}

% Découpage en petits termes.
\regle{S-Name-1}
  {\tycon\,\ttyp_1\,\ldots\,\ttyp_i\,\ldots\,\ttyp_n=\meq}
  {\exists\tvar.(\tvar=\ttyp_i \wedge
   \tycon\,\ttyp_1\,\ldots\,\tvar\,\ldots\,\ttyp_n=\meq)}
  {\ttyp_i\not\in\tyvarset \wedge
   \tvar\not\in\ftv{\ttyp_1,\ldots,\ttyp_n,\meq}}

% Erreur.
\regle{S-Clash}
  {\tycon\,\ttypc=\tycon'\,\ttypcp=\meq}
  {\cfalse}
  {\tycon\not=\tycon'}

% Suppression des multi-équations de taille 1.
\regle{S-Single}
  {\ttyp}
  {\ctrue}
  {}
% On pourrait restreindre la règle au cas où $\ttyp$ n'est pas une variable, pour mieux
% refléter l'implantation, où toute variable doit apparaître dans au moins une multi-équation.
% Cependant, sans explications, cette restriction serait surprenante. En plus, à ce niveau
% de détail, il n'est pas vraiment facile de déterminer si un ``reflet'' est fidèle ou non...

% Élimination des multi-équations vides.
\regle{S-True}
  {\mc\wedge\ctrue}
  {\mc}
  {}

% Occurs check.
\regle{S-Cycle}
  {\mc}
  {\cfalse}
  {\text{the model is syntactic and $\mc$ is cyclic}}

% Propagation de l'erreur.
\regle{S-Fail}
  {\ucon[\cfalse]}
  {\cfalse}
  {\ucon\not=\chole}
\end{regles}
$$ \\
\vspace*{-2ex}
\TTbottomrule \\
\vspace*{-2ex}
\bcpcaption{unif-trs}{Unification}
\end{widefigure}

\Rule{S-ExAnd} is a directed version of \Rule{C-ExAnd}, whose effect is to
float up all existential quantifiers. In the process, all multi-equations
become part of a single conjunction, possibly causing rules whose left-hand
side is a conjunction of multi-equations, namely \Rule{S-Fuse} and
\Rule{S-Cycle}, to become applicable. \Rule{S-Fuse} identifies two
multi-equations that share a common variable $\tvar$, and fuses them.
%
% An efficient implementation of it relies on both ``find'' (to identify an
% instance of the left-hand side) and ``union'' (to rewrite it into the
% corresponding instance of the right-hand side) operations.
%
The new multi-equation is not necessarily \standard, even if the two original
multi-equations were. Indeed, it may have repeated variables or contain two
nonvariable terms. The purpose of the next few rules, whose left-hand side
consists of a single multi-equation, is to deal with these situations.
\Rule{S-Stutter} eliminates redundant variables. It only deals with variables,
as opposed to terms of arbitrary size, so as to have constant time cost. The
comparison of nonvariable terms is implemented by \Rule{S-Decompose} and
\Rule{S-Clash}. \Rule{S-Decompose} decomposes an equation between two terms
whose head symbols match. It produces a conjunction of equations between their
subterms, namely $\tvarc=\ttypc$. Only one of the two terms remains in the
original multi-equation, which may thus become \standard. The terms $\tvarc$
are copied: there are two occurrences of $\tvarc$ on the right-hand side. For
this reason, we require them to be type variables, as opposed to terms of
arbitrary size. (We slightly abuse notation by using $\tvarc$ to denote a
vector of type variables whose elements are {not} necessarily distinct.)
By doing so, we allow explicit reasoning about \emphfull{sharing}: since a
variable represents a pointer to an equivalence class, we explicitly specify
that only \emphfull{pointers}, not whole terms, are copied.  As a result of this
decision, \Rule{S-Decompose} is not applicable when both terms at hand have a
nonvariable subterm.  \Rule{S-Name-1} remedies this problem by introducing a
fresh variable that stands for one such subterm.  When repeatedly applied,
\Rule{S-Name-1} yields a unification problem composed of so-called \emph{small
terms} only---that is, where sharing has been made fully
explicit. \Rule{S-Clash} complements \Rule{S-Decompose} by dealing with the
case where two terms with different head symbols are equated; in a free tree
model, such an equation is false, so failure is signaled.
\Rule{S-Single} and \Rule{S-True} suppress multi-equations of size 1 and 0,
respectively, which are tautologies.
\Rule{S-Cycle} is the \occurs: it signals failure if the constraint
is cyclic.  It is applicable only in the case of syntactic unification, that
is, when ground types are finite trees. It is a global check: its left-hand
side is an entire conjunction of multi-equations.  \Rule{S-Fail} propagates
failure; $\ucon$ ranges over unification constraint contexts.

The constraint rewriting system in Figure~\ref{fig:unif-trs} enjoys the
following properties. First, rewriting is strongly normalizing, so the rules
define a (nondeterministic) algorithm. Second, rewriting is
meaning-preserving. Third, every normal form is either $\cfalse$ or of the
form $\exists\tvars.\mc$, where $\mc$ is satisfiable. The latter two
properties indicate that the algorithm is indeed a constraint solver.

\begin{lemma}
\label{lemma-unif-trs-normalizes}
The rewriting system $\red$ is strongly normalizing.
\end{lemma}
%
\begin{full}
\begin{Proof}
\demoreset
\newcommand{\cw}[1]{\kw{cw}(#1)}
\newcommand{\iw}[1]{\kw{iw}(#1)}
\newcommand{\wg}[1]{\kw{w}(#1)}
Let every type constructor $\tycon$ have an integer \emph{weight} 
$\cw\tycon \geq 4 + 2 \times \arity\tycon$~\dlabel{w}. 
%
Define the \emph{internal weight} of a type by $\iw\tvar=0$ and
$\iw{\tapp \tycon \ttypc}=\cw\tycon + \Sigma_{\ttyp\in\ttypc} \iw\ttyp$. 
Define the \emph{weight} of a type by
$\wg\ttyp = \iw\ttyp + 1$ if $\ttyp \in \tyvarset$ and 
$\wg\ttyp = \iw\ttyp - 2$ otherwise. 
Then, $\iw\ttyp+1\geq\wg\ttyp$
holds for all terms $\ttyp$~\dlabel{D},
and $\iw\ttyp>\wg\ttyp+1$ holds for all nonvariable terms $\ttyp$~\dlabel{N}.
Define the 
weight of a multi-equation to be the sum of the weights of its members and the
weight of a constraint to be the sum of the weights of its multi-equations.
By \dref{w} and \dref{D}, \Rule{S-Decompose} is weight decreasing.
By \dref{N}, so is \Rule{S-Name-1}.
It is straightforward to check that \Rule{S-Fuse}, \Rule{S-Stutter},
\Rule{S-Clash}, \Rule{S-Single}, and 
\Rule{S-Cycle} are weight decreasing, while the remaining rules are
non-weight increasing. Thus, it is sufficient to check that \Rule{S-ExAnd},
\Rule{S-True}, and \Rule{S-Fail} form a strongly normalizing
system, which is immediate.
\end{Proof}
\end{full}
%
\begin{lemma}
\label{lemma-unif-trs-correct}
$\mc_1\red\mc_2$ implies $\mc_1\logeq\mc_2$.
\end{lemma}
%
\begin{lemma}
\label{lemma-unif-trs-normal-forms}
Every normal form is either $\cfalse$ or of the form $\xc[\mc]$, where $\xc$
is an existential constraint context, $\mc$ is a standard conjunction of
multi-equations and, if the model is syntactic, $\mc$ is acyclic. These
conditions imply that $\mc$ is satisfiable.
\end{lemma}
%
\begin{full}
\begin{Proof}
Let $\mc$ be a normal form. If $\mc$ contains $\cfalse$ as a subconstraint,
then, because $\mc$ is normal with respect to \Rule{S-Fail}, $\mc$ must be
$\cfalse$. Let us now assume that $\mc$ does not contain $\cfalse$. Because
$\mc$ is normal with respect to \Rule{S-ExAnd}, it must be
of the form $\xc[\mc']$, where $\mc'$ is also a normal form and does
not contain any existential quantification constructs. Thus, $\mc'$ must be a
(possibly empty) conjunction, whose conjuncts are either $\ctrue$ or
multi-equations. Because $\mc'$ is normal with respect to \Rule{S-True},
$\mc'$ must in fact be a (possibly empty) conjunction of multi-equations.
Because $\mc'$ is normal with respect to \Rule{S-Fuse}, every type variable is
a member of at most one multi-equation. Because $\mc'$ is normal with respect
to \Rule{S-Stutter}, \Rule{S-Decompose}, \Rule{S-Name-1}, and \Rule{S-Clash},
every multi-equation in $\mc'$ must be standard. Thus, $\mc'$ is standard.
Last, because $\mc'$ is normal with respect to \Rule{S-Cycle}, $\mc'$ must be
acyclic if the model is a finite tree model, as opposed to a regular tree
model.

Now, in each multi-equation, pick an arbitrary distinguished member, while
ensuring that every nondistinguished member is a variable; this is made
possible by the fact that every multi-equation is standard. Then, let $\mc''$
be the conjunction of multi-equations obtained from $\mc'$ by replacing every
multi-equation $\tvar_1=\ldots=\tvar_n=\ttyp$, where $\ttyp$ is the
distinguished member, with the conjunction of binary equations $\tvar_1=\ttyp
\wedge\ldots\wedge \tvar_n=\ttyp$. It is not difficult to check that
$\isdominated{\mc'}$ and $\isdominated{\mc''}$ coincide; so, when $\mc'$ is
acyclic, so is $\mc''$. Furthermore, because $\mc'$ is standard, every
equation in $\mc''$ has a distinct left-hand side; in other words, $\mc''$ is
of the form $\tvarc=\ttypc$. Naturally, $\mc'$ and $\mc''$ are equivalent, so
there only remains to show that $\mc''$ is satisfiable.

We now distinguish two cases. First, if the model is a regular tree
model, it is a standard result that every constraint of the form
$\tvarc=\ttypc$ is satisfiable \cite{Courcelle83}, so $\mc''$
is satisfiable. Second, assume that the model is a finite tree model. Then,
$\isdominated{\mc''}$ is acyclic. We have
$\mc''\logeq\tvarc_1=\ttypc_1\wedge\tvarc_2=\ttypc_2$~\dlabel{h1}, where
$\disjoint{\ftv{\ttyps_2}}{\tvars_1\tvars_2}$~\dlabel{h2}, provided $\tvarc_1$
is $\tvarc$, $\ttypc_1$ is $\ttypc$, and $\tvarc_2$ and $\ttypc_2$ are
empty. We now show, by induction on the length of the vectors $\tvarc_1$ and
$\ttypc_1$, that the satisfiability of $\mc''$ follows from \dref{h1} and
\dref{h2}. In the base case, $\tvarc_1$ and $\ttypc_1$ are empty, so $\mc''$
is equivalent to a constraint of the form $\tvarc_2=\ttypc_2$, where
$\disjoint{\ftv{\ttyps_2}}{\tvars_2}$, that is, a solved form. By
Lemmas~\ref{lemma-canonical-form} and~\ref{lemma-canonical-solutions}, $\mc''$
is satisfiable. In the inductive case, $\tvarc_1=\ttypc_1$ may be written
$\tvarcpi1=\ttypcpi1\wedge\tvar_1=\ttyp_1$, where $\tvar_1$ is minimal within
$\tvars_1$ with respect to $\isdominated{\mc''}$, which implies
$\disjoint{\tvars_1}{\ftv{\ttyp_1}}$. By \Rule{C-Eq}, $\mc''$ is equivalent to
$\tvarcpi1=\ttypcpi1\wedge(\tvar_1=\subst{\tvarc_2}{\ttypc_2}{\ttyp_1}\wedge
\tvarc_2=\ttypc_2)$. Furthermore, we have
$\disjoint{\ftv{\subst{\tvarc_2}{\ttypc_2}{\ttyp_1}}}{\tvars_1\tvars_2}$. The
result then follows from the induction hypothesis.
\end{Proof}
\end{full}

\index{unification|)}

% ----------------------------------------------------------------------------

\subsection*{A Constraint Solver}

\shortpage

On top of the unification algorithm, we now define a constraint solver. Its
specification is independent of the rules and strategy employed by the
unification algorithm. However, the structure of the unification algorithm's
normal forms as well as the logical properties of multi-equations are
exploited when performing generalization, that is, when creating and
simplifying type schemes. Like the unification algorithm, the constraint
solver is specified in terms of a \emphfull{reduction system}. However, the
objects that are subject to rewriting are not just constraints: they have more
complex structure. Working with such richer \emphfull{states} allows
distinguishing the solver's external language---namely, the full constraint
language, which is used to express the problem that one wishes to solve---and
an internal language, introduced below, which is used to describe the solver's
private data structures. In the following, $\co$ and $\cp$ range over
\emphfull{external} constraints, that is, constraints that were part of the
solver's input.  External constraints are to be viewed as abstract syntax
trees, subject to no implicit laws other than \aconv. As a simplifying
assumption, we require external constraints not to contain any occurrence of
$\cfalse$---otherwise the problem at hand is clearly
false. \emph{Internal} data structures include unification constraints
$\mc$, as previously studied, and \emph{stacks}, whose syntax is as
follows:
%
$$\st ::= \chole \mid 
          \st[\chole\wedge\co] \mid 
          \st[\exists\tvars.\chole] \mid
          \st[\cxlet{\evid:\scheme\tvars\chole\ttyp}\co] \mid
          \st[\cxlet{\evid:\ts}\chole]$$
%
In the second and fourth productions, $\co$ is an external constraint. In the
last production, we require $\ts$ to be of the form $\scheme\tvars\mc\tvar$,
and we demand $\exists\ts\logeq\ctrue$.
%
% On peut en dire plus sur la forme des schémas, mais c'est peut-être trop
% tôt ici.
%
Every stack may be viewed as a one-hole constraint context
(page~\pageref{syntax-ccon}); indeed, one may interpret $\chole$ as the empty
context and $\cdot[\cdot]$ as context composition, which replaces the hole of
its first context argument with its second context argument. A stack may also
be viewed, literally, as a list of \emph{frames}. Frames may be added and
deleted at the inner end of a stack, that is, near the hole of the constraint
context that it represents. We refer to the four kinds of frames as
\emph{conjunction}, \emph{existential}, \kwd{let}, and \emph{environment}
frames, respectively.
%
A \emph{state} of the constraint solver is a triple $\st;\mc;\co$ where $\st$
is a stack, $\mc$ is a unification constraint, and $\co$ is an external
constraint.
%
The state $\st;\mc;\co$ is to be understood as a representation of the
constraint $\st[\mc\wedge\co]$, that is, the constraint obtained by placing
both $\mc$ and $\co$ within the hole of the constraint context $\st$. The
notion of \aequiv between states is defined accordingly. In particular, one
may rename type variables in $\dtv\st$, provided $\mc$ and $\co$ are renamed
as well. In brief, the three components of a state play the following roles.
$\co$ is an external constraint that the solver intends to examine next. $\mc$
is the internal state of the underlying unification algorithm; one might think
of it as the knowledge that has been obtained so far. $\st$ tells where the
type variables that occur free in $\mc$ and $\co$ are bound, associates type
schemes with the program variables that occur free in $\co$, and records what
should be done after $\co$ is solved. The solver's initial state is usually of
the form $\chole;\ctrue;\co$, where $\co$ is the external constraint that one
wishes to solve, that is, whose satisfiability one wishes to determine. If
the constraint to be solved is of the form $\cxlet\ienv\co$, and if the type
schemes that appear within $\ienv$ meet the requirements that bear on
environment frames, as defined above, then it is possible to pick
$\cxlet\ienv\chole;\ctrue;\co$ as an initial state.
For simplicity, we make the (unessential) assumption that states have no free
type variables.

\begin{widefigure}
\TTtoprule
\vspace*{-2ex}
\newcommand{\gc}[3]{\st[\cxlet{\evid:\scheme{#1}\chole{#3}}\co];#2;\ctrue}
\def \rline{\noalign{\hrule\smallskip}}
$$
\renewcommand{\regleskip}{1.5mm}
\begin{regles}
\regle{S-Unify}
  {\st;\mc;\co}
  {\st;\mc';\co}
  {\mc\red\mc'}

\rline

\regle{S-Ex-1}
  {\st;\exists\tvars.\mc;\co}
  {\st[\exists\tvars.\chole];\mc;\co}
  {\disjoint\tvars{\ftv\co}}

\regle{S-Ex-2}
  {\st[(\exists\tvars.\st')\wedge\cp];\mc;\co}
  {\st[\exists\tvars.(\st'\wedge\cp)];\mc;\co}
  {\disjoint\tvars{\ftv\cp}}

\regle{S-Ex-3}
  {\st[\cxlet{\evid:\scheme\tvars{\exists\twars.\st'}\ttyp}\cp];\mc;\co}
  {\st[\cxlet{\evid:\scheme{\tvars\twars}{\st'}\ttyp}\cp];\mc;\co}
  {\disjoint\twars{\ftv\ttyp}}

\regle{S-Ex-4}
  {\st[\cxlet{\evid:\ts}{\exists\tvars.\st'}];\mc;\co}
  {\st[\exists\tvars.\cxlet{\evid:\ts}\st'];\mc;\co}
  {\disjoint\tvars{\ftv\ts}}

\rline

\regle{S-Solve-Eq}
  {\st;\mc;\ttyp_1=\ttyp_2}
  {\st;\mc\wedge\ttyp_1=\ttyp_2;\ctrue}
  {}

\regle{S-Solve-Id}
  {\st;\mc;\ccall\evid\ttyp}
  {\st;\mc;\ccall{\st(\evid)}\ttyp}
  {}

\regle{S-Solve-And}
  {\st;\mc;\co_1\wedge\co_2}
  {\st[\chole\wedge\co_2];\mc;\co_1}
  {}

\regle{S-Solve-Ex}
  {\st;\mc;\exists\tvars.\co}
  {\st[\exists\tvars.\chole];\mc;\co}
  {\disjoint\tvars{\ftv\mc}}

\regle{S-Solve-Let}
  {\st;\mc;\cxlet{\evid:\scheme\tvars\cp\ttyp}\co}
  {\st[\cxlet{\evid:\scheme\tvars\chole\ttyp}\co];\mc;\cp}
  {\disjoint\tvars{\ftv\mc}}

\rline

\regle{S-Pop-And}
  {\st[\chole\wedge\co];\mc;\ctrue}
  {\st;\mc;\co}
  {}

\regle{S-Name-2}
  {\gc\tvars\mc\ttyp}
  {\gc{\tvars\tvar}{\\&&&\mc\wedge\tvar=\ttyp}\tvar}
  {\tvar\not\in\ftv{\mc,\ttyp} \wedge \ttyp\not\in\tyvarset}

\regle{S-Compress}
  {\gc{\tvars\twar}{\twar=\tzar=\meq\wedge\mc}\tvar}
  {\gc{\tvars\twar}
      {\\&&&\twar\wedge\tzar=\unifier(\meq)\wedge \unifier(\mc)}
      {\unifier(\tvar)}}
  {\twar\not=\tzar \wedge \unifier=\subst\twar\tzar}
% NOTE Remarque de Alan: il n'est pas facile de prouver, à l'aide de notre axiomatization,
% que cette règle préserve l'équivalence. La preuve utilise C-LetEx, C-LetAll
% (si X = Y et Z n'est pas dans \vec X), et C-NameEq.

\regle{S-UnName}
  {\gc{\tvars\twar}{\twar=\meq\wedge\mc}\tvar}
  {\gc{\tvars}
      {\meq\wedge\mc}
      \tvar}
  {\twar\not\in\tvar\cup\ftv{\meq,\mc}}

\regle{S-LetAll}
  {\gc{\tvars\twars}\mc\tvar}
  {\st[\exists\twars.\cxlet{\evid:\scheme\tvars\chole\tvar}\co];\mc;\ctrue}
  {\disjoint\twars{\ftv\co} \wedge
   \text{$\exists\tvars.\mc$ determines $\twars$}}

\regle{S-Pop-Let}
  {\gc\tvars{\mc_1\wedge\mc_2}\tvar}
  {\st[\cxlet{\evid:\scheme\tvars{\mc_2}\tvar}\chole];\mc_1;\co}
  {\disjoint\tvars{\ftv{\mc_1}} \wedge
   \exists\tvars.\mc_2\logeq\ctrue}

\regle{S-Pop-Env}
  {\st[\cxlet{\evid:\ts}\chole];\mc;\ctrue}
  {\st;\mc;\ctrue}
  {}
\end{regles}
$$ \\
\vspace*{-2ex}
\TTbottomrule \\
\vspace*{-2ex}
\bcpcaption{solver-trs}{A constraint solver}
% FIXME diviser les règles en groupes par un espacement visuel
% TEMPORARY Vincent suggère de distinguer les side-conditions triviales
% et non-triviales
\end{widefigure}

The solver consists of a (nondeterministic) state rewriting system, given in
Figure~\ref{fig:solver-trs}. Rewriting is performed modulo \aconv.
\Rule{S-Unify} makes the unification algorithm a component of the constraint
solver, and allows the current unification problem $\mc$ to be solved at any
time. Rules \Rule{S-Ex-1} to \Rule{S-Ex-4} float existential quantifiers out
of the unification problem into the stack and through the stack up to the
nearest enclosing \kwd{let} frame, if there is any, or to the outermost level,
otherwise. Their side-conditions prevent capture of type variables, and can
always be satisfied by suitable \aconv of the left-hand state. If
$\st;\mc;\co$ is a normal form with respect to these five rules, then $\mc$
must be either $\cfalse$ or a conjunction of standard multi-equations, and
every type variable in $\dtv\st$ must be either universally quantified at a
\kwd{let} frame or existentially bound at the outermost level. (Recall that,
by assumption, states have no free type variables.) In other words, provided
these rules are applied in an eager fashion, \emphfull{there is no need for
existential frames to appear in the machine representation of
stacks}. \label{page-ranks}Instead, it suffices to maintain, at every \kwd{let} frame and at the
outermost level, a list of the type variables that are bound at this point
and, conversely, to annotate every type variable in $\dtv\st$ with an integer
\emph{rank}, which allows telling, in constant time, where the variable is
bound: type variables of rank 0 are bound at the outermost level, and type
variables of rank $k\geq 1$ are bound at the \kth{k} \kwd{let} frame down in
the stack $\st$. The code that accompanies \this adopts this convention. Ranks
were initially described in \longcite{Remy!mleth} and have also been
studied by  \longcite{mcallester-03}.

\shortpage

Rules \Rule{S-Solve-Eq} to \Rule{S-Solve-Let} encode an analysis of the
structure of the third component of the current state. There is one rule for
each possible case, except $\cfalse$, which by assumption cannot arise, and
$\ctrue$, which is dealt with further on. \Rule{S-Solve-Eq} discovers an
equation and makes it available to the unification algorithm.
\Rule{S-Solve-Id} discovers an instantiation constraint $\ccall\evid\ttyp$ and
replaces it with $\ccall\ts\ttyp$, where the type scheme $\ts=\st(\evid)$ is
the type scheme carried by the nearest environment frame that defines $\evid$
in the stack $\st$. It is defined as follows:
%
$$\begin{array}{rcll}
\st[\chole\wedge\co] (\evid) & = & \st(\evid) \\
\st[\exists\tvars.\chole] (\evid) & = & \st(\evid) &
\text{if $\disjoint\tvars{\ftv{\st(\evid)}}$} \\
\st[\cxlet{\ewid:\scheme\tvars\chole\ttyp}\co] (\evid) & = & \st(\evid) &
\text{if $\disjoint\tvars{\ftv{\st(\evid)}}$} \\
\st[\cxlet{\ewid:\ts}\chole] (\evid) & = & \st(\evid) &
\text{if $\evid\not=\ewid$} \\
\st[\cxlet{\evid:\ts}\chole] (\evid) & = & \ts
\end{array}$$
%
If $\evid\in\dpv\st$ does not hold, then $\st(\evid)$ is undefined and the
rule is not applicable. If it does hold, then the rule may always be made
applicable by suitable \aconv of the left-hand state. Recall that, if
$\ts$ is of the form $\scheme\tvars\mc\tvar$, where
$\disjoint\tvars{\ftv\ttyp}$, then $\ccall\ts\ttyp$ stands for
$\exists\tvars.(\mc\wedge\tvar=\ttyp)$. The process of constructing this
constraint is informally referred to as ``taking an instance of $\ts$.'' In
the worst case, it is just as inefficient as textually expanding the
corresponding \kwlet construct in the program's source code, and leads to
exponential time complexity. In
practice, however, the unification constraint $\mc$ is often compact because
it was simplified before the environment frame $\cxlet{\evid:\ts}\chole$ was
created, which explains why the solver usually performs well. (The creation of
environment frames, performed by \Rule{S-Pop-Let}, is discussed below.)
\Rule{S-Solve-And} discovers a conjunction. It arbitrarily chooses to explore
the left branch first, and pushes a conjunction frame onto the stack, so as to
record that the right branch should be explored afterwards. \Rule{S-Solve-Ex}
discovers an existential quantifier and enters it, creating a new existential
frame to record its existence. Similarly, \Rule{S-Solve-Let} discovers a
\kwd{let} form and enters its left-hand side, creating a new \kwd{let} frame
to record its existence. The choice of examining the left-hand side first is
{not} arbitrary. Indeed, examining the right-hand side first would
require creating an environment frame---but environment frames must contain
\emphfull{simplified} type schemes of the form $\scheme\tvars\mc\tvar$, whereas
the type scheme $\scheme\tvars\cp\ttyp$ is arbitrary. In other words, our
strategy is to simplify type schemes prior to allowing them to be copied by
\Rule{S-Solve-Id}, so as to avoid any duplication of effort. The
side-conditions of \Rule{S-Solve-Ex} and \Rule{S-Solve-Let} may always be
satisfied by suitable \aconv of the left-hand state.

Rules \Rule{S-Solve-Eq} to \Rule{S-Solve-Let} may be referred to as
\emph{forward} rules, because they ``move down into'' the external constraint,
causing the stack to grow. This process stops when the external constraint at
hand becomes $\ctrue$. Then part of the work has been finished, and the
solver must examine the stack in order to determine what to do next. This task
is performed by the last series of rules, which may be referred to as
\emph{backward} rules, because they ``move back out,'' causing the stack to
shrink and possibly scheduling new external constraints for examination.
These rules encode an analysis of the structure of the innermost stack frame.
There are three cases, corresponding to conjunction, \kwd{let}, and
environment frames. The case of existential stack frames need not be
considered, because rules \Rule{S-Ex-2} to \Rule{S-Ex-4} allow either fusing
them with \kwd{let} frames or floating them up to the outermost level, where
they shall remain inert. \Rule{S-Pop-And} deals with conjunction frames. The
frame is popped, and the external constraint that it carries is scheduled for
examination. \Rule{S-Pop-Env} deals with environment frames. Because the
right-hand side of the \kwd{let} construct at hand has been solved---that is,
turned into a unification constraint $\mc$---it cannot contain an occurrence
of $\evid$. Furthermore, by assumption, $\exists\ts$ is $\ctrue$. Thus, this
environment frame is no longer useful: it is destroyed. The remaining rules
deal with \kwd{let} frames. Roughly speaking, their purpose is to change the
state $\st[\cxlet{\evid:\scheme\tvars\chole\ttyp}\co];\mc;\ctrue$ into
$\st[\cxlet{\evid:\scheme\tvars\mc\ttyp}\chole];\ctrue;\co$, that is, to turn
the current unification constraint $\mc$ into a type scheme, turn the
\kwd{let} frame into an environment frame, and schedule the right-hand side of
the \kwd{let} construct (that is, the external constraint $\co$) for
examination. In fact, the process is more complex, because the type scheme
$\scheme\tvars\mc\ttyp$ must be \emph{simplified} before becoming part of an
environment frame. The simplification process is described by rules
\Rule{S-Name-2} to \Rule{S-Pop-Let}. In the following, we refer to type
variables in $\tvars$ as \emph{young} and to type variables in
$\dtv\st\setminus\tvars$ as \emph{old}. The former are the universal
quantifiers of the type scheme that is being created; the latter contain its free
type variables.

\Rule{S-Name-2} ensures that the body $\ttyp$ of the type scheme that is being
created is a type variable, as opposed to an arbitrary term. If it isn't, then
it is replaced with a fresh variable $\tvar$, and the equation $\tvar=\ttyp$
is added so as to recall that $\tvar$ stands for $\ttyp$. Thus, the rule moves
the term $\ttyp$ into the current unification problem, where it potentially
becomes subject to \Rule{S-Name-1}. This ensures that sharing is made explicit
everywhere. \Rule{S-Compress} determines that the (young) type variable
$\twar$ is an alias for the type variable $\tzar$. Then, every free occurrence
of $\twar$ other than its defining occurrence is replaced with~$\tzar$. In an
actual implementation, this occurs transparently when the union-find algorithm
performs \emph{path compression} \cite{tarjan-75,tarjan-79}. We note that the
rule does not allow substituting a younger type variable for an older one;
indeed, that would make no sense, since the younger variable could then
possibly escape its scope. In other words, in implementation terms, the
union-find algorithm must be slightly modified so that, in each equivalence
class, the representative element is always a type variable with minimum
rank. \Rule{S-UnName} determines that the (young) type variable $\twar$ has no
occurrences other than its defining occurrence in the current type
scheme. (This occurs, in particular, when \Rule{S-Compress} has just been
applied.) Then, $\twar$ is suppressed altogether. In the particular case where
the remaining multi-equation $\meq$ has cardinal 1, it may then be suppressed
by \Rule{S-Single}. In other words, the combination of \Rule{S-UnName} and
\Rule{S-Single} is able to suppress young unused type variables as well as the
term that they stand for. This may, in turn, cause new type variables to
become eligible for elimination by \Rule{S-UnName}. In fact, assuming the
current unification constraint is acyclic, an inductive argument shows that
every young type variable may be suppressed unless it is dominated either by
$\tvar$ or by an old type variable. (In the setting of a regular tree model,
it is possible to extend the rule so that young cycles that are not dominated
either by $\tvar$ or by an old type variable are suppressed as well.)
\Rule{S-LetAll} is a directed version of \Rule{C-LetAll}. It turns the young
type variables $\twars$ into old variables. How to tell whether
$\exists\tvars.\mc$ determines $\twars$ is discussed later (see
Lemma~\ref{lemma-determines}). Why \Rule{S-LetAll} is an interesting and
important rule will be explained shortly. \Rule{S-Pop-Let} is meant to be
applied when the current state has become a normal form with respect to
\Rule{S-Unify}, \Rule{S-Name-2}, \Rule{S-Compress}, \Rule{S-UnName}, and
\Rule{S-LetAll}, that is, when the type scheme that is about to be created is
fully simplified. It splits the current unification constraint into two
components $\mc_1$ and $\mc_2$, where $\mc_1$ is made up entirely of
\emph{old} variables, as expressed by the side-condition
$\disjoint\tvars{\ftv{\mc_1}}$, and $\mc_2$ constrains \emph{young} variables
only, as expressed by the side-condition
$\exists\tvars.\mc_2\logeq\ctrue$. Note that $\mc_2$ may still contain
free occurrences of old type variables, so the type scheme
$\scheme\tvars{\mc_2}\tvar$ that appears on the right-hand side is not
necessarily closed. It is not obvious why such a decomposition must exist;
Lemma~\ref{lemma-solver-trs-normal-forms} proves that it does. Let
us say for now that \Rule{S-LetAll} plays a role in guaranteeing its
existence, whence comes part of its importance. Once the decomposition
$\mc_1\wedge\mc_2$ is obtained, the behavior of \Rule{S-Pop-Let} is simple.
The unification constraint $\mc_1$ concerns old variables only, that is,
variables that are not quantified in the current \kwd{let} frame; thus, it
need not become part of the new type scheme and may instead remain part of
the current unification constraint. This is justified by \Rule{C-LetAnd} and
\Rule{C-InAnd*}\iffull { (see the proof of
Lemma~\ref{lemma-solver-trs-correct})} and corresponds to the difference
between \Rule{hmx-Gen'} and \Rule{hmx-Gen} discussed in
\S\ref{section-hmx}. The unification constraint $\mc_2$, on the other hand,
becomes part of the newly built type scheme $\scheme\tvars{\mc_2}\tvar$. The
property $\exists\tvars.\mc_2\logeq\ctrue$ guarantees that the newly created
environment frame meets the requirements imposed on such frames. Note
that the more type variables are considered old, the larger $\mc_1$ may
become, and the smaller $\mc_2$. This is another reason why \Rule{S-LetAll} is
interesting: by allowing more variables to be considered old, it decreases the
size of the type scheme $\scheme\tvars{\mc_2}\tvar$, making it cheaper to
instantiate.

To complete our description of the constraint solver, there remains to explain
how to decide when $\exists\tvars.\mc$ determines $\twars$, since this
predicate occurs in the side-condition of \Rule{S-LetAll}. The following lemma
describes two important situations where, by examining the structure of an
equation, it is possible to discover that a constraint $\co$ \emphfull{determines}
some of its free type variables $\twars$ (Definition~\ref{def-determines}). In
the first situation, the type variables $\twars$ are \emphfull{equated} with or
\emphfull{dominated} by a distinct type variable $\tvar$ that occurs \emphfull{free}
in $\co$. In that case, because the model is a free tree model, the values of
the type variables $\twars$ are determined by the value of $\tvar$: they are
subtrees of it at specific positions. For instance, $\tvar=\twar_1\arw\twar_2$
determines $\twar_1\twar_2$, while $\exists\twar_1.(\tvar=\twar_1\arw\twar_2)$
determines $\twar_2$. In the second situation, the type variables $\twars$ are
equated with a term $\ttyp$, \emphfull{all} of whose type variables are
\emphfull{free} in $\co$. Again, the value of the type variables $\twars$ is then
determined by the values of the type variables $\ftv\ttyp$. For instance,
$\tvar=\twar_1\arw\twar_2$ determines $\tvar$, while
$\exists\twar_1.(\tvar=\twar_1\arw\twar_2)$ does not. In the second situation,
no assumption is in fact made about the model. (Note that
$\tvar=\twar_1\arw\twar_2$ determines $\twar_1\twar_2$ and determines $\tvar$,
but does \emphfull{not} simultaneously determine $\tvar\twar_1\twar_2$.)
%
% TEMPORARY Alan: il faut clarifier ces explications et ce lemme.
%
\begin{lemma}
\label{lemma-determines}
Let $\disjoint\tvars\twars$. Assume
either $\meq$ is $\tvar=\meq'$, where $\tvar\not\in\tvars\twars$
      and $\twars\subseteq\ftv{\meq'}$,
or $\meq$ is $\twars=\ttyp=\meq'$, where
      $\disjoint{\ftv\ttyp}{\tvars\twars}$.
Then, $\exists\tvars.(\co\wedge\meq)$ determines $\twars$.
\end{lemma}
%
\begin{Proof}
\demoreset Let $\disjoint\tvars\twars$~\dlabel{disj}. Let
$\satdef\ga\env{\exists\tvars.(\co\wedge\meq)}$~\dlabel{phi} and
$\satdef{\ga'}\env{\exists\tvars.(\co\wedge\meq)}$~\dlabel{phip}, where $\ga$
and $\ga'$ coincide outside of $\twars$. We may assume, \spdg,
$\disjoint\tvars{\ftv\env}$~\dlabel{dj}. By \dref{phi}, \dref{dj},
\Rule{CM-Exists}, and \Rule{CM-And}, we obtain
$\satdef{\ga_1}\env\meq$~\dlabel{phi1}, where $\ga$ and $\ga_1$ coincide
outside $\tvars$. By \Rule{CM-Predicate}, \dref{phi1} implies that all members
of $\meq$ have the same image through $\ga_1$. Similarly, exploiting
\dref{phip} and \dref{dj}, we find that all members of $\meq$ have the same
image through $\ga'_1$, where $\ga'$ and $\ga'_1$ coincide outside $\tvars$.
Now, we claim that $\ga_1$ and $\ga'_1$ coincide on $\twars$. Once the
claim is established, by \dref{disj}, there follows that $\ga$ and $\ga'$ must
coincide on $\twars$ as well, which is the goal. So, there only remains to
establish the claim; we distinguish two subcases.

\proofsubcase $\meq$ is $\tvar=\meq'$ and
$\tvar\not\in\tvars\twars$~\dlabel{x} and
$\twars\subseteq\ftv{\meq'}$~\dlabel{free}. Because $\ga_1$ and $\ga'_1$
coincide outside $\tvars\twars$ and by \dref{x}, we have
$\ga_1(\tvar)=\ga'_1(\tvar)$.  As a result, all members of $\meq'$ have the
same image through $\ga_1$ and $\ga'_1$. In a free tree model, where
decomposition is valid, a simple inductive argument shows that
$\ga_1$ and $\ga'_1$ must coincide on $\ftv{\meq'}$, hence---by
\dref{free}---also on $\twars$.

\proofsubcase $\meq$ is $\twars=\ttyp=\meq'$ and
$\disjoint{\ftv\ttyp}{\tvars\twars}$~\dlabel{t}.  Because $\ga_1$ and $\ga'_1$
coincide outside $\tvars\twars$ and by \dref{t}, we have
$\ga_1(\ttyp)=\ga'_1(\ttyp)$. Thus, for every $\twar\in\twars$, we have
$\ga_1(\twar)=\ga_1(\ttyp)=\ga'_1(\ttyp)=\ga'_1(\twar)$. That is, $\ga_1$ and
$\ga'_1$ coincide on $\twars$.
\end{Proof}

Thanks to Lemma~\ref{lemma-determines}, an efficient implementation of
\Rule{S-LetAll} comes to mind. The problem is, given a constraint
$\exists\tvars.\mc$, where $\mc$ is a standard conjunction of multi-equations,
to determine the greatest subset $\twars$ of $\tvars$ such that
$\exists(\tvars\setminus\twars).\mc$ determines $\twars$. By the first part of
the lemma, it is safe for $\twars$ to include all members of $\tvars$ that are
directly or indirectly dominated (with respect to $\mc$) by some free variable
of $\exists\tvars.\mc$. Those can be found, in time linear in the size of
$\mc$, by a top-down traversal of the graph of $\isdominated\mc$. By the
second part of the lemma, it is safe to close $\twars$ under the closure law
$\tvar\in\tvars \wedge
(\forall\twar\quad\twar\isdominated\mc\tvar\Rightarrow\twar\in\twars)
\Rightarrow \tvar\in\twars$. That is, it is safe to also include all members
of $\tvars$ whose descendants (with respect to $\mc$) have already been found
to be members of $\twars$. This closure computation may be performed, again in
linear time, by a bottom-up traversal of the graph of $\isdominated\mc$. When
$\mc$ is acyclic, it is possible to show that this procedure is complete, that
is, does compute the greatest subset $\twars$ that meets our requirement.
\begin{full}
This is the topic of the following exercise.
%
\begin{exercise}[\Moderate, \nosolution]
\label{exercise-det-proc}
Assuming $\mc$ is acyclic, prove that the above procedure computes the
greatest subset $\twars$ of $\tvars$ such that
$\exists(\tvars\setminus\twars).\mc$ determines $\twars$. In the setting of a
regular tree model, exhibit a satisfiable constraint $\mc$ such that the above
procedure is incomplete. Can you define a complete procedure in that setting?
\end{exercise}
% TEMPORARY est-on sûr que l'énoncé est correct?
\end{full}

The above discussion has shown that when $\twar$ and $\tzar$ are
equated, if $\twar$ is young and $\tzar$ is old, then \Rule{S-LetAll}
allows making $\twar$ old as well. If binding information is encoded
in terms of integer ranks, as suggested earlier, then this remark may
be formulated as follows: when $\twar$ and $\tzar$ are equated, if the
rank of $\twar$ exceeds that of $\tzar$, then it may be decreased so
that both ranks match. As a result, it is possible to attach ranks
with \emphfull{multi-equations}, rather than with variables. When two
multi-equations are fused, the smaller rank is kept. This treatment of
ranks is inspired by \longcite{Remy!mleth}; see the resolution rule
\textsc{Fuse}, as well as the simplification rules \textsc{Propagate}
and \textsc{Realize}, in that paper.

\begin{full}
\Rule{S-Solve-Let} and \Rule{S-Name-2} to \Rule{S-Pop-Let} are unnecessarily
complex when $\evid$ is assigned a \emph{monotype} $\ttyp$, rather than an
arbitrary type scheme $\scheme\tvars\cp\ttyp$. In that case, the combined
effect of these rules may be obtained directly via the following two new
rules, which may be implemented in a more efficient way:
%
$$\begin{regles}
\regle{S-Name-2-Mono}
  {\st;\mc;\cxlet{\evid:\ttyp}\co}
  {\st[\exists\tvar.\chole];\mc\wedge\tvar=\ttyp;\cxlet{\evid:\tvar}\co
   \span \\&&&}
  {\tvar\not\in\ftv{\mc,\ttyp,\co} \wedge \ttyp\not\in\tyvarset}

\regle{S-Solve-Let-Mono}
  {\st;\mc;\cxlet{\evid:\tvar}\co}
  {\st[\cxlet{\evid:\tvar}\chole];\mc;\co}
  {}
\end{regles}$$
%
If $\ttyp$ isn't a variable, it is replaced with a fresh variable $\tvar$,
together with the equation $\tvar=\ttyp$. This corresponds to the effect of
\Rule{S-Name-2}. Then, we directly create an environment frame for $\evid$,
without bothering to create and discard a \kwlet frame, since there is no way
the type scheme $\tvar$ may be further simplified.
\end{full}

Let us now state\iffull{ and establish} the properties of the constraint
solver. First, the reduction system is terminating, so it defines an
algorithm. 
%
\begin{lemma}
\label{lemma-solver-trs-normalizes}
The reduction system $\red$ is strongly normalizing.
\end{lemma}
%
% TEMPORARY des volontaires pour la preuve? moi ça ne me botte pas vraiment...
% si possible elle devrait être abstraite vis-à-vis de la preuve de terminaison
% de l'algo d'unification.

Second, every rewriting step preserves the meaning of the constraint that the
current state represents. We recall that the state $\st;\mc;\co$ is meant to
represent the constraint $\st[\mc\wedge\co]$.
%
\begin{lemma}
\label{lemma-solver-trs-correct}
$\st;\mc;\co \red \st';\mc';\co'$ implies
$\st[\mc\wedge\co] \logeq \st'[\mc'\wedge\co']$.
\end{lemma}
%
\begin{Proof}
By examination of every rule.

\proofcase\Rule{S-Unify}. By Lemma~\ref{lemma-unif-trs-correct}.

\proofcase\Rule{S-Ex-1}, \Rule{S-Ex-2}, \Rule{S-Solve-Ex}. By \Rule{C-ExAnd}.

\proofcase\Rule{S-Ex-3}. By \Rule{C-LetEx}.

\proofcase\Rule{S-Ex-4}. By \Rule{C-InEx}.

\proofcase\Rule{S-Solve-Eq}, \Rule{S-Pop-And}. By \Rule{C-Dup}.

\proofcase\Rule{S-Solve-Id}. Because $\st(\evid)$ is of the form
$\scheme\tvars\mc\tvar$, we have $\fpv{\st(\evid)}=\varnothing$. The result follows by
\Rule{C-InId}.

\proofcase\Rule{S-Solve-And}. By \Rule{C-AndAnd}.

\proofcase\Rule{S-Solve-Let}. By \Rule{C-LetAnd}.

\proofcase\Rule{S-Name-2}. By Definition~\ref{def-csubsume} and
\Rule{C-NameEq}, $\tvar\not\in\ftv{\mc,\ttyp}$ implies
$\csubsumeq\ctrue{\scheme\tvars\mc\ttyp}
{\scheme{\tvars\tvar}{\mc\wedge\tvar=\ttyp}\tvar}$. The result follows by
Lemma~\ref{lemma-subsume-env}.

\proofcase\Rule{S-Compress}. Let $\unifier=\subst\twar\tzar$. By
Definition~\ref{def-csubsume} and \Rule{C-NameEq}, $\twar\not=\tzar$ implies
$\csubsumeq\ctrue{\scheme{\tvars\twar}{\twar=\tzar=\meq\wedge\mc}\tvar}
{\scheme{\tvars\twar}{\twar\wedge\tzar=\unifier(\meq)\wedge\unifier(\mc)}
{\unifier(\tvar)}}$.
The result follows by Lemma~\ref{lemma-subsume-env}.

\proofcase\Rule{S-UnName}. Using Lemma~\ref{lemma-collect}, it is
straightforward to check that $\twar\not\in\ftv\meq$ implies
$\exists\twar.(\twar=\meq) \logeq \meq$. The result follows by
\Rule{C-ExAnd} and \Rule{C-LetEx}.

\proofcase\Rule{S-LetAll}. By \Rule{C-LetAll}.

\proofcase\Rule{S-Pop-Let}. By \Rule{C-LetAnd} and \Rule{C-InAnd*}.

\proofcase\Rule{S-Pop-Env}. By \Rule{C-In*}, recalling that $\exists\ts$
must be $\ctrue$.
\end{Proof}

Last, we classify the normal forms of the reduction system:
%
\begin{lemma}
\label{lemma-solver-trs-normal-forms}
A normal form for the reduction system $\red$ is one of
(i) $\st;\mc;\ccall\evid\ttyp$, where $\evid\not\in\dpv\st$;
(ii) $\st;\cfalse;\ctrue$; or
(iii) $\xc;\mc;\ctrue$, where $\xc$ is an existential constraint context
      and $\mc$ a satisfiable conjunction of multi-equations.
\end{lemma}
%
\begin{Proof}
Because, by definition, $\st;\mc;\cfalse$ is not a valid state, a normal form
for \Rule{S-Solve-Eq}, \Rule{S-Solve-Id}, \Rule{S-Solve-And},
\Rule{S-Solve-Ex}, and \Rule{S-Solve-Let} must be either an instance of the
left-hand side of \Rule{S-Solve-Id}, with  $\evid\not\in\dpv\st$, which
corresponds to case (i), or of the form $\st;\mc;\ctrue$. Let us consider the
latter case. Because $\st;\mc;\ctrue$ is a normal form with respect to
\Rule{S-Unify}, by Lemma~\ref{lemma-unif-trs-normal-forms}, $\mc$ must be
either $\cfalse$ of the form $\xc[\mc']$, where $\mc'$ is a standard
conjunction of multi-equations and, if the model is syntactic, $\mc'$ is
acyclic. The former case corresponds to (ii); thus, let us consider the latter
case. Because $\st;\xc[\mc'];\ctrue$ is a normal form with respect to
\Rule{S-Ex-1}, the context $\xc$ must in fact be empty, and $\mc'$ is $\mc$.
If $\st$ is an existential constraint context, then we are in situation (iii).
Otherwise, because $\st;\mc;\ctrue$ is a normal form with respect to
\Rule{S-Ex-2}, \Rule{S-Ex-3}, and \Rule{S-Ex-4}, the stack $\st$ does not end
with an existential frame. Because $\st;\mc;\ctrue$ is a normal form with
respect to \Rule{S-Pop-And} and \Rule{S-Pop-Env}, $\st$ must then be of the
form $\st'[\cxlet{\evid:\scheme\tvars\chole\ttyp}\co]$. Because
$\st;\mc;\ctrue$ is a normal form with respect to \Rule{S-Name-2}, $\ttyp$
must be a type variable $\tvar$. Let us write $\mc$ as $\mc_1\wedge\mc_2$,
where $\disjoint\tvars{\ftv{\mc_1}}$, and where $\mc_1$ is maximal for this
criterion. Then, consider a multi-equation $\meq\in\mc$. By the first part of
Lemma~\ref{lemma-determines}, if one variable member of $\meq$ is old (that
is, outside $\tvars$), then $\exists\tvars.\mc$ determines all other variables
in $\ftv\meq$. Because $\st;\mc;\ctrue$ is a normal form with respect to
\Rule{S-LetAll}, all variables in $\ftv\meq$ must then be old (that is,
outside $\tvars$). By definition of $\mc_1$, this implies $\meq\in\mc_1$. By
contraposition, for every multi-equation $\meq\in\mc_2$, \emph{all variable
  members of $\meq$ are in $\tvars$}. Furthermore, let us recall that $\mc_2$
is a standard conjunction of multi-equations and, if the model is syntactic,
$\mc_2$ is acyclic. We let the reader establish that this implies
$\exists\tvars.\mc_2\logeq\ctrue$; the proof is a slight generalization of
the last part of that of Lemma~\ref{lemma-unif-trs-normal-forms}. Then,
$\st;\mc;\ctrue$ is reducible via \Rule{S-Pop-Let}. This is a contradiction,
so this last case cannot arise.
\end{Proof}

In case (i), the constraint $\st[\mc\wedge\co]$ has a free program identifier
$\evid$. In other words, the source program contains an unbound program
identifier. Such an error could of course be detected prior to constraint
solving, if desired. In case (ii), the unification algorithm failed. By
Lemma~\ref{lemma-false}, the constraint $\st[\mc\wedge\co]$ is then false. In
case (iii), the constraint $\st[\mc\wedge\co]$ is equivalent to $\xc[\mc]$,
where $\mc$ is satisfiable, so it is satisfiable as well. If the initial
constraint is closed, case (i) cannot arise, while cases (ii) and (iii)
respectively denote failure and success. Thus,
Lemmas~\ref{lemma-solver-trs-correct} and~\ref{lemma-solver-trs-normal-forms}
indeed prove that the algorithm is a constraint solver.

\begin{remark}
Type inference for \MLcalc is \textsc{dexptime}-complete
\cite{Kfoury90ml,mairson-kanellakis-mitchell-91}. Thus, our constraint
solver cannot run  any faster, asymptotically. This cost is essentially due to
\letpoly, which requires a constraint to be duplicated at every
occurrence of a \kwtt{let}-bound variable (\Rule {S-Solve-Id}). In
order to limit the amount of duplication to a bare minimum, it is
important that rule \Rule {S-LetAll} be applied before \Rule
{S-Pop-Let}, allowing variables and constraints that need not be
duplicated to be shared. We have observed that algorithms based on
this strategy behave remarkably well in practice \cite{Remy!mleth}. In
fact, \longcite{mcallester-03} has proved that they have
linear time complexity, provided the size of type schemes and the
(left-) nesting depth of \kwtt{let} constructs are
bounded. Unfortunately, many implementations of type inference for
\MLlang do not behave as efficiently as the algorithm presented
here. Some spend an excessive amount of time in computing the set of
nongeneralizable type variables; some do not treat types as dags, thus
losing precious sharing information; others perform the expensive
occurs check after every unification step, rather than only once at
every \kwtt{let} construct, as suggested here (\Rule{S-Pop-Let}).
\end{remark}

% --------------------------------------------------------------------------

\begin{full}
\subsection*{An Alternate Constraint Solver}

\newcommand{\mono}{\emph{mono}\xspace}
\newcommand{\poly}{\emph{poly}\xspace}

The constraint solver presented above deals with instantiation constraints in
an \emph{eager} way: as soon as one is found, it is rewritten by
\Rule{S-Solve-Id} into a unification constraint. In other words, instantiation
constraints may appear only within \emph{external} constraints; they are not
part of the solver's internal data structures. However, it is possible to
imagine other constraint solving strategies, where instantiation constraints
are longer-lived and may become part of the solver's internal state. Here, we
briefly (and somewhat informally) present one such strategy. It is perhaps
slightly less efficient, but offers some interesting theoretical properties.
In particular, under this alternate strategy, the type schemes found inside
environment frames have \emph{no free type variables}, but may have free
program variables. In the following, we say that $\ts$ is \emph{closed} when
$\ftv\ts$ is empty. Let us recall that the constraint solver presented above
has the converse property: environment frames contain type schemes of the form
$\scheme\tvars\mc\tvar$, which have no free program variables, but may have
free type variables.

The new strategy exploits the following observation about the structure of the
constraints produced by the generation rules: in every constraint of the form
$\cxlet{\evid:\ts}\co$, either $\ts$ is a monotype $\ttyp$---this occurs when
$\evid$ was $\lambda$-bound in the source program---or $\ts$ is closed---this
occurs when $\evid$ is a constant or was \kwlet-bound in the source program.
Indeed, the type scheme assigned to a \kwlet-bound identifier is of the form
$\scheme\tvar{\calcule\et\tvar}\tvar$.  Because $\ftv{\calcule\et\tvar}$ is
$\tvar$, this type scheme is closed. Our observation breaks down when terms
are allowed to contain type annotations (\S\ref{section-calc-lang}); it
seems possible to come up with a workaround, which we do not describe here.
In the following, we assume that we are able to distinguish between
$\lambda$-bound program identifiers, on the one hand, and constants
and \kwlet-bound program identifiers, on the other hand; we refer to them
as \mono and \poly identifiers, respectively.

The idea behind the alternate strategy is to preserve the property that \poly
identifiers have closed type schemes throughout the constraint solving
process. More precisely, the new strategy maintains the invariant that all
\poly stack frames are closed. That is, if $\evid$ is \poly, then in every
state of the form $\st_1[\cxlet{\evid:\ts}{\st_2}];\mc;\co$, we have
$\ftv\ts=\varnothing$, and in every state of the form
$\st_1[\cxlet{\evid:\scheme\tvars{\st_2}\ttyp}{\co_1}];\mc;\co_2$, we have
$\ftv{\st_2[\mc\wedge\co_2],\ttyp}\subseteq\tvars$.  We now review how this
invariant is violated by the previous constraint solver, and explain how the
alternate solver addresses each of these violations.

One important culprit is \Rule{S-Solve-Id}, which replaces a program
identifier $\evid$ with a type scheme $\st(\evid)$. If $\st(\evid)$ has free
type variables, then any \kwlet frame comprised between the environment frame
that defines $\evid$ and the inner end of the stack acquires free type
variables through the rewriting step. If $\st(\evid)$ is closed, however, the
invariant is preserved. Thus, in the alternate constraint solver,
\Rule{S-Solve-Id} is replaced with the following two rules:
%
$$\begin{regles}
\regle{AS-Solve-Id-Poly}
  {\st;\mc;\ccall\evid\ttyp}
  {\st;\mc;\ccall{\st(\evid)}\ttyp}
  {\text{$\evid$ is \poly}}

\regle{AS-Solve-Id-Mono}
  {\st;\mc;\ccall\evid\ttyp}
  {\st;\mc\wedge\ccall\evid\ttyp;\ctrue}
  {\text{$\evid$ is \mono}}
\end{regles}$$
%
\Rule{AS-Solve-Id-Poly} is identical to \Rule{S-Solve-Id}, but is restricted
to \poly identifiers. In that case, according to the invariant, the type
scheme $\st(\evid)$ is closed, so the invariant is preserved. The definition
of $\st(\evid)$ must be slightly modified to prevent the type scheme's free
program variables from capture---an issue that did not arise in the previous
solver. In \Rule{AS-Solve-Id-Mono}, we cannot replace $\evid$ with the
monotype $\st(\evid)$, since that might break the invariant; so, the
instantiation constraint is kept in symbolic form. For the rule to be
well-formed, we must extend the syntax of unification constraints:
%
$$\mc ::= \ctrue \mid \cfalse \mid \meq \mid \mc\wedge\mc \mid
          \exists\tvars.\mc \mid \ccall\evid\ttyp$$
%
We should really introduce a letter other than $\mc$ to range over this new
syntactic class. However, this abuse of notation allows us to keep many of
the previous constraint solving rules unchanged. Thus, in addition to a
system of equations, the constraint solver now maintains a system of
unresolved instantiation constraints. To ensure that each of these bears on
a distinct program variable, it is possible to introduce the following rule:
%
$$\begin{regles}
\regle{AS-Fuse}
  {\st;\mc\wedge\ccall\evid{\ttyp_1}\wedge\ccall\evid{\ttyp_2};\co}
  {\st;\mc\wedge\ccall\evid{\ttyp_1}\wedge\ttyp_1=\ttyp_2;\co}
  {}
\end{regles}$$
%
Its logical validity is guaranteed by the fact that $\evid$ must be \mono, so
$\st(\evid)$ must be a monotype, all of whose instances are equal.

Another reason why the previous constraint solver violates the invariant is
\Rule{S-Solve-Let}. The \kwlet frame that it creates is in general not
closed, because the current unification constraint $\mc$ concerns type
variables that are not bound in it. The alternate constraint solver
works around this issue by \emph{not} pushing $\mc$ inside the new
\kwlet frame when $\evid$ is \poly:
%
$$\begin{regles}
\regle{AS-Solve-Let}
  {\st;\mc;\cxlet{\evid:\scheme\tvars\cp\ttyp}\co}
  {\st[\mc\wedge\cxlet{\evid:\scheme\tvars\chole\ttyp}\co];\ctrue;\cp}
  {\text{$\evid$ is \poly}}
\end{regles}$$
%
The current unification constraint $\mc$ is left behind as part of the new
stack frame, and we start afresh with an empty unification constraint. There
would be no point in keeping $\mc$ at hand, anyway. Indeed, the type scheme
$\scheme\tvars\cp\ttyp$ is closed and will remain so throughout reduction, so
the information contained in $\mc$ cannot affect its reduction in any way.
For \Rule{AS-Solve-Let} to be well-formed, we must extend the syntax of
stacks, so as to be able to store $\mc$ as part of a \kwlet frame; we skip the
details and (again) continue to let $\st$ range over stacks. When $\evid$ is
\mono, the alternate solver uses \Rule{S-Name-2-Mono} and
\Rule{S-Solve-Let-Mono}, as before.
% TEMPORARY mentionner que S-Compress et S-UnName sont conservées

The last reason why the previous constraint solver violates the invariant is
\Rule{S-LetAll}. In the alternate solver, this rule is simply removed. The
last two rules, namely \Rule{S-Pop-Let} and \Rule{S-Pop-Env}, are replaced
with the following three:
%
$$\begin{regles}
\regle{AS-Pop-Let}
  {\st[\mc_1\wedge\cxlet{\evid:\scheme\tvars\chole\tvar}\co];\mc_2;\ctrue}
  {\st[\cxlet{\evid:\scheme\tvars{\mc_2}\tvar}\chole];\mc_1;\co}
  {\evid\not\in\ftv{\mc_1}}

\regle{AS-Pop-Env-Poly}
  {\st[\cxlet{\evid:\scheme\tvars{\mc_1}\tvar}\chole];\mc_2;\ctrue}
  {\st;\exists\tvars.\mc_1\wedge\mc_2;\ctrue}
  {\text{$\evid$ is \poly}}

\regle{AS-Pop-Env-Mono}
  {\st[\cxlet{\evid:\tvar}\chole];\mc;\ctrue}
  {\st;\subst\evid\tvar\mc;\ctrue}
  {\text{$\evid$ is \mono}}
\end{regles}$$
%
\Rule{AS-Pop-Let} simply creates the type scheme $\scheme\tvars{\mc_2}\tvar$,
which, by the invariant, must be closed. Thus, the invariant is preserved.
The unification constraint $\mc_1$, which was put away by \Rule{AS-Solve-Let}
when this \kwlet frame was created, becomes current again.
\Rule{AS-Pop-Env-Poly} discards a \poly environment frame. The constraint
$\exists\tvars.\mc_1$ is not, in general, equivalent to $\ctrue$, so we must
keep track of it in order to ensure the rule's logical
validity. \Rule{AS-Pop-Env-Mono} discards a \mono environment frame. If
unresolved instantiation constraints bearing on $\evid$ remain, they are
turned into subtyping constraints bearing on $\tvar$; then, the frame is
discarded.
%
\begin{exercise}[\Challenging, \nosolution]
Write down a full specification of the alternate constraint solver. Prove that
it defines a terminating rewriting system. Prove that it preserves constraint
equivalence. Characterize its normal forms.
\end{exercise}
%
\begin{exercise}[\Challenging, \nosolution]
Implement the alternate constraint solver.
\end{exercise}

What is the point of developing such an alternate constraint solver? For one
thing, its invariant property---all \poly stack frames are closed---is strong.
It may help reason about the solver's implementation and simplify its internal
data structures. For instance, in a state $\st;\mc;\co$, every type variable
that appears free in $\mc$ or $\co$ must be bound at the nearest enclosing
\poly \kwlet frame. Thus, keeping track of where type variables are bound
becomes very simple; the mutable integer ranks used in the previous solver's
implementation are no longer required here. As a related point, in the machine
representation of a type scheme, all type variables now have the same status;
there is no need to distinguish between free and bound variables. This may
help simplify the representation of type schemes as well as the algorithms
that simplify them.
% TEMPORARY suggestion de Vincent: la formulation de ce solveur est très
% indépendante de la nature des contraintes primitives, et fournit donc un
% bon candidat pour un solveur générique, d'où son intérêt.

It is worth making a couple of points here. First, the ideas developed here
are not specific of the setting of first-order unification in a free theory.
Indeed, we have been mainly concerned with the treatment of type scheme
introduction and instantiation constraints; this aspect is, to some extent,
independent of the nature of primitive constraints. In the presence of
subtyping, for instance, our two approaches to solving instantiation
constraints would still make sense. Second, it is the expressiveness of the
constraint language that makes it easy to come up with these two
algorithms---and possibly yet with others---and to reason about their
correctness. If our only starting point was Damas and Milner's typing rules,
it would perhaps be more difficult to reason about such variations.

The idea of treating instantiation constraints differently, depending
on whether the program identifier at hand is \poly or \mono, may be
traced back to two independent sources
\cite{mitchell:book,TrifonovSmith96}. Mitchell's algorithm PTL
implements type inference for \dm. It manipulates pairs of the form
$(\env\vdash\ttyp)$, where $\env$ is an environment mapping \mono
program identifiers to types and $\ttyp$ is a type; we call such
objects \mono typings. All type variables that appear in a \mono
typing are to be viewed as implicitly universally quantified, so \mono
typings are closed objects.  Note that a type scheme produced by our
alternate constraint solver closely corresponds to a \mono typing:
indeed, it is closed, and the conjunction of unresolved instantiation
constraints that it contains is a mapping $\env$ from \mono program
identifiers to types. Mitchell's algorithm accepts a term $\et$, as
well as a mapping $A$ from \poly program identifiers to \mono typings,
and produces a principal \mono typing for $\et$ under $A$. The
algorithm's definition bears a rather strong resemblance (at least in
spirit) with that of our alternate constraint solver.  Mitchell's
algorithm was later given a more declarative flavor (that is, turned
into a set of typing rules) by Chitil, who noticed that such a
constraint solving strategy may help interactively trace the source of
type errors \cite{chitil-01}. Trifonov and Smith independently defined
a type system whose expressive power is exactly that of \hmx, but
where all types schemes are closed. Again, the idea is to include
unresolved instantiation constraints inside type schemes. Their
approach makes it somewhat easier to reason about the simplification
of subtyping constraints, and was followed by
\longcite{pottier-ic-01}.

% --------------------------------------------------------------------------

\subsection*{Reporting Type Errors}

The constraint solvers presented above do not explain type errors: they only
answer the question: ``is the program well-typed?'' with ``yes'' or
``no.'' The answer is ``no'' when the underlying unification algorithm
produces the constraint $\cfalse$, or when an unbound program identifier is
found. The latter kind of error is easy to explain, so we disregard it
here. How does one produce a good error message when the constraint at hand
reduces to $\cfalse$?

In many compilers, constraint generation and constraint solving are not
distinguished. No constraints are explicitly generated: the type inference
algorithm analyzes the program's abstract syntax tree and drives the
unification algorithm directly. The unification algorithm, in turn, solves
equations in an eager fashion and reports a failure as soon as possible. As a
result, when failure is detected, the type inference algorithm is able to
pinpoint a program location, namely the current abstract syntax node, and to
display an unsatisfiable type equation.

This basic approach has the merit of simplicity, and yields tolerable error
messages. Yet, it suffers from several weaknesses. One is that the program
location that is current when an inconsistency is first detected may not be
that of the actual programming mistake; it may, in fact, be quite far from it.
Indeed, an expression that contains a mistake might still be well-typed,
albeit usually with a different type than expected. Because \MLlang programs
usually contain very little explicit type information, it might take a great
deal of context to determine that the expression's type is incompatible with
that of its context. The compiler may then pinpoint an arbitrary location
within the context, instead of the erroneous expression itself. Another
weakness is that the program location and type equation that form the
error message may vary greatly according to the strategy that was chosen to
generate and solve constraints. For instance, generating
$\calcule{\et_1}{\ttyp_1}\wedge\calcule{\et_2}{\ttyp_2}$ instead of
$\calcule{\et_2}{\ttyp_2}\wedge\calcule{\et_1}{\ttyp_1}$, or
$\calcule\et\ttyp$ instead of
$\exists\tvar.(\calcule\et\tvar\wedge\tvar=\ttyp)$, may lead to dramatically
different error messages. Such phenomena have been studied, for example, 
in \longcite{mcadam-98}, \longcite{LeeYi:Folklore}.

In the literature, the issue has been approached from two somewhat different
angles. One is, given a failed constraint solving run, to try and produce an
explanation of the failure \cite{Wand86,beaven-stansifer-93,duggan-bent-96},
or a set of program points involved in
it \cite{flanagan-al-96,choppella-02,haack-wells-03}. This usually involves
annotating constraints with information about their origin. Some authors
attempt to determine which, among the program points that participate in the
type error, are more likely to contain the actual programming
mistake \cite{johnson-walz-86}. The second angle is allow the programmer to
understand the program's type derivation, by interactively exploring it.
Tools that allow determining the type of a program fragment \emph{in
  isolation} are useful for this purpose \cite{bernstein-stark-95,chitil-01}.
The development of such tools is made somewhat difficult by the fact that
\MLtype does not have \emphindex{principal typings} \cite{jim-95}.

We lack space to give a more in-depth treatment of this issue. Let us simply
propose two remarks.
%
First, it is possible to develop \emph{several} constraint solvers, one geared
towards efficiency, the other towards good error reports. These two solvers
may employ different strategies and different data structures. Correct
programs may be accepted quickly by first running the former. If it fails,
then time is less of the essence, and the latter may be run to obtain a good
error message.
%
Second, there is no reason why a constraint solver should stop as soon as
failure is detected. On the contrary, we have pointed out above that this
design choice makes the error report dependent on the strategy that was
followed. We suggest that, in order to make the error report independent of
the strategy, the unification algorithm should report not only the
\emph{first} cause of failure, but \emph{all} of them. Let us remove the rules
\Rule{S-Clash} and \Rule{S-Cycle}, because, by replacing the cause of an error
with $\cfalse$, they discard crucial information. Then, a type error manifests
itself either as an inconsistent multi-equation (one that contains two
incompatible nonvariable terms) or as a cycle in the type structure. It is
possible to show that, up to a few simple structural equivalence rules, namely
commutativity and associativity of conjunction and exchange of existential
quantifiers, the algorithm is \emph{confluent}. In other words, all constraint
solving strategies lead to the same normal form. In general, a normal form
contains a number of type errors, all of which may be presented to the programmer.
The fact that the constraint rewriting system remains confluent, even in the
presence of failures, guarantees that error reports are independent of the
constraint solving strategy.
% There remains to map error reports back to
% (hopefully small) program slices, an issue that we do not address here.
\end{full}

\index{constraints!solving|)}

% --------------------------------------------------------------------------

\section{From ML-the-Calculus to ML-the-Language}
\label{section-calc-lang}

In this section, we explain how to extend the framework developed so far
to accommodate operations on values of base type (such as integers), pairs,
sums, references, and recursive function definitions.
\begin{full}
We explain how to combine such extensions when they are independent of one another.
\end{full}
Then, we describe 
\shortfull
{algebraic data type definitions}
{more complex extensions, namely algebraic data type
definitions, pattern matching, and type annotations}. Last, the issues
associated with recursive types are briefly discussed. For space reasons, exceptions are
not discussed; the reader is referred to~(\TAPLCHAPTER{14}).

\subsection*{Simple Extensions}

Introducing new constants and extending $\reduces[\delta]$ and $\ienv$
appropriately allows adding many features of \MLlang to \MLcalc. In each case,
it is necessary to check that the requirements of
Definition~\ref{def-requirements} are met, that is, to ensure that the new
initial environment faithfully reflects the nature of the new constants as
well as the behavior of the new reduction rules. Below, we describe several
such extensions in isolation. The first exercise establishes a
technical result that is useful in the next exercises.
%
\begin{exercise}[\Recommended, \QuickCheck]
Let $\ienv$ contain the binding $\econ :
\dmscheme\tvars{\ttyp_1\arw\ldots\arw\ttyp_n\arw\ttyp}$. Prove 
$\cxlet\ienv{\calcule{\tapp[3]\econ{\et_1}\ldots{\et_n}}{\ttyp'}}$ 
equivalent to $\cxlet\ienv{\exists\tvars.(\bigwedge_{i=1}^n
\calcule{\et_i}{\ttyp_i} \wedge \ttyp\subtype\ttyp')}$.
\solref{prim-apply}
\end{exercise}
%
\begin{exercise}[Integers, \Recommended, \Easy]
Integer literals and integer addition have been introduced and given an
operational semantics in Examples~\ref{example-integer-literals},
\ref{example-integer-literals-continued}, and~\ref{example-integer-addition}.
Let us now introduce an isolated type constructor $\tint$ of {\tcsignature}
$\normalkind$ and extend the initial environment $\ienv$ with the bindings
$\hat n : \tint$, for every integer $n$, and $\hat+ :
\tint\arw\tint\arw\tint$. Check that these definitions meet the requirements
of Definition~\ref{def-requirements}.  \solref{integers-sound}
\end{exercise}
%
\begin{full}
\begin{exercise}[Booleans, \Recommended, \Easy, \nosolution]
\label{exercise-booleans}
Booleans and conditionals have been introduced and given an operational
semantics in Exercise~\ref{ex:conditional}. Introduce an isolated type
constructor $\tbool$ to represent Boolean values and explain how to extend the
initial environment. Check that your definitions meet the requirements of
Definition~\ref{def-requirements}. What is the constraint generation rule for
the syntactic sugar $\eif{\et_0}{\et_1}{\et_2}$?
\end{exercise}
\end{full}
%
\begin{exercise}[Pairs, \Easy, \nosolution]
\label{exercise-typing-pairs}
Pairs and pair projections have been introduced and given an operational
semantics in Examples~\ref{example-pair-constructor} and
\ref{example-pair-destructors}. Let us now introduce an isolated type
constructor $\times$ of {\tcsignature}
$\normalkind\kindprod\normalkind\kindarrow\normalkind$, covariant in both of
its parameters, and extend the initial environment $\ienv$ with the following
bindings:
$$\begin{array}{rl}
(\cdot,\cdot) :& \dmscheme{\tvar\twar}{\tvar\arw\twar\arw\tvar\times\twar} \\
\fst{} :& \dmscheme{\tvar\twar}{\tvar\times\twar\arw\tvar} \\
\snd{} :& \dmscheme{\tvar\twar}{\tvar\times\twar\arw\twar}
\end{array}$$
Check that these definitions meet the requirements of
Definition~\ref{def-requirements}.
\end{exercise}
%
\begin{exercise}[Sums, \Easy, \nosolution]
\label{exercise-typing-sums}
Sums have been introduced and given an operational semantics in
Example~\ref{example-sums}. Let us now introduce an isolated type constructor
$+$ of {\tcsignature} $\normalkind\kindprod\normalkind\kindarrow\normalkind$,
covariant in both of its parameters, and extend the initial environment
$\ienv$ with the following bindings:
$$\begin{array}{rl}
\einj{1}{} :& \dmscheme{\tvar\twar}{\tvar\arw\tvar+\twar} \\
\einj{2}{} :& \dmscheme{\tvar\twar}{\twar\arw\tvar+\twar} \\
\ecase :& \dmscheme{\tvar\twar\tzar}{(\tvar+\twar)\arw(\tvar\arw\tzar)\arw(\twar\arw\tzar)\arw\tzar}
\end{array}$$
Check that these definitions meet the requirements of
Definition~\ref{def-requirements}.
\end{exercise}
%
\begin{exercise}[References, \Moderate]
References have been introduced and given an operational semantics in
Example~\ref{example-refs}. The type constructor $\tcref$ has been introduced
in Definition~\ref{def-tref}. Let us now extend the initial environment
$\ienv$ with the following bindings:
$$\begin{array}{rl}
{\eref{}} :& \dmscheme\tvar{\tvar\arw\tref\tvar} \\
{\ederef{}} :& \dmscheme\tvar{\tref\tvar\arw\tvar} \\
{\eassign{}{}} :& \dmscheme\tvar{\tref\tvar\arw\tvar\arw\tvar}
\end{array}$$
Check that these definitions meet the requirements of
Definition~\ref{def-requirements}.
\solref{sr-references}
\end{exercise}
%
\begin{exercise}[Recursion, \Recommended, \Moderate\fullsolution]
\newcommand{\ab}{{\tvar\arw\twar}}
The fixpoint combinator $\efix$ has been introduced and given an operational
semantics in Example~\ref{example-fix}. Let us now extend the initial
environment $\ienv$ with the following binding:
$$\begin{array}{rl}
\efix :& \dmscheme{\tvar\twar}{((\ab)\arw(\ab))\arw\ab}
\end{array}$$
Check that these definitions meet the requirements of
Definition~\ref{def-requirements}. Recall how the
\kwletrec syntactic sugar was defined in Example~\ref{example-fix},
and check that this gives
rise to the following constraint generation rule:
%
$$\begin{array}{rl}
       & \cxlet\ienv{\calcule{\eletrec\efar\evar{\et_1}{\et_2}}\ttyp} \\
\logeq &
\cxlet\ienv{\cxlet{\efar:\scheme{\tvar\twar}{
\cxlet{\efar:\ab;\evar:\tvar}{\calcule{\et_1}\twar}
}\ab}{\calcule{\et_2}\ttyp}}
\end{array}$$
Note the somewhat peculiar structure of this constraint: the program variable
$\efar$ is bound twice in it, with different type schemes. The constraint
requires all occurrences of $\efar$ within $\et_1$ to be assigned
the \emphfull{monomorphic} type $\ab$. This type is generalized and turned into a
type scheme before inspecting $\et_2$, however, so every occurrence of $\efar$
within $\et_2$ may receive a different type, as usual with \letpoly. 
%
\shortfull
{A more powerful way of typechecking recursive function definitions, 
proposed by \cite {Mycroft84} and known as \emphindex{polymorphic recursion},
allows the types of occurrences of $\efar$ within 
$\et_1$ to be possibly distinct instances of a single type scheme. However, 
type inference for this extension is equivalent to semi-unification \cite
{Henglein93}, which has been proved undecidable\index{undecidability!of
  type inference with polymorphic recursion} \cite 
{Kfo+Tiu+Urz:IAC-1993-v102n1}. Hence, type inference must either
require type annotations or rely on a semi-algorithm.}
{A more
powerful way of typechecking recursive function definitions is discussed in
\S\ref{section-univ} (page~\pageref{page-polymorphic-recursion}).}
%
\iffull{\solref{solfix}}
\end{exercise}

\begin{short}
In the exercises above, we have considered a number of extensions (integers,
booleans, pairs, etc.) {in isolation}. We have checked that each of them
preserves type soundness. Unfortunately, this does not in general imply that
their {combination} preserves type soundness. In fact, it is possible to
prove that these extensions are {independent} in a suitable sense and
that independent extensions may be safely combined. Unfortunately, we lack
space to further explain these notions.
\end{short}


\begin{full}
% TEMPORARY section à écrire

\subsection* {Combining Independent Extensions}
\label{sec-combining-independent-extensions}


Type soundness has been proved for integers, booleans, pairs, etc.  but
when added to \MLcalc, independently.  What happens when there are taken
altogether? What happens if, for instance, integers must later be extended
with an exponential primitive? Of course, some conditions should be set
before this question even makes sense. A constant could certainly not belong
to both extensions and be assigned incompatible types!  Even two extensions
with disjoint set of constants cannot be safely combined, in general. For
instance, adding a new constructor to represent infinity to integers will
not work magically, without redefining the all operations on integers so
that they can also treat the infinity.  Thus, all requirements of
Definition~\ref{def-requirements} must be checked again, and it is unclear
whether any part can be reused at all.  In fact, even extensions that are a
priori orthogonal do raise problem in principle.
%
Consider two constants, $\ecid$ of arity~1 with reduction rule $\eapp
\ecid \ev \reduces[\delta] \ev$ and typing assumption $\ecid :
\scheme{\tvar}{}{\tvar \arw \tvar}$, and $\ecseq$ of arity~2 with reduction
rule $\eapp[2] \ecseq \ev \ev \reduces[\delta] \ev$ and typing assumption
%
      $\ecid : \scheme {\tvar \twar}{}{\tvar \arw \twar \arw \tvar}$.
%
Extending \MLcalc with either $\ecid$ or $\ecseq$ is safe and so is the extension
with both of them. However, the later cannot be formally deduced from the
former.
\begin{exercise}[\QuickCheck, \nosolution]
Check that $\ecid$ and $\ecseq$ satisfy the
Definition~\ref{def-requirements} both when taken independently and when
taken simultaneously.
\end{exercise}
The problem is that the two proofs taken independently will never consider
the program $\eapp[2] \ecseq \ecid \ecid$.  This program is actually 
stuck when combining the extensions per say (by taking the union of the
reduction rules), but  reduces to $\ecid$ in the extension that
consider both constants simultaneously. The problem is that requirements of
Definition~\ref {def-requirements} must interpret the meta-variable $\ev$ in
the current extension, while some form of open-world interpretation is
needed for an extension to be compositional.

% TEMPORARY This rest of this subsection is a draft. 
% Lemmas have not been check yet, and probably need to be adjusted.}
\iffalse

\begin{definition}
A \emph {generic $\delta$-rule} is a rule $\et \reduces \et'$ where $\et$ is
of the form $\eapp[3] \econ {\ew_1} \ldots {\ew_n}$ and $\econ$ is a
destructor of arity $n$ and $w_i$'s contain only constructors or
variables. Moreover, each variable should appear at most once in $\et$.  The
meaning of a generic $\delta$-rule $\et \reduces[g]
\et'$ the set of its closed instances, obtained by replacing program
variables by arbitrary closed values.

A constructor $\econ$ has shape $\bar\tycon$ if $\env$ if
$\ga \satisfies \cxlet  \env {\ccall \econ {\tvars \arw \ttyp}}$
implies that $\ttyp(\epsilon) \in \bar\tycon$. 

An \emph{generic extension} is a quadruple $(\env, \bar\tycon, \bar \econ,
\Delta)$ where $\Delta$ is a set of generic $\delta$-rules and
every constructor $\econ \in \bar \econ$ is in $\Dom \env$, has a shape
in~$\bar \tycon$ and is invertible \XXX{forward pointer}.
\end{definition}

\begin{definition}
\label{def-generic-requirements}
An extension $(\env,\bar\tycon, \bar \econ, \Delta)$ is \emph{safe} if and
only if (i) $\Delta \subseteq (\sqsubseteq)$ and (ii) for all constructor
$\econ$ of $\bar \econ$ of arity $n$, and all expressions $\et$ of the form
$\eapp[3] \econ {\ev_1} \ldots {\ev_n}$, such that $\entails \exists
\tvars.\cxlet {\env} {\calcule {\ev}{\tapp \tycon \ttypc}}$, there
exists $\et'$ such that $\et \reduces[\Delta] \et'$.
\end{definition}


\begin{lemma}
If $(\env, \bar \tycon, \Dom \env, \Delta)$ is a safe generic extension
then the extension $(\env, \Delta)$ satisfies the requirements
of Definition~\ref {def-requirements} holds.
\end{lemma}
\begin{Proof}
\demoreset
(i) We must first show that for every $\delta$-rule $\et \reduces \et'
\in\Delta$, for every ground term substitution $\theta$ of the form $\evars
\mapsto \bar\ew$ where $\evars$ are free variables of $\et$, 
then $\theta(\et) \sqsubseteq \theta(\et')$, that is, 
if the reduction is pure that 
$\cxlet \ienv {\calcule {\theta(\et')}\ttyp}
\entails \cxlet \ienv {\calcule {\theta(\et')}\ttyp}$. 
For $\et'$, which is an arbitrary term, we have 
$\exists \tvars. \cxlet{\evars:\tvars}{\calcule {\et'} \ttyp \wedge
\calcule {\bar\ev} \tvars}$
entails $\calcule {\subst{\evars}{\bar\ev}\et'}\ttyp$~\dlabel 1.  Moreover,
the converse entailment holds for $\et$ in place of $\et'$, since $\et$ is a
value~\dlabel 2.  By requirement (i) of Definition~\ref
{def-generic-requirements}, we have $\et \sqsubseteq \et'$, which implies
that $\cxlet \ienv {\calcule \et \ttyp} \entails
\cxlet \ienv {\calcule {\et'} \ttyp}$~\dlabel 3. Thus we have
$$
\begin{tabular*}{\linewidth}{C.R!{\extracolsep{0em}}L.R:}&
&\cxlet \ienv {\calcule {\theta(\et)}\ttyp} \\ &
\entails &
\cxlet \ienv {\exists \tvars. 
\cxlet{\evars:\tvars}{\calcule {\et} \ttyp \wedge
\calcule {\bar\ev} \tvars}} & \dref 2
\\ & \logeq &
\exists \tvars. \cxlet{\evars:\tvars}{\cxlet \ienv \calcule {\et} \ttyp
 \wedge \cxlet \ienv {\calcule {\bar\ev} \tvars}} \\ &
\entails &
\exists \tvars. \cxlet{\evars:\tvars}{\cxlet \ienv \calcule {\et'} \ttyp
 \wedge \cxlet \ienv {\calcule {\bar\ev} \tvars}} & \dref 3
\\ &
\logeq &
\cxlet \ienv {\exists \tvars. 
\cxlet{\evars:\tvars}{\calcule {\et'} \ttyp \wedge
\calcule {\bar\ev} \tvars}} \\ &
\entails &
\cxlet \ienv {\calcule {\theta(\et')}\ttyp} 
& \dref 1\\&
\end{tabular*}
$$

(ii) Assume that $\exists \tvar. \cxlet \ienv {\calcule 
{\eapp[3] \econ {\ev_1}{\ldots}{\ev_n}}\tvar}$ holds. 
\proofcase {$n \le \arity \econ$ or $n = \arity \econ$ and $\econ \in
\econstructors$} then $\eapp[3] \econ {\ev_1}{\ldots}{\ev_n}$ is a value. 
\proofcase {$\econ \in \econstructors$}
Necessarily
\proofcase {$\econ \in \edestructors$}

\end{Proof}

\begin{lemma}
Let $(\env,\bar\tycon, \Delta, \env_E)$ 
and $(\env_I',\bar\tycon', \Delta', \env_E')$ 
be two \emph{safe} generic extensions such that 
(i) $\disjoint{\bar \tycon}  {\bar \tycon'}$, 
(ii) $\disjoint {\Dom {\env_E}} {\Dom {\env_E'}}$, 
(iii) $\env_I$, $\env_I'$, $\env_E$, and $\env_E'$ agrees.
Then $(\env_I \cup \env_I' \setminus \env_E \setminus \env_E', \bar\tycon\bar
\tycon', \Delta \cup \Delta', \env_E \cup \env_E')$ 
is a \emph{safe} generic extension.
\end{lemma}

% TEMPORARY \XXX{Extensible data types might reference this section}
% \cite{Bonniot}\cite{Chambers}
\fi

%WHIZZY\subsection*
\end{full}

% -----------------------------------------------------------------------------

\subsection*{Algebraic Data Types}

\index{algebraic data types and ML type inference|(}
\index{data types and ML type inference|(}
\index{type inference!and algebraic data types|(}

\index{recursive types!and ML type inference|(}
\index{type inference!and recursive types|(}


Exercises~\ref{exercise-typing-pairs} and~\ref{exercise-typing-sums} have
shown how to extend the language with binary, anonymous products and sums.
These constructs are quite general but still have several shortcomings.
First, they are only binary, while we would like to have $k$-ary products and
sums, for arbitrary $k\geq 0$. Such a generalization is of course
straightforward. Second, more interestingly, their components must be referred
to by numeric index (as in ``extract the {second} component of the
pair''), rather than by name (``extract the component named \verb+y+''). In
practice, it is crucial to use names, because they make programs more readable
and more robust in the face of changes. One could introduce a mechanism that
allows defining names as syntactic sugar for numeric indices. That would help
a little, but not much, because these names would not appear in {types},
which would still be made of anonymous products and sums. Third, in the
absence of recursive types, products and sums do not have sufficient
expressiveness to allow defining unbounded data structures, such as lists.
Indeed, it is easy to see that every value whose type $\ttyp$ is composed of
base types ($\tint$, $\tbool$, etc.), products, and sums must have bounded
size, where the bound $\cardinal\ttyp$ is a function of $\ttyp$. More
precisely, up to a constant factor, we have
$\cardinal\tint=\cardinal\tbool=1$,
$\cardinal{\ttyp_1\times\ttyp_2}=1+\cardinal{\ttyp_1}+\cardinal{\ttyp_2}$, and
$\cardinal{\ttyp_1+\ttyp_2}=1+\max(\cardinal{\ttyp_1},\cardinal{\ttyp_2})$.
The following example describes another facet of the same problem.
%
\begin{example}
\label{example-length-norec}
A list is either empty, or a pair of an element and another list. So, it seems
natural to try and encode the type of lists as a sum of some arbitrary type
(say, $\tunit$) on the one hand, and of a product of some element type and of
the type of lists itself on the other hand. With this encoding in mind, we
can go ahead and write code---for instance, a function that computes the
length of a list:
\newcommand{\lgth}{{\kwtt{length}}}
\newcommand{\liste}{{\kwtt{l}}}
$$\dletrec\lgth\liste{\eapp[3]\ecase\liste
  {(\efun\wildpat{\hat0})}
  {(\efun\evar{\hat1\mathbin{\hat+}\eapp\lgth{(\snd\evar)}})}}$$
We have used integers, pairs, sums, and the \kwletrec construct introduced in
the previous section. The code analyzes the list $\liste$ using a $\ecase$
construct. If the left branch is taken, the list is empty, so $0$ is returned.
If the right branch is taken, then $\evar$ becomes bound to a pair of some
element and the tail of the list. The latter is obtained using the projection
operator $\snd$.  Its length is computed using a recursive call to $\lgth$ and
incremented by 1.  This code makes perfect sense. However, applying the
constraint generation and constraint solving algorithms eventually leads to an
equation of the form $\tvar=\twar+(\tzar\times\tvar)$, where $\tvar$ stands
for the type of $\liste$. This equation accurately reflects our encoding of
the type of lists.  However, in a syntactic model, it has no solution, so our
definition of $\lgth$ is ill-typed. It is possible to adopt a free regular tree
model, thus introducing \emph{equirecursive}\index{equirecursive types} types into the
system~(\TAPLCHAPTER{20}); however, there are good reasons not to do so
(see the section on Recursive Types on p.~\pageref{section-recursive-types}).%
\end{example}

To work around this problem,
\MLlang offers \emph{algebraic data type} definitions, whose elegance lies in
the fact that, while representing only a modest theoretical extension, they do
solve the three problems mentioned above. An algebraic data type may be viewed
as an \emph{abstract type} that is declared to be \emph{isomorphic} to a
($k$-ary) product or sum type with named components. The type of each
component is declared, as well, and may refer to the algebraic data type that
is being defined: thus, algebraic data types are
\index{isorecursive types}\emph{isorecursive}~(\TAPLCHAPTER{20}). In order to
allow sufficient 
flexibility when declaring the type of each component, algebraic data type
definitions may be {parameterized} by a number of type variables. Last,
in order to allow the description of complex data structures, it is necessary
to allow several algebraic data types to be defined at once; the definitions
may then be {mutually recursive}. In fact, in order to simplify this
formal presentation, we assume that {all} algebraic data types are
defined at once at the beginning of the program. This decision is, of course, at
odds with modular programming but will not otherwise be a problem.

In the following, $\tnam$ ranges over a set of \emph{data types}. We assume
that data types form a subset of type constructors. We require each of them to
be isolated and to have image kind
$\normalkind$. Furthermore, $\elab$ ranges over a set
$\rowlabels$ of \emph{labels}, which we use both as \emphfull{data
constructors} and as \emphfull{record labels}.
%
% En fait, dans le source, on distingue ces deux catégories, en employant les
% macros \dc et \rl. Mais la distinction a été supprimée pour le lecteur, de
% façon à employer les mêmes notations que dans la partie rangées.
%
An \emph{algebraic data type definition} is either a \emph{variant
type} definition or a \emph{record type} definition, whose respective forms
are
%
$$\datadef\tnam\tvarc{\sum_{i=1}^k\, \dc_i:\ttyp_i}
\qquad\text{and}\qquad
\datadef\tnam\tvarc{\prod_{i=1}^k\, \rl_i:\ttyp_i}.$$
%
In either case, $k$ must be nonnegative. If $\tnam$ has {\tcsignature}
$\vec\kind\kindarrow\normalkind$, then the type variables $\tvarc$ must have
kind $\vec\kind$. Every $\ttyp_i$ must have kind $\normalkind$. We refer to
$\tvars$ as the \emph{parameters} and to $\ttypc$ (the vector formed by
$\ttyp_1,\ldots,\ttyp_k$) as the \emph{components} of the definition. The
parameters are bound within the components, and the definition must be closed,
that is, $\ftv{\ttypc}\subseteq\tvars$ must hold. Last, for an algebraic data
type definition to be valid, the behavior of the type constructor $\tnam$ with
respect to subtyping must match its definition. This requirement is clarified
below.
%
\begin{definition}
\label{def-algebraic-variance}
Consider an algebraic data type definition whose parameters and components are
respectively $\tvarc$ and $\ttypc$. Let $\tvarcp$ and $\ttypcp$ be their
images under an arbitrary renaming. Then, $\tnam\,\tvarc\subtype\tnam\,\tvarcp
\entails \ttypc\subtype\ttypcp$ must hold.
\end{definition}
%
% TEMPORARY This Definition could be called a Requirement. 
% \newbcptheorem{requirement}{Requirement}

Because it is stated in terms of an entailment assertion,
the above requirement bears on the interpretation of subtyping. The
idea is, since $\tnam\,\tvarc$ is declared to be isomorphic to (a sum or a
product of) $\ttypc$, whenever two types built with $\tnam$ are comparable,
their unfoldings should be comparable as well. The reverse entailment
assertion is not required for type soundness, and it is sometimes useful to
declare algebraic data types that do not validate it---so-called \emph{phantom
types} \cite{fluet-02}. Note that the requirement may always be satisfied by
making the type constructor $\tnam$ {invariant} in all of its
parameters. Indeed, in that case, $\tnam\,\tvarc\subtype\tnam\,\tvarcp$
entails $\tvarc=\tvarcp$, which must entail $\ttypc=\ttypcp$ since $\ttypcp$
is precisely $\subst\tvarc\tvarcp\ttypc$. In an equality free tree model,
every type constructor is naturally invariant, so the requirement is trivially
satisfied. In other settings, however, it is often possible to satisfy the
requirement of Definition~\ref{def-algebraic-variance} while assigning $\tnam$
a less restrictive variance. The following example illustrates such a case.
%
\begin{example}
\label{example-lists-trees}
Let $\tclist$ be a data type of {\tcsignature}
$\normalkind\kindarrow\normalkind$.  Let $\nil$ and $\cons$ be data
constructors. Then, the following is a definition of $\tclist$ as a variant
type:
%
$$\datadef\tclist\tvar{\Sigma\,(\nil:\tunit; \cons:\tvar\times\tlist\tvar)}$$
%
Because data types form a subset of type constructors, it is valid
to form the type $\tlist\tvar$ in the right-hand side of the definition, even
though we are still in the process of defining the meaning of $\tclist$. In
other words, data type definitions may be recursive. However, because $\iso$
is not interpreted as equality, the type $\tlist\tvar$ is {not} a
recursive type: it is nothing but an application of the unary type constructor
$\tclist$ to the type variable $\tvar$. To check that the definition of
$\tclist$ satisfies the requirement of Definition~\ref{def-algebraic-variance},
we must ensure that
$$\tlist\tvar\subtype\tlist{\tvar'} \entails
  \tunit\subtype\tunit \wedge
  \tvar\times\tlist\tvar \subtype  \tvar'\times\tlist{\tvar'}$$
holds. This assertion is equivalent to
$\tlist\tvar\subtype\tlist{\tvar'} \entails \tvar\subtype\tvar'$.
To satisfy the requirement, it is sufficient to make $\tclist$ a
{covariant} type constructor, that is, to define subtyping in the model
so that $\tlist\tvar\subtype\tlist{\tvar'} \logeq \tvar\subtype\tvar'$ holds.

\newcommand{\tctree}{{\kwd{tree}}}
\newcommand{\ttree}[1]{\tctree\,#1}
\newcommand{\racine}{{\kwtt{root}}}
\newcommand{\sons}{{\kwtt{sons}}}
%
Let $\tctree$ be a data type of {\tcsignature}
$\normalkind\kindarrow\normalkind$. Let $\racine$ and $\sons$ be record
labels. Then, the following is a definition of $\tctree$ as a record type:
%
$$\datadef\tctree\tvar{\Pi\,(\racine:\tvar; \sons:\tlist{(\ttree\tvar)})}$$
%
This definition is again recursive, and relies on the previous definition.
Because $\tclist$ is covariant, it is straightforward to check that the
definition of $\tctree$ is valid if $\tctree$ is made a covariant type
constructor as well.
\end{example}
%
\begin{full}
\begin{exercise}[\Moderate, \nosolution]
Consider a nonrecursive algebraic data type definition, where the variance of
every type constructor that appears on the right-hand side is known. Can you
systematically determine, for each of the parameters, the least restrictive
variance that makes the definition valid? Generalize this procedure to the
case of recursive and mutually recursive algebraic data type definitions.
\end{exercise}
% TEMPORARY il serait bon d'écrire le corrigé pour vérifier l'énoncé...
% TEMPORARY il faudrait aussi la variance vide si on veut pouvoir inférer
% des variances minimales
\end{full}

A \emph{prologue} is a set of algebraic data type definitions, where each data
type is defined at most once and where each data constructor or record label
appears at most once. A \emph{program} is a pair of a prologue and an
expression.
%
The effect of a prologue is to enrich the programming language with new
constants. That is, a variant type definition extends the operational
semantics with several injections and a $\ecase$ construct, as in
Example~\ref{example-sums}. A record type definition extends it with a record
formation construct and several projections, as in
Examples~\ref{example-pair-constructor} and~\ref{example-pair-destructors}.
In either case, the initial typing environment $\ienv$ is extended with
information about these new constants. Thus, algebraic data type definitions
might be viewed as a simple configuration language that allows specifying in
which instance of \MLcalc the expression that follows the prologue should be
typechecked and interpreted. Let us now give a precise account of this
phenomenon.

To begin, suppose the prologue contains the definition
$\datadef\tnam\tvarc{\sum_{i=1}^k\, \dc_i:\ttyp_i}$. Then, for each
$i\in\{1,\ldots,k\}$, a constructor of arity 1, named $\dc_i$, is introduced.
Furthermore, a destructor of arity $k+1$, named $\ecase_\tnam$, is introduced.
When $k>0$, it is common to write $\eapp[2]\ecase\et{[ \dc_i:\et_i ]_{i=1}^k}$
for the application $\eapp[4]{\ecase_\tnam}\et{\et_1}\ldots{\et_n}$. The
operational semantics is extended with the following reduction rules,
for $i\in\{1,\ldots,k\}$:
%
\infax[R-Alg-Case]{
\eapp[2]\ecase{(\dc_i\,\ev)}{[ \dc_j:\ev_j ]_{j=1}^k}
\reduces[\delta]
\eapp{\ev_i}\ev
}
%
For each $i\in\{1,\ldots,k\}$, the initial environment is extended with the
binding $\dc_i : \dmscheme\tvars{\ttyp_i\arw\tnam\,\tvarc}$. It is further
extended with the binding $\ecase_\tnam : \dmscheme{\tvars\tzar}
{\tnam\,\tvarc\arw(\ttyp_1\arw\tzar)\arw\ldots(\ttyp_k\arw\tzar)\arw\tzar}$.

Now, suppose the prologue contains the definition
$\datadef\tnam\tvarc{\prod_{i=1}^k\, \rl_i:\ttyp_i}$.  Then, for each
$i\in\{1,\ldots,k\}$, a destructor of arity 1, named $\rl_i$, is introduced.
Furthermore, a constructor of arity $k$, named $\emake_\tnam$, is introduced.
It is common to write $\et.\rl$ for the application $\eapp\rl\et$ and, when
$k>0$, to write $\{ \rl_i = \et_i \}_{i=1}^k$ for the application
$\eapp[3]{\emake_\tnam}{\et_1}\ldots{\et_k}$. The
operational semantics is extended with the following reduction rules,
for $i\in\{1,\ldots,k\}$:
%
\infax[R-Alg-Proj]{
(\{ \rl_j = \ev_j \}_{j=1}^k). \rl_i \reduces[\delta] \ev_i
}
%
For each $i\in\{1,\ldots,k\}$, the initial environment is extended with the
binding $\rl_i : \dmscheme\tvars{\tnam\,\tvarc\arw\ttyp_i}$. It is further
extended with the binding $\emake_\tnam : \dmscheme\tvars
{\ttyp_1\arw\ldots\arw\ttyp_k\arw\tnam\,\tvarc}$.
%
\begin{example}
\label{example-length}
The effect of defining $\tclist$ (Example~\ref{example-lists-trees}) is to
make $\nil$ and $\cons$ data constructors of arity 1 and to introduce a
binary destructor $\ecase_\tclist$. The definition also extends the initial
environment as follows:
$$\begin{array}{rl}
\nil :& \dmscheme\tvar{\tunit\arw\tlist\tvar} \\
\cons :& \dmscheme\tvar{\tvar\times\tlist\tvar\arw\tlist\tvar} \\
\ecase_\tclist :& \dmscheme{\tvar\tzar}{\tlist\tvar\arw(\tunit\arw\tzar)\arw
(\tvar\times\tlist\tvar\arw\tzar)\arw\tzar}
\end{array}$$
Thus, the value $\cons(\hat0, \nil\eunit)$, an integer list of length 1, has
type $\tlist\tint$. A function that computes the length of a list may now be
written as follows:
%
\newcommand{\lgth}{{\kwtt{length}}}
\newcommand{\liste}{{\kwtt{l}}}
%
$$\dletrec\lgth\liste{\eapp[2]\ecase\liste{[\,
  \nil:\efun\wildpat{\hat0} \mid
  \cons:\efun\evar{\hat1\mathbin{\hat+}\eapp\lgth{(\snd\evar)}}
\,]}}$$
%
Recall that this notation is syntactic sugar for
%
$$\dletrec\lgth\liste{\eapp[3]{\ecase_\tclist}\liste
  {(\efun\wildpat{\hat0})}
  {(\efun\evar{\hat1\mathbin{\hat+}\eapp\lgth{(\snd\evar)}})}}$$
%
The difference with the code in Example~\ref{example-length-norec} appears
minimal: the $\ecase$ construct is now annotated with the data type $\tclist$.
As a result, the type inference algorithm employs the type scheme assigned to
$\ecase_\tclist$, which is derived from the definition of $\tclist$, instead
of the type scheme assigned to the anonymous $\ecase$ construct, given in
Exercise~\ref{exercise-typing-sums}. This is good for a couple of reasons.
First, the former is more informative than the latter, because it contains the
type $\ttyp_i$ associated with the data constructor $\dc_i$. Here, for
instance, the generated constraint requires the type of $\evar$ to be
$\tvar\times\tlist\tvar$ for some $\tvar$, so a good error message would be
given if a mistake was made in the second branch, such as omitting the use of
$\snd$. Second, and more fundamentally, \emphfull{the code is now well-typed},
even in the absence of recursive types. In Example~\ref{example-length-norec},
a cyclic equation was produced because $\ecase$ required the type of
$\liste$ to be a sum type and because a sum type carries the types of its left
and right branches as subterms. Here, $\ecase_\tclist$ requires
$\liste$ to have type $\tlist\tvar$ for some $\tvar$. This is an abstract
type: it does not explicitly contain the types of the branches. As a result,
the generated constraint no longer involves a cyclic equation. It is, in
fact, satisfiable; the reader may check that $\lgth$ has type
$\dmscheme\tvar{\tlist\tvar\arw\tint}$, as expected.
\end{example}

Example~\ref{example-length} stresses the importance of using \emphfull{declared,
  abstract} types, as opposed to \emphfull{anonymous, concrete} sum or product
types, in order to obviate the need for recursive types. The essence of the
trick lies in the fact that the type schemes associated with operations on
algebraic data types implicitly \emphfull{fold} and \emphfull{unfold} the data type's
definition. More precisely, let us recall the type scheme assigned to the
$i^{\text{th}}$ injection in the setting of ($k$-ary) anonymous sums: it is
$\dmscheme{\tvar_1\ldots\tvar_k}{\tvar_i\arw\tvar_1+\ldots+\tvar_k}$, or, more
concisely,
$\dmscheme{\tvar_1\ldots\tvar_k}{\tvar_i\arw\sum_{i=1}^k\,\tvar_i}$. By
instantiating each $\tvar_i$ with $\ttyp_i$ and generalizing again, we find
that a more specific type scheme is
$\dmscheme\tvars{\ttyp_i\arw\sum_{i=1}^k\,\ttyp_i}$. Perhaps this could have
been the type scheme assigned to $\dc_i$? Instead, however, it is
$\dmscheme\tvars{\ttyp_i\arw\tnam\,\tvarc}$. We now realize that the latter type
scheme not only reflects the operational behavior of the $i^{\text{th}}$
injection but also \emphfull{folds} the definition of the algebraic data type
$\tnam$ by turning the anonymous sum $\sum_{i=1}^k\,\ttyp_i$---which forms the
definition's right-hand side---into the parameterized abstract type
$\tnam\,\tvarc$---which is the definition's left-hand side. Conversely, the
type scheme assigned to $\ecase_\tnam$ \emphfull{unfolds} the definition.  The
situation is identical in the case of record types: in either case,
\emphfull{constructors fold, destructors unfold.} In other words, occurrences of
data constructors and record labels in the code may be viewed as explicit
instructions for the typechecker to fold or unfold an algebraic data type
definition. This mechanism is characteristic of \index{isorecursive types}{isorecursive} types.
%
\begin{exercise}[\QuickCheck, \nosolution]
For a fixed $k$, check that all of the machinery associated with $k$-ary
anonymous products---that is, constructors, destructors, reduction rules, and
extensions to the initial typing environment---may be viewed as the result of
a single algebraic data type definition. Conduct a similar check in the case
of $k$-ary anonymous sums.
\end{exercise}
%
\begin{exercise}[\Moderate, \nosolution]
Check that the above definitions meet the requirements of
Definition~\ref{def-requirements}.
\end{exercise}
%
\begin{exercise}[\Moderate, \nosolution]
For the sake of simplicity, we have assumed that all data constructors have
arity one. If desired, it is possible to accept variant data type definitions
of the form $\datadef\tnam\tvarc{\sum_{i=1}^k\, \dc_i:\ttypc_i}$, where the
arity of the data constructor $\dc_i$ is the length of the vector $\ttypc_i$,
and may be an arbitrary nonnegative integer. This allows, for instance,
altering the definition of $\tclist$ so that the data constructors $\nil$ and
$\cons$ are respectively nullary and binary. Make the necessary changes in the
above definitions and check that the requirements of
Definition~\ref{def-requirements} are still met.
\end{exercise}

\begin{full}
In this formal presentation of algebraic data types, we have assumed that all
algebraic data type definitions are known before the program is typechecked.
This simplifying assumption is forced on us by the fact that we interpret
constraints in a fixed model, that is, we assume a fixed universe of types.
In practice, programming languages have \emph{module systems}, which allow
distinct modules to have distinct, partial views of the universe of types.
Then, it becomes possible for each module to come with its own data type
definitions. Interestingly, it is even possible, in principle, to split the
definition of a single data type over several modules, yielding
\emph{extensible} algebraic data types. 
\label{page-extensible-variants}
For instance, module $A$ might declare
the existence of a parameterized variant type $\tnam\,\tvarc$, without giving
its components. Later on, module $B$ might define a component $\dc:\ttyp$,
where $\ftv\ttyp\subseteq\tvars$. Such a definition makes $\dc$ a unary
constructor with type scheme $\dmscheme\tvars{\ttyp\arw\tnam\,\tvarc}$, as
before. It becomes impossible, however, to introduce a destructor
$\ecase_\tnam$, because the definition of an extensible variant type can never
be assumed to be complete---other, unknown modules might extend it further. To
compensate for its absence, one may supplement every constructor $\dc$ with a
destructor $\dc^{-1}$, whose semantics is given by
$\eapp[3]{\dc^{-1}}{(\eapp\dc\ev)}{\ev_1}{\ev_2} \reduces[\delta]
\eapp{\ev_1}\ev$ and $\eapp[3]{\dc^{-1}}{(\eapp{\dc'}\ev)}{\ev_1}{\ev_2}
\reduces[\delta] \eapp{\ev_2}{(\eapp{\dc'}\ev)}$ when $\dc\not=\dc'$, and
whose type scheme is $\dmscheme{\tvars\tzar}{\tnam\,\tvarc\arw
  (\ttyp\arw\tzar)\arw(\tnam\,\tvarc\arw\tzar)\arw\tzar}$. When pattern
matching is available, $\dc^{-1}$ may in fact be defined in the language.
\MLlang does not offer extensible algebraic data types as a language
feature, but does have one built-in extensible variant type, namely the type
@exn@ of exceptions. Thus, it is possible to define new constructors for the
type @exn@ within any module. The price of this extra flexibility is that no
exhaustive case analysis on values of type @exn@ is possible.
\end{full}

One significant drawback of algebraic data type definitions resides in the
fact that a label $\elab$ cannot be \emph{shared} by two distinct variant or
record type definitions. Indeed, every algebraic data type definition extends
the calculus with new constants. Strictly speaking, our presentation does not
allow a single constant $\econ$ to be associated with two distinct
definitions. Even if we did allow such a collision, the initial environment
would contain two bindings for $\econ$, one of which would then hide the
other. This phenomenon arises in actual implementations of \MLlang, where a
new algebraic data type definition may hide some of the data constructors or
record labels introduced by a previous definition. An elegant solution to this
lack of expressiveness is discussed in \S\ref{section-rows}.

\begin{FULL}
\subsection*{Pattern Matching}

Our presentation of products, sums and algebraic data types has remained
within the setting of \MLcalc: that is, data structures have been built out of
constructors, while the case analysis and record access operations have been
viewed as destructors. Some syntactic sugar has been used to recover standard
notations. The language is now expressive enough to allow defining and
manipulating complex data structures, such as lists and trees. Yet, experience
shows that programming in such a language is still somewhat cumbersome.
Indeed, case analysis and record access are low-level operations: the former
allows inspecting a tag and branching, while the latter allows dereferencing a
pointer. In practice, one often needs to carry out more complex tasks, such as
determining whether a data structure has a certain shape or whether two data
structures have comparable shapes.  Currently, the only way to carry out these
tasks is to program an explicit sequence of low-level operations. It would be
much preferable to extend the language so that it becomes directly possible to
describe shapes, called \emph{patterns}, and so that checking whether a
patterns \emph{matches} a value becomes an elementary operation. \MLlang
offers this feature, called \emph{pattern matching}. Although pattern matching
may be added to \MLcalc by introducing a family of destructors, we rather
choose to extend the calculus with a new @match@ construct, which subsumes the
existing @let@ construct. This approach appears somewhat simpler and more
powerful. We now carry out this extension.

%
% Note terminologique: je choisis de dire `a pattern matches a value', et
% non l'inverse, car on écrit p\mapsto v. Ainsi on peut lire de gauche à
% droite.
%

\begin{TTCOMPONENT}{Patterns and pattern matching}{}
\ttlabel{patterns}
\let \\ \TTSyntaxAlternative

\TTSyntaxCategoryNamed{\ppat}{}{Patterns}
\\ {\wildpat}                              {Wildcard}
\\ {\evar}                                 {Variable}
\\ {\eapp[3]\econ{\ppat_1}\ldots{\ppat_k}} {Data}
\\ {}                          {$\econ\in\econset^+ \wedge k=\arity\econ$}
\\ {\andpat\ppat\ppat}                     {Conjunction}
\\ {\orpat\ppat\ppat}                      {Disjunction}

\summaryhead{Pattern matching}
$$
#>\begin{array}{r@{\;}c@{\;}l}
\subst\wildpat\ev & = & \varnothing \\
& & \subst{\eapp[3]\econ{\ppat_1}\ldots{\ppat_k}}
      {\eapp[3]\econ{\ev_1}\ldots{\ev_k}} \\ & = &
\subst{\ppat_1}{\ev_1} \gswedge \ldots \gswedge \subst{\ppat_k}{\ev_k} \\
\subst{\andpat{\ppat_1}{\ppat_2}}\ev & = & 
\subst{\ppat_1}\ev \gswedge \subst{\ppat_2}\ev \\
\subst{\orpat{\ppat_1}{\ppat_2}}\ev & = & 
\subst{\ppat_1}\ev \gsvee \subst{\ppat_2}\ev
\end{array}$$

\extraspacehack{.07in}
\end{TTCOMPONENT}
%
% En ce qui concerne les règles de génération de contraintes pour les motifs,
% on pourrait souhaiter que l'application soit binaire: $\eapp\ppat\ppat$, car
% cela simplifierait la règle et soulignerait la similarité avec la règle
% associée aux applications dans les expressions. Cependant, il ne semble
% pas souhaitable que l'application binaire fasse partie de la syntaxe des
% motifs, car cela n'a pas vraiment de sens sémantique?

% L'emploi de la notation $\dpv\ppat$ ci-dessous est-il gênant?
%
Let us first define the syntax of patterns (Figure~\ref{fig:patterns}) and
describe (informally, for now) which values they match. To a pattern $\ppat$,
we associate a set of \emph{defined program variables} $\dpv\ppat$, whose
definition appears in the text that follows. The pattern $\ppat$ is
well-formed if and only if $\dpv\ppat$ is defined. To begin, the wildcard
$\wildpat$ is a pattern, which matches every value and binds no variables. We
let $\dpv\wildpat=\varnothing$. Although the wildcard may be viewed as an
anonymous variable, and we have done so thus far, it is now simpler to view it
as a distinct pattern. A program variable $\evar$ is also a pattern, which
matches every value and binds $\evar$ to the matched value. We let
$\dpv\evar=\{\evar\}$. Next, if $\econ$ is a constructor of arity $k$, then
$\eapp[3]\econ{\ppat_1}\ldots{\ppat_k}$ is a pattern, which matches
$\eapp[3]\econ{\ev_1}\ldots{\ev_k}$ when $\ppat_i$ matches $\ev_i$ for every
$i\in\{1,\ldots,k\}$. We let $\dpv{\eapp[3]\econ{\ppat_1}\ldots{\ppat_k}}=
\dpv{\ppat_1}\uplus\ldots\uplus\dpv{\ppat_k}$. That is, the pattern
$\eapp[3]\econ{\ppat_1}\ldots{\ppat_k}$ is well-formed when
$\ppat_1,\ldots,\ppat_k$ define \emph{disjoint} sets of variables.  This
condition rules out \emph{nonlinear} patterns such as $(\evar,\evar)$.
Defining the semantics of such a pattern would require a notion of equality at
every type, which introduces various complications, so it is commonly
considered ill-formed.
% TEMPORARY peut-on donner une meilleure justification?
The pattern $\andpat{\ppat_1}{\ppat_2}$ matches all
values that both $\ppat_1$ and $\ppat_2$ match. It is commonly used with
$\ppat_2$ a program variable: then, it allows
examining the shape of a value and binding a name to it at the same time.
Again, we define
$\dpv{\andpat{\ppat_1}{\ppat_2}}=\dpv{\ppat_1}\uplus\dpv{\ppat_2}$. The
pattern $\orpat{\ppat_1}{\ppat_2}$ matches all values that either $\ppat_1$ or
$\ppat_2$ matches. We define
$\dpv{\orpat{\ppat_1}{\ppat_2}}=\dpv{\ppat_1}=\dpv{\ppat_2}$. That is, the
pattern $\orpat{\ppat_1}{\ppat_2}$ is well-formed when $\ppat_1$ and $\ppat_2$
define the \emph{same} variables. Thus,
$\orpat{(\einj{1}\evar)}{(\einj{2}\evar)}$ is a well-formed pattern, which
binds $\evar$ to the component of a binary sum, without regard for its tag.
However, $\orpat{(\einj{1}{\evar_1})}{(\einj{2}{\evar_2})}$ is ill-formed,
because one cannot statically predict whether it defines $\evar_1$ or
$\evar_2$.

Let us now formally define whether a pattern $\ppat$ matches a value $\ev$ and
how the variables in $\dpv\ppat$ become bound to values in the process.  This
is done by introducing a \emph{generalized substitution}, written
$\subst\ppat\ev$, which is either undefined or a substitution of values for
the program variables in $\dpv\ppat$. If the former, then $\ppat$ does not
match $\ev$. If the latter, then $\ppat$ matches $\ev$ and, for every
$\evar\in\dpv\ppat$, the variable $\evar$ becomes bound to the value
$\subst\ppat\ev\evar$. Of course, when $\ppat$ is a variable $\evar$, the
generalized substitution $\subst\evar\ev$ is defined and coincides with the
substitution $\subst\evar\ev$, which justifies our abuse of notation. To
construct generalized substitutions, we use two simple combinators. First,
when $\dpv{\ppat_1}$ and $\dpv{\ppat_2}$ are disjoint,
$\subst{\ppat_1}{\ev_1}\gswedge\subst{\ppat_2}{\ev_2}$ stands for the
set-theoretic union of $\subst{\ppat_1}{\ev_1}$ and $\subst{\ppat_2}{\ev_2}$,
if both are defined, and is undefined otherwise. We use this combinator to
ensure that $\ppat_1$ matches $\ev_1$ and $\ppat_2$ matches $\ev_2$ and to
combine the two corresponding sets of bindings. Second, when $\eobj_1$ and
$\eobj_2$ are two possibly undefined mathematical objects that belong to the
same space when defined, $\eobj_1\gsvee\eobj_2$ stands for $\eobj_1$, if it is
defined, and for $\eobj_2$ otherwise---that is, $\gsvee$ is an angelic choice
operator with a left bias. In particular, when $\dpv{\ppat_1}$ and
$\dpv{\ppat_2}$ coincide, $\subst{\ppat_1}{\ev_1}\gsvee\subst{\ppat_2}{\ev_2}$
stands for $\subst{\ppat_1}{\ev_1}$, if it is defined, and for
$\subst{\ppat_2}{\ev_2}$ otherwise. We use this combinator to ensure that
$\ppat_1$ matches $\ev_1$ or $\ppat_2$ matches $\ev_2$ and to retain the
corresponding set of bindings. The full definition of generalized
substitutions, which relies on these combinators, appears in
Figure~\ref{fig:patterns}. It reflects the informal presentation of the
previous paragraph.

\begin{TTCOMPONENT}{Extended syntax and semantics of \MLcalc}{}
\ttlabel{patsem}\let \\ \TTSyntaxAlternative
\TTSyntaxCategoryNamed{\et}{\ldots}{Expressions}
\\ {\ematch\et\usualclauses}       {}

\TTSyntaxCategoryNamed{\ec}{\ldots}{Evaluation Contexts} 
\\ {\ematch\ec\usualclauses}       {}

\summaryhead{Reduction rules}

\divide \labelcolwidth by 3
\infax[R-Match]{
\ematch\ev\usualclauses
\reduces
\biggsvee\limits_{i=1}^k \subst{\ppat_i}\ev{\et_i}}\hskip -1em

\extraspacehack{.07in}
\end{TTCOMPONENT}

Once patterns and pattern matching are defined, it is straightforward to
extend the syntax and operational semantics of \MLcalc. We enrich the syntax
of expressions with a new construct, $\ematch\et\usualclauses$, where
$k\geq1$. It consists of a term $\et$ and a nonempty, ordered list of
clauses, each of which is composed of a pattern $\ppat_i$ and a term $\et_i$.
The syntax of evaluation contexts is extended as well, so that the term $\et$
that is being examined is first reduced to a value $\ev$. The operational
semantics is extended with a new rule, \Rule{R-Match}, which states that
$\ematch\ev\usualclauses$ reduces to $\subst{\ppat_i}\ev{\et_i}$, where $i$ is
the least element of $\{1,\ldots,k\}$ such that $\ppat_i$ matches $\ev_i$.
Technically, $\biggsvee_{i=1}^k \subst{\ppat_i}\ev{\et_i}$ stands for
$\subst{\ppat_1}\ev{\et_1}\gsvee\ldots\gsvee\subst{\ppat_k}\ev{\et_k}$, so
that the reduct is the first term that is defined in this sequence.

As far as semantics is concerned, the \kwmatch construct may be viewed as a
generalization of the \kwlet construct. Indeed, $\elet\evar{\et_1}{\et_2}$ may
now be viewed as syntactic sugar for $\ematch{\et_1}{\evar.\et_2}$, that is, a
\kwmatch construct with a single clause and a variable pattern. Then,
\Rule{R-Let} becomes a special case of \Rule{R-Match}.

It is pleasant to introduce some more syntactic sugar. We write
$\lambda\usualclauses$ for $\efun\evar{\ematch\evar\usualclauses}$, where
$\evar$ is fresh for $\usualclauses$. Thus, it becomes possible to define
functions by cases---a common idiom in \MLlang.
%
\begin{example}
\label{example-length-pat}
Using pattern matching, a function that computes the length of a list
(Example~\ref{example-length}) may now be written as follows:
%
\newcommand{\lgth}{{\kwtt{length}}}
\newcommand{\liste}{{\kwtt{l}}}
%
$$\sdletrec\lgth{\efunction{(
  \eapp\nil\wildpat . \hat0 \mid
  \eapp\cons{(\wildpat,\evar)} . \hat1\mathbin{\hat+}\eapp\lgth\evar
)}}$$
%
The second pattern matches a nonempty list and binds $\evar$ to its tail
at the same time, obviating the need for an explicit application of $\snd$.
\end{example}
%
\begin{exercise}[\Recommended, \Easy, \nosolution]
Under the above definition of \kwtt{length}, consider an application of
\kwtt{length} to the list $\cons(\hat0, \nil\eunit)$. After eliminating the
syntactic sugar, determine by which reduction sequence this expression reduces
to a value.
\end{exercise}

Before we can proceed and extend the type system to deal with the new \kwmatch
construct, we must make two mild extensions to the syntax and meaning of
constraints. First, if $\ts$ is $\scheme\tvars\co\ttyp$, where
$\disjoint\tvars{\ftv{\ttyp'}}$, then $\ccallbelow\ts{\ttyp'}$ stands for the
constraint $\exists\tvars.(\co\wedge\ttyp'\subtype\ttyp)$. This relation is
identical to the instance relation (Definition~\ref{def-ts-instance}), except
the direction of subtyping is reversed. We extend the syntax of constraints
with instantiation constraints of the form $\ccallbelow\evid\ttyp$ and define
their meaning by adding a symmetric counterpart of \Rule{CM-Instance}. We % TEMPORARY broken
note that, when subtyping is interpreted as equality, the relations
$\ccall\ts\ttyp$ and $\ccallbelow\ts\ttyp$ coincide, so this extension
is unnecessary in that particular case. Second, we extend the syntax of
environments so that several successive bindings may \emph{share} a set
of quantifiers and a constraint. That is, we allow writing
$\scheme\tvars\co{(\evid_1:\ttyp_1;\ldots;\evid_k:\ttyp_k)}$ for
$\evid_1:\scheme\tvars\co{\ttyp_1};\ldots;\evid_k:\scheme\tvars\co{\ttyp_k}$.
From a theoretical standpoint, this is little more than syntactic sugar;
however, in practice, it is useful to implement this new idiom literally,
since it avoids unnecessary copying of the constraint $\co$.
\label{shared-type-schemes}

\begin{widefigure}
\begin{bnf}
    \calcule\ttyp\wildpat
\eq \ctrue
\\
    \calcule\ttyp\evar
\eq \ccallbelow\evar\ttyp
\\
    \calcule\ttyp{\eapp[3]\econ{\ppat_1}\ldots{\ppat_k}}
\eq \exists\tvars.(\ccallbelow\econ{\tvarc\arw\ttyp} \wedge
                   \bigwedge_{i=1}^k\, \calcule{\tvar_i}{\ppat_i})
\\  
    \calcule\ttyp{\andpat{\ppat_1}{\ppat_2}}
\eq \calcule\ttyp{\ppat_1} \wedge
    \calcule\ttyp{\ppat_2}
\\  
    \calcule\ttyp{\orpat{\ppat_1}{\ppat_2}}
\eq \calcule\ttyp{\ppat_1} \wedge
    \calcule\ttyp{\ppat_2}
\end{bnf}%
$$\begin{array}{rcl}
\calcule{\ematch\et\usualclauses}\ttyp & = &
\bigwedge_{i=1}^k\, \cxlet
  {\scheme{\tvar\tvars_i}
    {\calcule\et\tvar \wedge
     \cxlet{\evarc_i:\tvarc_i}{\calcule\tvar{\ppat_i}}}
    {({\evarc_i:\tvarc_i})}}
  {\calcule{\et_i}\ttyp} \\
& & \text{where }\evarc_i=\dpv{\ppat_i}
\end{array}$$
\bcpcaption{patinf}{Constraint generation for patterns and pattern matching}
\end{widefigure}

Let us now extend the type system.  For the sake of brevity, we extend the
constraint generation rules only. Of course, it would also be possible to
define corresponding extensions of the rule-based type systems shown earlier,
namely \dm, \hmx, and \pcb. We begin by defining a constraint
$\calcule\ttyp\ppat$ that represents a necessary and sufficient condition for
values of type $\ttyp$ to be acceptable inputs for the pattern $\ppat$. Its
free type variables are a subset of $\ftv\ttyp$, while its free program
identifiers are either constructors or program variables bound by $\ppat$.
It is defined in the upper part of Figure~\ref{fig:patinf}.
%
The first rule states that a wildcard matches values of arbitrary type.  The
second and third rules govern program variables and constructor applications
in \emph{patterns}. They are identical to the rules that govern these
constructs in \emph{expressions} (page~\pageref{fig:genrules}), except that the
direction of subtyping is reversed. In the absence of subtyping, they would be
entirely identical. We write $\tvarc$ for $\tvar_1\ldots\tvar_k$ and
$\tvarc\arw\ttyp$ for $\tvar_1\arw\ldots\arw\tvar_k\arw\ttyp$. As usual, the
type variables $\tvar_1,\ldots,\tvar_k$ must have kind~$\normalkind$ and must
be distinct and fresh for the equation's left-hand side. The last two rules
simply distribute the type $\ttyp$ to both subpatterns. It is easy to check
that $\calcule\ttyp\ppat$ is \emph{contravariant} in $\ttyp$:
%
% On peut déjà noter à ce niveau que la règle de typage pour les
% constructeurs est tout à fait louche (la direction de la contrainte
% d'instanciation n'est pas renversée) et qu'en effet elle n'est valide
% que sous l'hypothèse d'invertibilité, clarifiée plus loin.
%
%
\begin{lemma}
\label{lemma-calcule-contravariant}
$\ttyp'\subtype\ttyp\wedge\calcule\ttyp\ppat$ entails $\calcule{\ttyp'}\ppat$.
\end{lemma}
%
This property reflects the fact that $\ttyp$ represents the type of an
\emph{input} for the pattern $\ppat$. Compare it with
Lemma~\ref{lemma-calcule-covariant}.
%
\begin{example}
\label{example-calcule-pat}
Consider the pattern $\eapp\cons{(\wildpat,\evar)}$, which appears in
Example~\ref{example-length-pat}. We have
$$\begin{array}{:rl:}
  & \calcule\ttyp{\eapp\cons{(\wildpat,\evar)}}
\\ \logeq
  & \exists\tzar_1.(
      \calcule{\tzar_1\arw\ttyp}\cons \wedge
      \calcule{\tzar_1}{(\wildpat,\evar)}
    )
\\ \logeq
  & \exists\tzar_1.(
      \ccallbelow\cons{\tzar_1\arw\ttyp} \wedge
      \exists\tzar_2\tzar_3.(
        \calcule{\tzar_2\arw\tzar_3\arw\tzar_1}{(\cdot,\cdot)} \wedge
        \calcule{\tzar_2}\wildpat \wedge
        \calcule{\tzar_3}\evar
      )
    )
\\ \logeq
  & \exists\tzar_1\tzar_2\tzar_3.(
      \ccallbelow\cons{\tzar_1\arw\ttyp} \wedge
      \ccallbelow{(\cdot,\cdot)}{\tzar_2\arw\tzar_3\arw\tzar_1} \wedge
      \ccallbelow\evar{\tzar_3}
    )
\end{array}$$
where $\tzar_1$, $\tzar_2$, $\tzar_3$ are fresh for $\ttyp$. Let us now place
this constraint within the scope of the initial environment, which assigns
type schemes to the constructors $\cons$ and $(\cdot,\cdot)$, and within
the scope of a binding of $\evar$ to some type $\ttyp'$. We find
$$\begin{array}{rl}
  & \cxlet\ienv{
      \cxlet{\evar:\ttyp'}{\calcule\ttyp{\eapp\cons{(\wildpat,\evar)}}}
    }
\\ \logeq
  & \exists\tzar_1\tzar_2\tzar_3.(
      \exists\tvar.(
        \tzar_1\arw\ttyp\subtype\tvar\times\tlist\tvar\arw\tlist\tvar
      ) \,\wedge
\\ & \phantom{\exists\tzar_1\tzar_2\tzar_3.(}
      \exists\twar_1\twar_2.(
        \tzar_2\arw\tzar_3\arw\tzar_1 \subtype
        \twar_1\arw\twar_2\arw\twar_1\times\twar_2
      ) \wedge
      \tzar_3\subtype\ttyp'
    )
\\ \logeq
   & \exists\tvar.(
       \ttyp\subtype\tlist\tvar \wedge \tlist\tvar\subtype\ttyp'
      )
\end{array}$$
where the final simplification relies mainly on \Rule{C-Arrow}, on the
corresponding rule for products, and on \Rule{C-ExTrans},
and is left as an exercise to the reader. Thus, the constraint states that the
pattern matches values that have type $\tlist\tvar$ (equivalently, values
whose type $\ttyp$ is a subtype of $\tlist\tvar$), for some undetermined
element type $\tvar$, and binds $\evar$ to values of type $\tlist\tvar$
(equivalently, values whose type $\ttyp'$ is a supertype of $\tlist\tvar$).
\end{example}

The above example seems to indicate that the constraint generation rules for
patterns make some sense. Still, the careful reader may be somewhat puzzled by
the third rule, which, compared to its analog for expressions, reverses the
direction of subtyping, but does not reverse the direction of instantiation.
Indeed, in order for this rule to make sense, and to be sound, we must
formulate a requirement concerning the type schemes assigned to constructors.
%
\begin{definition}
\label{def-invertible}
A constructor $\econ$ is \emph{invertible} if and only if, when $\tvarc$ and
$\tvarcp$ have length $\arity\econ$, the constraint
$\cxlet\ienv{(\ccallbelow\econ{\tvarcp\arw\ttyp} \wedge
  \ccall\econ{\tvarc\arw\ttyp})}$ entails $\tvarc\subtype\tvarcp$. In the
following, we assume patterns contain invertible constructors only.
\end{definition}
%
Intuitively, when $\econ$ is invertible, it is possible to recover the type of
every $\ev_i$ from the type of $\eapp[3]\econ{\ev_1}\ldots{\ev_k}$, a crucial
property for pattern matching to be possible. Note that, if
$\ienv(\econ)$ is monomorphic, then $\econ$ is invertible. The following lemma
identifies another important class of invertible constructors.
%
\begin{lemma}
\label{lemma-invertible}
The constructors of algebraic data types are invertible.
\end{lemma}
%
\begin{Proof}
Let $\econ$ be a constructor introduced by the definition of an algebraic data
type $\tnam$. Let $k=\arity\econ$. Then, the type scheme $\ienv(\econ)$ is of
the form $\dmscheme\twars{\ttypc\arw\tnam\,\twarc}$, where $\twarc$ are the
parameters of the definition and $\ttypc$, a vector of length $k$, consists of
\emph{some of} the definition's components. (More precisely, $\ttypc$ contains
just one component in the case of variant types and contains all components in
the case of record types.) Let $\tvarc$ and $\tvarcp$ have length $k$. Let
$\dmscheme{\twars_1}{\ttypc_1\arw\tnam\,\twarc_1}$ and
$\dmscheme{\twars_2}{\ttypc_2\arw\tnam\,\twarc_2}$ be two $\alpha$-equivalent
forms of the type scheme $\ienv(\econ)$, with $\disjoint{\twars_1}{\twars_2}$
and $\disjoint{\twars_1\twars_2}{\ftv{\tvars,\tvars',\ttyp}}$. The constraint
$\cxlet\ienv{(\ccallbelow\econ{\tvarcp\arw\ttyp} \wedge
  \ccall\econ{\tvarc\arw\ttyp})}$ is, by definition, equivalent to
$\ccallbelow{\ienv(\econ)}{\tvarcp\arw\ttyp} \wedge
\ccall{\ienv(\econ)}{\tvarc\arw\ttyp}$, that is,
$\exists\twars_1.(\tvarcp\arw\ttyp\subtype\ttypc_1\arw\tnam\,\twarc_1) \wedge
\exists\twars_2.(\ttypc_2\arw\tnam\,\twarc_2\subtype\tvarc\arw\ttyp)$. By
\Rule{C-ExAnd} and \Rule{C-Arrow}, this may be written
$\exists\twars_1\twars_2.(\tnam\,\twarc_2\subtype\ttyp\subtype\tnam\,\twarc_1
\wedge \tvarc\subtype\ttypc_2 \wedge \ttypc_1\subtype\tvarcp)$. Now, by
Definition~\ref{def-algebraic-variance},
$\tnam\,\twarc_2\subtype\tnam\,\twarc_1$ entails $\ttypc_2\subtype\ttypc_1$,
so the previous constraint entails
$\exists\twars_1\twars_2.(\tvarc\subtype\tvarcp)$, that is,
$\tvarc\subtype\tvarcp$.
\end{Proof}

An important class of \emph{noninvertible} constructors are those associated
with existential type definitions (page~\pageref{page-existential-types}),
where not all quantifiers of the type scheme $\ienv(\econ)$ are parameters of
the type constructor $\tnam$. For instance, under the definition
$\datadef\tnam{}{\dc : \exists\tvar.\tvar}$, the type scheme associated with
$\dc$ is $\dmscheme\tvar{\tvar\arw\tnam}$. Then, it is easy to check that
$\dc$ is not invertible. This reflects the fact that it is not possible to
recover the type of $\ev$ from the type of $\eapp\dc\ev$---which must be
$\tnam$ in any case---and explains why existential types require special
treatment.

We are now ready to associate a constraint generation rule with the \kwmatch
construct. It is given in the lower part of Figure~\ref{fig:patinf}. In the
rule's right-hand side, we write $\evarc_i$ for the program variables bound by
the pattern $\ppat_i$, and we write $\tvarc_i$ for a vector of type variables
of the same length. The type variables $\tvar\tvars_i$ must have
kind~$\normalkind$, must be pairwise distinct and must not appear free in the
rule's left-hand side. Let us now explain the rule. Its right-hand side is a
conjunction, where each conjunct deals with one clause of the \kwmatch
construct, requiring $\et_i$ to have type $\ttyp$ under certain assumptions
about the program variables $\evarc_i$ bound by the pattern $\ppat_i$.  There
remains to explain how these assumptions are built. First, as in the case of a
\kwlet construct, we summon a fresh type variable $\tvar$ and produce
$\calcule\et\tvar$, the least specific constraint that guarantees $\et$ has
type $\tvar$. Then, reflecting the operational semantics, which feeds (the
value produced by) $\et$ into the pattern $\ppat_i$, we feed the type $\tvar$
into $\ppat_i$ and produce
$\cxlet{\evarc_i:\tvarc_i}{\calcule\tvar{\ppat_i}}$, a constraint that
guarantees that $\tvarc_i$ is a correct vector of type assumptions for the
program variables $\evarc_i$ (see Example~\ref{example-calcule-pat}).  This
explains why we may place $\calcule\ttyp{\et_i}$ within the scope of
$(\evarc_i:\tvarc_i)$. There remains to point out that, as in the case of the
\kwlet construct, \emph{every} assignment of ground types to $\tvar\tvars_i$
that satisfies the constraint $\calcule\et\tvar \wedge
\cxlet{\evarc_i:\tvarc_i}{\calcule\tvar{\ppat_i}}$ is acceptable, so it is
valid to universally quantify these type variables. This allows the program
variables $\evarc_i$ to receive polymorphic type schemes when $\et$ itself has
polymorphic type.
%
\begin{exercise}[\Recommended, \QuickCheck]
We have previously suggested viewing $\elet\evar{\et_1}{\et_2}$ as syntactic
sugar for $\ematch{\et_1}{\evar.\et_2}$, and shown that the operational
semantics validates this view. Check that it is also valid from a typing
perspective. \solref{let-match}
\end{exercise}

The \kwmatch constraint generation rule, if implemented literally, takes $k$
copies of the constraint $\calcule\et\tvar$. When $k$ is greater than 1, this
compromises the linear time and space complexity of constraint generation. To
remedy this problem, one may modify the rule as follows: replace every copy of
$\calcule\et\tvar$ with $\ccall\evar\tvar$ and place the constraint within the
context $\cxlet{\evar:\scheme\tvar{\calcule\et\tvar}\tvar}\chole$, where
$\evar$ is a fresh program variable. It is not difficult to check that the
logical meaning of the constraint is not affected and that a linear behavior is
recovered. In practice, solving the new constraint requires taking instances
of the type scheme $\scheme\tvar{\calcule\et\tvar}\tvar$, which essentially
requires copying $\calcule\et\tvar$ again---however, an efficient solver may
now \emph{simplify} this subconstraint before duplicating it.

The following lemma is a key to establishing subject reduction for
\Rule{R-Match}. It relies on the requirement that constructors be invertible.
%
\begin{lemma}
\label{lemma-basic-matching}
Assume $\subst\ppat\ev$ is defined and maps $\evarc$ to $\ewc$, where
$\evars=\dpv\ppat$. Let $\evarc:\ttypc$ be an arbitrary monomorphic
environment of domain $\evars$. Then, $\cxlet\ienv{(\calcule\ev\ttyp \wedge
  \cxlet{\evarc:\ttypc}{\calcule\ttyp\ppat})}$ entails
$\cxlet\ienv{\calcule\ewc\ttypc}$.
\end{lemma}
%
\begin{full}
\begin{Proof}
The proof is by induction on the structure of $\ppat$.

\proofcase $\wildpat$. The goal is the tautology
$\cxlet\ienv{\calcule\ev\ttyp}\entails\cxlet\ienv\ctrue$.

\proofcase $\evar$. The goal is $\cxlet\ienv{(\calcule\ev\ttyp \wedge
  \cxlet{\evar:\ttyp'}{\ccallbelow\evar\ttyp})} \entails
\cxlet\ienv{\calcule\ev{\ttyp'}}$, that is, $\cxlet\ienv{(\calcule\ev\ttyp
  \wedge \ttyp\subtype\ttyp')} \entails \cxlet\ienv{\calcule\ev{\ttyp'}}$,
which follows from Lemma~\ref{lemma-calcule-covariant}.

\demoreset\proofcase $\eapp[3]\econ{\ppat_1}\ldots{\ppat_k}$. Any value
matched by this pattern must be of the form
$\eapp[3]\econ{\ev_1}\ldots{\ev_k}$, so the goal is to show that
$\cxlet\ienv{(\calcule{\eapp[3]\econ{\ev_1}\ldots{\ev_k}}\ttyp \wedge
  \cxlet{\evarc:\ttypc}
  {\calcule\ttyp{\eapp[3]\econ{\ppat_1}\ldots{\ppat_k}}})}$~\dlabel{c} entails
$\cxlet\ienv{\calcule\ewc\ttypc}$. By definition of constraint generation and
by 
\Rule{C-ExAnd}, \Rule {C-In*}, \Rule {C-InAnd}, \Rule {C-InAnd*}, \Rule
{C-InEx}, 
and \dref{c} is equivalent to
%
$\cxlet\ienv{\exists\tvars\tvars'.(
  \ccallbelow\econ{\tvarcp\arw\ttyp} \wedge
  \ccall\econ{\tvarc\arw\ttyp} \wedge \bigwedge_{i=1}^k\,
  (\calcule{\ev_i}{\tvar_i} \wedge
  \cxlet{\evarc_i:\ttypc_i}{\calcule{\tvar'_i}{\ppat_i}}))}$,
%
where $\tvars$ and $\tvars'$ are fresh, $\tvarc$ (respectively\ $\tvarcp$) is short
for $\tvar_1\ldots\tvar_k$ (respectively\ $\tvar'_1\ldots\tvar'_k$), and
$\evarc_i:\ttypc_i$ is the fragment of $\evarc:\ttypc$ whose domain is
$\dpv{\ppat_i}$. By Definition~\ref{def-invertible}, this entails
%
$\cxlet\ienv{\exists\tvars\tvars'.
  \bigwedge_{i=1}^k\,
  (\calcule{\ev_i}{\tvar_i} \wedge \tvar_i\subtype\tvar'_i \wedge
  \cxlet{\evarc_i:\ttypc_i}{\calcule{\tvar'_i}{\ppat_i}})}$, which
by Lemma~\ref{lemma-calcule-exists} is
$\cxlet\ienv{\exists\tvars'.
  \bigwedge_{i=1}^k\,
  (\calcule{\ev_i}{\tvar'_i} \wedge
  \cxlet{\evarc_i:\ttypc_i}{\calcule{\tvar'_i}{\ppat_i}})}$~\dlabel{d}.
%
Applying the induction hypothesis for every $i\in\{1,\ldots,k\}$, we
find that \dref{d} entails
$\cxlet\ienv{\exists\tvars'.
  \bigwedge_{i=1}^k\, \calcule{\ewc_i}{\ttypc_i}}$,
where $\ewc_i$ is the fragment of $\ewc$ associated with $\evarc_i$,
that is, $\cxlet\ienv{\calcule\ewc\ttypc}$.

\proofcases $\andpat{\ppat_1}{\ppat_2}$ and $\orpat{\ppat_1}{\ppat_2}$. By
the induction hypothesis, \Rule {C-In*}, \Rule{C-InAnd}, and \Rule{C-InAnd*}.
\end{Proof}
\end{full}

We now prove that our extension of \MLcalc with pattern matching enjoys
subject reduction. We only state that \Rule{R-Match} preserves types, and
leave the new subcase of \Rule{R-Context}, where the evaluation context
involves a \kwmatch construct, to the reader. For this subcase to succeed, the
value restriction (Definition~\ref{def-value-restriction}) must be extended to
require that either all constants have pure semantics or all \kwmatch
constructs are in fact of the form $\ematch\ev\usualclauses$.
%
\begin{theorem}[Subject reduction]
\label{theorem-sr-pat}
$(\Rule{R-Match})\subseteq(\sr)$.
\end{theorem}
%
\begin{full}
\begin{Proof}
\demoreset
The goal is to prove that
$\cxlet\ienv{\calcule{\ematch\ev\usualclauses}\ttyp}$ entails
$\cxlet\ienv{\calcule{\biggsvee_{i=1}^k \subst{\ppat_i}\ev{\et_i}}\ttyp}$.
It is clear that the latter is entailed by
$\cxlet\ienv{\bigwedge_{i=1}^k\, \calcule{\subst{\ppat_i}\ev{\et_i}}\ttyp}$.
Because the former is also, by definition of constraint generation, a
conjunction of $k$ conjuncts, we may decompose the goal on a per-clause
basis and abandon the index $i$. The goal becomes, assuming that
$\subst\ppat\ev$ is defined, to prove that
%
$\cxlet\ienv{\cxlet
    {\scheme{\tvar\tvars}
      {\calcule\ev\tvar \wedge
       \cxlet{\evarc:\tvarc}{\calcule\tvar\ppat}}
      {({\evarc:\tvarc})}}
    {\calcule\et\ttyp}}$~\dlabel{h}
%
entails $\cxlet\ienv{\calcule{\subst\ppat\ev\et}\ttyp}$~\dlabel{g}, where
$\evarc$ are the program variables bound by the pattern $\ppat$. The
substitution $\subst\ppat\ev$ must be of the form $\subst\evarc\ewc$.
By Lemma~\ref{lemma-basic-matching}, \dref{h} entails
%
$\cxlet\ienv{\cxlet
    {\scheme\tvars
      {\calcule\ewc\tvarc}
      {({\evarc:\tvarc})}}
    {\calcule\et\ttyp}}$~\dlabel{i}.
%
Let us assume, \spdg, $\disjoint\evars{\fpv\ev}$, which implies
$\disjoint\evars{\fpv\ewc}$. By definition of shared quantifiers
and constraints within environments (page~\pageref{shared-type-schemes}),
by \Rule {C-Ex*}, \Rule{C-LetEx}, \Rule{C-LetAnd}, and \Rule{C-Dup},
\dref{i} is found 
to be equivalent to
%
$\cxlet\ienv{\cxlet
    {\overrightarrow{\evar:\scheme\tvar{\calcule\ew\tvar}\tvar}}
    {\calcule\et\ttyp}}$~\dlabel{j}.
%
By repeated application of Lemma~\ref{lemma-substitution-inference},
\dref{j} entails
$\cxlet\ienv{\calcule{\subst\evarc\ewc\et}\ttyp}$, that is, \dref{g}.
\end{Proof}
\end{full}
%
\begin{exercise}[\Moderate, \nosolution]
For the sake of simplicity, we have omitted the production $\eref\ppat$ from
the syntax of patterns. The pattern $\eref\ppat$ matches every memory location
whose content (with respect to the current store) is matched by $\ppat$.
Determine how the previous definitions and proofs must be extended in order
to accommodate this new production.
\end{exercise}

The progress property does \emph{not} hold in general: for instance,
$\ematch\nil{(\eapp\cons\evar.\evar)}$ is well-typed (with type
$\dmscheme\tvar\tvar$) but is stuck. In actual implementations of \MLlang,
such errors are dynamically detected. This may be considered a weakness of
\MLtype. Fortunately, however, it is often possible to statically prove that a
particular \kwmatch construct is \emph{exhaustive} and cannot go wrong.
Indeed, if $\ematch\ev\usualclauses$ is well-typed, then for every
$i\in\{1,\ldots,k\}$, the constraint $\cxlet\ienv{(\calcule\ev\tvar\wedge
\exists\tvars.\cxlet{\evarc_i:\tvarc}{\calcule\tvar{\ppat_i}})}$, where
$\evars_i$ are the program variables bound by $\ppat_i$, must be satisfiable;
that is, $\ev$ must have some type that is an acceptable input for $\ppat_i$.
%
% TEMPORARY
% Les choses sont un peu compliquées par le fait que j'ai autorisé v à être
% généralisée. Sans cela, v aurait un type acceptable *simultanément* pour
% tous les motifs, ce qui n'est pas le cas ici.
%
This fact yields information about $\ev$, from which it may be possible to
derive that $\ev$ must match one of the patterns $\ppat_i$.
%
\begin{example}
Let $k=2$, $\ppat_1=\eapp\nil\wildpat$, and
$\ppat_2=\eapp\cons{(\evar_1,\evar_2)}$. Then, the constraints
$\cxlet\ienv{\exists\tvars.\cxlet{\evarc_i:\tvarc}{\calcule\tvar{\ppat_i}}}$,
for $i\in\{1,2\}$, are both equivalent (after simplification, when $i=2$) to
$\exists\tzar.\tvar\subtype\tlist\tzar$. Because the type constructor
$\tclist$ is isolated, every closed value $\ev$ whose type $\tvar$ satisfies
this constraint must be an application of $\nil$ or $\cons$. If the latter,
because $\cons$ has type
$\dmscheme\tvar{\tvar\times\tlist\tvar\arw\tlist\tvar}$, and because the type
constructor $\times$ is isolated, the argument to $\cons$ must be a pair.  We
conclude that $\ev$ must match either $\ppat_1$ or $\ppat_2$, which guarantees
that this \kwmatch construct is exhaustive and its evaluation cannot go wrong.
\end{example}

It is beyond the scope of \this to give more details about the check for
exhaustiveness. The reader is referred
to \longcite{sekar-al-95} and \longcite{LeFessantMaranget01}.

% TEMPORARY quels sont les papiers de base à citer? Augustsson, Cardelli,
% Maranget...
% On peut aussi citer des papiers proposant des notions de patterns plus
% avancées (XDuce, spatial logics de Cardelli, ...)

% -----------------------------------------------------------------------------

\end{FULL}


\begin{FULL}

\subsection*{Type Annotations}
\label{page-type-annotations}

So far, we have been interested in a very pure, and extreme, form of type
inference. Indeed, in \MLcalc, expressions contain no explicit type information
whatsoever: it is entirely inferred. In practice, however, it is often useful
to insert \emph{type annotations} within expressions, because they provide a
form of machine-checked documentation. Type annotations are also helpful when
attempting to trace the cause of a type error: by supplying the typechecker
with (supposedly) correct type information, one runs a better chance of
finding a type inconsistency near an actual programming mistake.

When type annotations are allowed to contain type variables, one must be quite
careful about \emph{where} (at which program point) and \emph{how}
(existentially or universally) these variables are bound. Indeed, the meaning
of type annotations cannot be made precise without settling these issues. In
what follows, we first explain how to introduce type annotations whose type
variables are bound \emph{locally} and existentially. We show that extending
\MLcalc with such limited type annotations is again a simple matter of
introducing new constants. Then, we turn to a more general case, where type
variables may be explicitly existentially introduced \emph{at any program
point}. We defer the discussion of \emph{universally} bound type variables
to \S\ref{section-univ}.

% TEMPORARY?
% Because \MLlang does not have subtyping, we will consider annotations
% by types only, as opposed to annotations by types and constraints.

Let a \emph{local existential type annotation} $\exists\tvars.\ttyp$ be a pair
of a set of type variables $\tvars$ and a type $\ttyp$, where $\ttyp$ has kind
$\normalkind$, $\tvars$ is considered bound within $\ttyp$, and $\tvars$
contains $\ftv\ttyp$. For every such annotation, we introduce a new unary
destructor $\ann\cdot$. Such a definition is valid only because a type
annotation must be \emph{closed}, that is, does not have any free type
variables. We write $\ann\et$ for the application
$\eapp{\ann\cdot}\et$. Since a type annotation does not affect the meaning
of a program, the new destructor has identity semantics:
%
\infax[R-Annotation]{
\ann\ev \reduces[\delta] \ev
}
%
Its type scheme, however, is not that of the identity, namely
$\dmscheme\tvar{\tvar\arw\tvar}$: instead, it is less general,
so that annotating an expression \emph{restricts} its type.
Indeed, we extend the initial environment $\ienv$ with the binding
%
$$\ann\cdot : \dmscheme\tvars{\ttyp\arw\ttyp}$$
%
%
\begin{exercise}[\QuickCheck]
Check that $\dmscheme\tvars{\ttyp\arw\ttyp}$ is an \emph{instance} of
$\dmscheme\tvar{\tvar\arw\tvar}$ in Damas and Milner's sense, that is,
the former is obtained from the latter via the rule \Rule{dm-Inst'}
given in Exercise~\ref{ex:diffdm}. Does this allow arguing that the
type scheme assigned to $\ann\cdot$ is sound? Check that the above
definitions meet the requirements of Definition~\ref{def-requirements}.
\solref{simpleannot}
\end{exercise}

Although inserting a type annotation does not change the semantics of the
program, it does affect constraint generation, hence type inference. We
let the reader check that, assuming
$\disjoint\tvars{\ftv{\et,\ttyp'}}$, the following derived constraint
generation rule holds:
%
$$\cxlet\ienv{\calcule{\ann\et}{\ttyp'}} \logeq
\cxlet\ienv{\exists\tvars.(\calcule\et\ttyp\wedge\ttyp\leq\ttyp')}$$
%
So far, expressions cannot have free type variables, so the hypothesis
$\disjoint\tvars{\ftv\et}$ may seem superfluous. However, we shall soon
allow expressions to contain type annotations with free type variables,
so we prefer to make this condition explicit now.
%
According to this rule, the effect of the type annotation is to force the
expression $\et$ to have type $\ttyp$, for \emph{some} choice of the type
variables $\tvars$. As usual in type systems with subtyping, the expression's
final type $\ttyp'$ may then be an arbitrary supertype of this particular
instance of $\ttyp$. When subtyping is interpreted as equality, $\ttyp'$ and
$\ttyp$ are equated by the constraint, so this constraint generation rule may
be read: \emph{a valid type for $\ann\et$ must be of the form $\ttyp$, for
  some choice of the type variables $\tvars$.}
%
\begin{example}
In \dm extended with integers, the expression
$\eannot{\efun\evar\evar}{\tint\arw\tint}$ has most general type
$\tint\arw\tint$, even though the underlying identity function has most
general type $\dmscheme\tvar{\tvar\arw\tvar}$, so the annotation restricts its
type. The expression
$\eannot{\efun\evar{\evar\mathbin{\hat+}\hat1}}{\exists\tvar.\tvar\arw\tvar}$ has type
$\tint\arw\tint$, which is also the most general type of the underlying
function, so the annotation acts merely as documentation in this case. Note
that the type variable $\tvar$ is instantiated to $\tint$ by the constraint
solver. The expression
$\eannot{\efun\evar{(\eapp\evar{\hat1})}}{\exists\tvar.\tvar\arw\tint}$ has
type $(\tint\arw\tint)\arw\tint$ because the underlying function has type
$(\tint\arw\twar)\arw\twar$, which successfully unifies with $\tvar\arw\tint$
by instantiating $\tvar$ to $\tint\arw\tint$ and $\twar$ to $\tint$.  Last,
the expression
$\eannot{\efun\evar{(\eapp\evar{\hat1})}}{\exists\tvar.\tint\arw\tvar}$ is
ill-typed---even though the underlying expression is well-typed---because the
equation $(\tint\arw\twar)\arw\twar=\tint\arw\tvar$ is unsatisfiable.
\end{example}
%
\begin{example}
\label{example-local-annotations}
\newcommand{\triv}[1]{\eannot{#1}{\exists\tvar.\tvar}}
\newcommand{\triw}[1]{\eannot{#1}{\exists\twar.\twar}}
In \dm extended with pairs, the expression
$\efun{\evar_1}{\efun{\evar_2}{(\triv{\evar_1},\triv{\evar_2})}}$
has most general type
$\dmscheme{\tvar\twar}{\tvar\arw\twar\arw\tvar\times\twar}$. In
other words, the two occurrences of $\tvar$ do not represent the
same type. Indeed, one could just as well have written
$\efun{\evar_1}{\efun{\evar_2}{(\triv{\evar_1},\triw{\evar_2})}}$.
If one wishes $\evar_1$ and $\evar_2$ to receive the same type,
one must lift the type annotations and merge them above the pair
constructor, as follows:
$\efun{\evar_1}{\efun{\evar_2}{
\eannot{(\evar_1,\evar_2)}{\exists\tvar.\tvar\times\tvar}}}$.
In the process, the type constructor
$\times$ has appeared in the annotation, causing its size to increase.
\end{example}

The above example reveals a limitation of this style of type annotations: by
requiring every type annotation to be closed, we lose the ability for two
separate annotations to \emph{share} a type variable. Yet, such a feature is
sometimes desirable. If the two annotations where sharing is desired are
distant in the code, it may be awkward to lift and merge them into a single
annotation; so, more expressive power is sometimes truly needed.

Thus, we are lead to consider more general type annotations, of the form
$\eannot\et\ttyp$, where $\ttyp$ has kind $\normalkind$, and where the type
variables that appear within $\ttyp$ are considered \emph{free}, so that
distinct type annotations may refer to shared type variables. For this idea to
make sense, however, it is still necessary to specify where these type
variables are bound. We do so using expressions of the form
$\exists\tvars.\et$. Such an expression binds the type variables $\tvars$
within the expression $\et$, so that all free occurrences of $\tvar$ (where
$\tvar\in\tvars$) in type annotations inside $\et$ stand for the same type.
Thus, we break the simple type annotation construct $\ann\cdot$ into two more
elementary constituents, namely \emph{existential type variable introduction}
$\exists\tvars.\cdot$ and \emph{type constraint} $\eannot\cdot\ttyp$. Note
that both are new forms of expressions; neither can be encoded by adding new
constants to the calculus, because it is not possible to assign \emph{closed}
type schemes to them.

Technically, allowing expressions to contain type variables requires some
care. Several constraint generation rules employ auxiliary type variables,
which become bound in the generated constraint. These type variables may be
chosen in an arbitrary way, provided they do not appear free in the rule's
left-hand side---a side-condition intended to avoid inadvertent capture.
% TEMPORARY faire référence à la déf. des règles
So far, this side-condition could be read: the auxiliary type variables used to
form the constraint $\calcule\et\ttyp$ must not appear free within $\ttyp$.
Now, since type annotations may contain free type variables, the
side-condition becomes: \emph{the auxiliary type variables used to form
  $\calcule\et\ttyp$ must not appear free within $\et$ or $\ttyp$.}

With this extended side-condition in mind, our original constraint
generation rules remain unchanged. We add two new rules to describe
how the new expression forms affect constraint generation:
%
\begin{bnf}
\calcule{\exists\tvars.\et}\ttyp
\eq
\exists\tvars.\calcule\et\ttyp 
& \text{provided $\disjoint\tvars{\ftv\ttyp}$}
\\
\calcule{\eannot\et\ttyp}{\ttyp'}
\eq
\calcule\et\ttyp \wedge \ttyp\subtype\ttyp'
\end{bnf}%
%
The effect of these rules is simple. The construct $\exists\tvars.\et$ is an
indication to the constraint generator that the type variables $\tvars$, which
may occur free within type annotations inside $\et$, should be existentially
bound at this point. The side-condition $\disjoint\tvars{\ftv\ttyp}$ ensures
that quantifying over $\tvars$ in the generated constraint does not capture
type variables in the expected type $\ttyp$. It can always be satisfied by
\aconv of the expression $\exists\tvars.\et$. The construct
$\eannot\et\ttyp$ is an indication to the constraint generator that the
expression $\et$ should have type $\ttyp$, and it is treated as such by
generating the subconstraint $\calcule\et\ttyp$. The expression's type may be
an arbitrary supertype of $\ttyp$, hence the auxiliary constraint
$\ttyp\subtype\ttyp'$.
%
\begin{example}
In \dm extended with pairs, the expression
$\efun{\evar_1}{\efun{\evar_2}{\exists\tvar.(\eannot{\evar_1}\tvar,\eannot{\evar_2}\tvar)}}$
has most general type $\dmscheme\tvar{\tvar\arw\tvar\arw\tvar\times\tvar}$.
Indeed, the constraint generated for this expression contains the pattern
$\exists\tvar.(\calcule{\evar_1}\tvar \wedge \calcule{\evar_2}\tvar \wedge
\ldots)$, which causes $\evar_1$ and $\evar_2$ to receive the same type.
Note that this style is more flexible than that employed in
Example~\ref{example-local-annotations}, where we were forced to use
a single, monolithic type annotation to express this sharing constraint.
\end{example}
%
\begin{remark}
In practice, a type variable is usually represented as a memory cell in the
typechecker's heap. So, one cannot say that the source code contains type
variables; rather, it contains \emph{names} that are meant to stand for type
variables. Let us write $\utvar$ for such a name, and $\uttyp$ for a type made
of type constructors and names, rather than of type constructors and type
variables. Then, our new expression forms are really $\exists\utvars.\et$ and
$\eannot\et\uttyp$. When the constraint generator enters the scope of an
introduction form $\exists\utvars.\et$, it allocates a vector of fresh type
variables $\tvars$, and augments an internal environment with the bindings
$\utvars\mapsto\tvars$. Because the type variables are fresh, the
side-condition of the first constraint generation rule above is automatically
satisfied. When the constraint generator finds a type annotation
$\eannot\et\uttyp$, it looks up the internal environment to translate the type
annotation $\uttyp$ into an internal type $\ttyp$---which fails if $\uttyp$
contains a name that is not in scope---and applies the second constraint
generation rule above.
% TEMPORARY Citer peyton-jones-shields-03.
\end{remark}
%
\begin{exercise}[\Easy, \nosolution]
Let $\tvars\supseteq\ftv\ttyp$ and $\disjoint\tvars{\ftv\et}$. Check that the
constraints $\calcule{\ann\et}{\ttyp'}$ and
$\calcule{\exists\tvars.\eannot\et\ttyp}{\ttyp'}$ are equivalent.
In other words, the \emph{local} type annotations introduced earlier may
be expressed in terms of the more complex constructs described above.
\end{exercise}
% TEMPORARY en écrivant le corrigé, attention aux renommages \spdg!
% il faut obtenir \tvars # \ttyp' \spdg. cf orrigé de ex:univsame
%
\begin{exercise}[\Easy, \nosolution]
\label{exercise-erasure}
One way of giving identity semantics to our new type annotation constructs is
to \emph{erase} them altogether prior to execution. Give an inductive
definition of $\erasure\et$, the expression obtained by removing all type
annotation constructs from the expression $\et$. Check that $\calcule\et\ttyp$
entails $\calcule{\erasure\et}\ttyp$ and explain why this is sufficient to
ensure type soundness.
\end{exercise}
% TEMPORARY on devrait spécifier aussi que la value restriction s'étend
% en considérant qu'on peut généraliser si l'effacement de l'expression
% considérée est une valeur?

It is interesting to study how explicit introduction of existentially
quantified type variables interacts with \letpoly. The source of their
interaction lies in the difference between the constraints
$\cxlet{\evar:\scheme\tvars{\exists\tvar.\co_1}\ttyp}{\co_2}$ and
$\exists\tvar.\cxlet{\evar:\scheme\tvars{\co_1}\ttyp}{\co_2}$, which was
explained in Example~\ref{example-let-left-ex}.
In the former constraint, every free occurrence of $\evar$ inside $\co_2$
causes a copy of $\exists\tvar.\co_1$ to be taken, thus creating its own fresh
copy of $\tvar$. In the latter constraint, on the other hand, every free
occurrence of $\evar$ inside $\co_2$ produces a copy of $\co_1$. All such
copies \emph{share} references to $\tvar$, because its quantifier was not
duplicated. In the former case, one may say that the type scheme assigned to
$\evar$ is \emph{polymorphic} with respect to $\tvar$, while in the latter
case it is \emph{monomorphic}. As a result, the placement of type variable
introduction expressions with respect to \kwlet bindings in the source
code is meaningful: introducing a type variable \emph{outside} of a \kwlet
construct \emph{prevents} it from being generalized.
%
\begin{example}
\label{example-let-annotations}
In \dm extended with integers and Booleans, the program
$\elet\efar{\exists\tvar.\efun\evar{\eannot\evar\tvar}}
{(\eapp\efar0,\eapp\efar\etrue)}$
is well-typed. Indeed, the type scheme assigned to $\efar$ is
$\dmscheme\tvar{\tvar\arw\tvar}$. However, the program
$\exists\tvar.\elet\efar{\efun\evar{\eannot\evar\tvar}}
{(\eapp\efar0,\eapp\efar\etrue)}$ is ill-typed.
Indeed, the type scheme assigned to $\efar$ is
$\tvar\arw\tvar$; then, no value of $\tvar$ satisfies
the constraints associated with the applications
$\eapp\efar0$ and $\eapp\efar\etrue$.
%
The latter behavior is observed in Objective Caml, where type variables are
\emph{implicitly} introduced at the outermost level of expressions:
#{&}
  # let f z = (z:'a) in (f 0, f true);;
  This expression has type bool but is here used with type int
#{@}
More details about the treatment of type annotations in Standard ML, Objective
Caml, and Haskell are given on page~\pageref{page-disc-annotations}.
\end{example}
%
\begin{exercise}[\QuickCheck, \nosolution]
Determine which constraints are generated for the two programs in
Example~\ref{example-let-annotations}. Check that the former is indeed
well-typed, while the latter is ill-typed.
\end{exercise}
%
\begin{comment}
\begin{exercise}[A relaxed value restriction, \Challenging, \nosolution]
\label{exercise-relax}
We pointed out earlier that the value restriction introduced in
Definition~\ref{def-value-restriction} is often too restrictive. We now
present a slightly more permissive variant of the value restriction that is
still very easy to implement. The statement of the relaxed restriction makes
use of the type annotation constructs. It is as follows: a program satisfies
the restriction if and only if \emph{every application expression is annotated
  with a type whose free variables are bound \emph{outside} of the nearest
  enclosing $\lambda$-abstraction, if there is one, and at the top level
  otherwise.}  Thus, the program fragment
$$\elet\erar{\exists\tvar.\eannot{\eref{\efun\evar\evar}}\tvar}\et$$
is
illegal, and rightly so, since it is equivalent, with respect to typing and
evaluation, to $\elet\erar{\eref{\efun\evar\evar}}\et$, which we know is
unsound.  The program fragment
$$\elet\efar{\exists\tvar.\efun\evar{
    \elet\erar{\eannot{\eref{\efun\evar\evar}}\tvar}\et }}\ldots$$
is legal.
As in Example~\ref{example-let-annotations}, annotating the application
$\eref{\efun\evar\evar}$ prevents $\erar$ from receiving a polymorphic type
scheme, because the type variable $\tvar$ is introduced outside of the scope
of the inner \kwlet construct. The program variable $\efar$, on the other
hand, may receive a type scheme in which $\tvar$ has been generalized, but
that is safe, since $\efar$ is being bound to a $\lambda$-abstraction, that
is, a value. In fact, this program fragment is valid under both the strict and
relaxed forms of the value restriction. Here is a program fragment that is
valid under the relaxed restriction only:
$$\exists\tvar\twar_1\twar_2.
\elet{(\erar,\efar)}{(\eannot{\eref{\efun\evar\evar}}\tvar,\efun\evar\evar)}
{(\eannot{\eapp\efar0}{\twar_1},\eannot{\eapp\efar\etrue}{\twar_2})}$$
% TEMPORARY j'ai utilisé le pattern matching.
As above, the type annotation prevents $\erar$ from receiving a polymorphic
type scheme. However, nothing prevents $\efar$ from receiving the type scheme
$\dmscheme\tzar{\tzar\arw\tzar}$, which is why the whole expression is
well-typed. This expression is invalid under the strict value restriction
because $(\eref{\efun\evar\evar},\efun\evar\evar)$ is not a value. \emph{Show
  that every program that is valid under the strict value restriction is also
  valid under the relaxed restriction.}

In practice, of course, we do not require every application to be explicitly
annotated. It is easy for the constraint generator to simulate the effect of
annotating every application with a distinct type variable $\tvar$ and of
binding $\tvar$ immediately outside the nearest enclosing
$\lambda$-abstraction, if there is one, and at the top level otherwise.

We strongly conjecture that no program that is well-typed under the relaxed
value restriction can go wrong. \emph{Can you prove this conjecture?} Here is
a roadmap. Define an operational semantics that preserves type annotations
throughout reduction, as opposed to the type-erasure semantics used in
Exercise~\ref{exercise-erasure}. Find a formulation of the relaxed value
restriction that is preserved by reduction. 
% TEMPORARY ça serait bien de la trouver nous-mêmes!
Last, reformulate the last
subcase in the proof of Theorem~\ref{subject-reduction} under the relaxed
value restriction: show that the type variables $\twars$ that describe newly
allocated store cells are \emph{determined} by the context, and use
\Rule{C-LetAll} to hoist the $\exists\twars$ quantifier out of the \kwd{let}
constraint. % TEMPORARY flou
\end{exercise}
% TEMPORARY on peut mentionner que sous la restriction relâchée, le let
% n'est plus équivalent à une simple expansion textuelle.
\end{comment}

% TEMPORARY pointer vers annotations avec vars. quantifiées universellement

% TEMPORARY noter qu'on n'a pas abordé la question des annotations bornées

% -----------------------------------------------------------------------------

\end{FULL}

\index{algebraic data types and ML type inference|)}
\index{data types and ML type inference|)}
\index{type inference!and algebraic data types|)}

\subsection*{Recursive Types}
\label{section-recursive-types}

We have shown that specializing \hmx with an equality-only syntactic model
yields \hmeq, a constraint-based formulation of Damas and Milner's type
system. Similarly, it is possible to specialize \hmx with an equality-only
free \emphfull{regular} tree model, yielding a constraint-based type system that
may be viewed as an extension of Damas and Milner's type discipline with
recursive types. This flavor of recursive types is sometimes known as
\index{equirecursive types}\emph{equirecursive}, since cyclic \emph{equations},
such as $\tvar=\tvar\arw\tvar$, are then satisfiable. Our theorems about type
inference and type soundness, which are independent of the model, remain
valid. The constraint solver described in \S\ref{section-solver} may be
used in the setting of an equality-only free regular tree model; the only
difference with the syntactic case is that the occurs check is no longer
performed.

Note that, although \emphfull{ground types} are regular,
\emphfull{types} remain finite objects: their syntax is unchanged. The $\mu$
notation commonly employed to describe recursive types may be emulated using
type equations: for instance, the notation $\mu\tvar.\tvar\arw\tvar$
corresponds, in our constraint-based approach, to the type scheme
$\scheme\tvar{\tvar=\tvar\arw\tvar}\tvar$.

Although recursive types come for free, as explained above, they have not been
adopted in mainstream programming languages based on \MLtype. The reason is
pragmatic: experience shows that many nonsensical expressions are well-typed
in the presence of recursive types, whereas they are not in their absence.
Thus, the gain in expressiveness is offset by the fact that many programming
mistakes are detected later than otherwise possible. Consider, for instance,
the following @OCaml@ session:
#{&}
ocaml -rectypes
# let rec map f = function
    | [] -> []
    | x :: l -> (map f x) :: (map f l);;
val map : 'a -> ('b list as 'b) -> ('c list as 'c) = <fun>
#{@}
This nonsensical version of @map@ is essentially useless, yet well-typed.
Its principal type scheme, in our notation, is $\scheme{\tvar\twar\tzar}
{\twar=\tlist\twar\wedge\tzar=\tlist\tzar}{\tvar\arw\twar\arw\tzar}$. In
the absence of recursive types, it is ill-typed, since the constraint
$\twar=\tlist\twar\wedge\tzar=\tlist\tzar$ is then false.

\begin{full}
Exercise~\ref{ex:all-terms-typable} shows that, in the presence of equirecursive
types, many nonsensical terms are well-typed: in fact, every pure $\lambda$-term is!
\end{full}

The need for \indexandsay{equirecursive types} is usually suppressed by the presence of
algebraic data types, which offer \indexandsay{isorecursive types}, in the
language. Yet, they are still necessary in some situations, such as in
Objective Caml's extensions with objects \cite{RemyVouillon97} or polymorphic
variants \cite{garrigue-98,garrigue-00,garrigue-02}, where recursive object or
variant types are commonly inferred. In order to allow recursive object or
variant types while still rejecting the above version of @map@, Objective
Caml's constraint solver implements a selective occurs check, which forbids
cycles unless they involve the type constructors $\langle\cdot\rangle$ or
$[\cdot]$ respectively associated with objects and variants. The corresponding
model is a tree model where every infinite path down a tree must encounter the
type constructor $\langle\cdot\rangle$ or $[\cdot]$ infinitely often.

% ----------------------------------------------------------------------------

\index{recursive types!and ML type inference|)}
\index{type inference!and recursive types|)}

\begin{FULL}
\section{Universal Quantification in Constraints}
\label{section-univ}
% TEMPORARY meilleur titre?

% TEMPORARY Müller:
%1) While this may work, why is it worthwhile doing? Why would you
%want to have universal type constraints? One line of explanation is
%worthwhile having.

% TEMPORARY
%% >  1. to explain why they are useful (give applications) before
%% >     explaining how to solve them;
%% >  2. to insist that the solver is restricted to the case of tree equations
%% >     and that the problem might be different/harder for other classes of
%% >     basic constraints.

The constraint logic studied so far allows a set of variables $\tvars$ to be
existentially quantified within a formula $\co$. The resulting formula
$\exists\tvars.\co$ receives its standard meaning: it requires $\co$ to hold
for \emph{some} $\tvars$. However, we currently have no way of requiring a
formula $\co$ to hold for \emph{all} $\tvars$. Is it possible to extend our
logic with universal quantification? If so, what are the new possibilities
offered by this extension, in terms of type inference? The present section
proposes some answers to these questions.

It is worth noting that, although the standard notation for type schemes
involves the symbol $\forall$, type scheme introduction and instantiation
constraints do not allow an encoding of universal quantification. Indeed, a
universal quantifier in a type scheme is very much like an existential
quantifier in a constraint: this is suggested, for instance, by
Definition~\ref{def-ts-instance} and by \Rule{C-LetEx}.

\subsection*{Constraints}

We extend the syntax of constraints as follows:
%
$$\co ::= \ldots \mid \forall\tvars.\co$$
%
Universally quantified variables are often referred to as \emph{rigid}, while
existentially quantified variables are known as \emph{flexible}. The logical
interpretation of constraints (Figure~\ref{fig:meaning-of-constraints}) is
extended as follows:
%
\infrule[CM-Forall]
  {\forall\gtypc\quad\satdef{\etend\ga\tvarc\gtypc}\env\co \\
   \disjoint\tvars{\ftv\env}}
  {\satdef\ga\env{\forall\tvars.\co}}
%
We let the reader check that none of the results established in
\S\ref{section-constraints} are affected by this addition.
Furthermore, the extended constraint language enjoys the following
properties.
%
\begin{lemma}
\label{lemma-forall-entail}
$\forall\tvars.\co \entails \co$. Conversely, $\disjoint\tvars{\ftv\co}$
implies $\co\entails\forall\tvars.\co$.
\end{lemma}
%
\begin{full}
\begin{Proof}
\demoreset Assume $\disjoint\tvars{\ftv\env}$~\dlabel{fresh} and
$\satdef\ga\env\forall\tvars.\co$~\dlabel{h}. By \dref{fresh}, \dref{h}, and
\Rule{CM-Forall}, we have
$\forall\gtypc\quad\satdef{\etend\ga\tvarc\gtypc}\env\co$, which implies
$\satdef\ga\env\co$~\dlabel{b}. Lemma~\ref{lemma-gamma} allows discharging
\dref{fresh} and shows that $\forall\tvars.\co$ entails $\co$.
% C'est la forme FORTE du lemme~\ref{lemma-gamma} qui est employée.
Conversely, let $\disjoint\tvars{\ftv\co}$~\dlabel{free}. Then, by
Lemma~\ref{lemma-sat-free}, \dref{fresh}, and \Rule{CM-Forall},
\dref{b} implies \dref{h}. We conclude as above.
\end{Proof}
\end{full}
%
\begin{lemma}
\label{lemma-all-and}
$\disjoint\tvars{\ftv{\co_2}}$ implies $\forall\tvars.(\co_1\wedge\co_2)
\logeq (\forall\tvars.\co_1)\wedge\co_2$.
\end{lemma}
%
\begin{full}
\begin{Proof}
Analogous to the proof of Lemma~\ref{lemma-ex-and}.
\end{Proof}
\end{full}
%
\begin{lemma}
\label{lemma-all-all}
$\forall\tvars.\forall\twars.\co \logeq
\forall\tvars\twars.\co$.
\end{lemma}
%
\begin{full}
\begin{Proof}
Analogous to the proof of Lemma~\ref{lemma-ex-ex}.
\end{Proof}
\end{full}
%
\begin{lemma}
\label{lemma-exists-forall}
Let $\disjoint\tvars\twars$. Then, $\exists\tvars.\forall\twars.\co$ entails
$\forall\twars.\exists\tvars.\co$. Conversely, if $\exists\twars.\co$
determines $\tvars$, then $\forall\twars.\exists\tvars.\co$ entails
$\exists\tvars.\forall\twars.\co$.
\end{lemma}
%
\begin{full}
\begin{Proof}
\demoreset Let $\disjoint\tvars\twars$~\dlabel{xy} and
$\disjoint{\tvars\twars}{\ftv\env}$~\dlabel{env}. Then, by \dref{env},
\Rule{CM-Exists}, and \Rule{CM-Forall}, the assertion
$\satdef\ga\env{\exists\tvars.\forall\twars.\co}$ is equivalent to
$\exists\gtypc\quad\forall\gtypcp\quad
\satdef{\etend{\etend\ga\tvarc\gtypc}\twarc\gtypcp}\env\co$~\dlabel{ass1}.
Similarly, the assertion $\satdef\ga\env{\forall\twars.\exists\tvars.\co}$ is
equivalent to $\forall\gtypcp\quad\exists\gtypc\quad
\satdef{\etend{\etend\ga\twarc\gtypcp}\tvarc\gtypc}\env\co$~\dlabel{ass2}.  By
\dref{xy}, \dref{ass1} implies \dref{ass2}. Lemma~\ref{lemma-gamma} allows
discharging \dref{env} and shows that $\exists\tvars.\forall\twars.\co$
entails $\forall\twars.\exists\tvars.\co$.

Conversely, assume $\exists\twars.\co$ determines $\tvars$~\dlabel{det}.
Assume $\dref{ass2}$ holds. By \dref{xy}, \dref{env}, and \Rule{CM-Exists},
$\satdef{\etend{\etend\ga\twarc\gtypcp}\tvarc\gtypc}\env\co$ implies
$\satdef{\etend\ga\tvarc\gtypc}\env{\exists\twars.\co}$. Thus, when $\gtypcp$
varies (and, presumably, $\gtypc$ varies as well), the ground assignments
$\etend\ga\tvarc\gtypc$ satisfy $\cplet\env{\exists\twars.\co}$, and coincide
outside of $\tvars$. By \dref{det} and Definition~\ref{def-determines}, they
must coincide on $\tvars$ as well: that is, $\gtypcp$ does \emph{not} vary
when $\gtypc$ varies. Thus, \dref{ass1} holds. We conclude as above.
\end{Proof}
\end{full}

\subsection*{Constraint Solving}

% TEMPORARY Müller:
%% 2) You correctly say that the binder in type schemes is not actually
%% a universal quantifier (page 108, second paragraph), but then seem to
%% suggest that it still universal constraints behave like let (sentence
%% bridging
%% pages 108 and 109). This looks like a contradiction to me.

We briefly explain how to extend the constraint solver described in
\S\ref{section-solver} with support for universal quantification.
(Thus, we again assume an equality-only free tree model.) Constraint solving
in the presence of equations and of existential and universal quantifiers is
known as \emph{unification under a mixed prefix}. It is a particular case of
the decision problem for the first-order theory of equality on trees; see,
for example, \longcite{comon-lescanne-89}.
Extending our solver is straightforward: in fact, the
treatment of universal quantifiers turns out to be surprisingly analogous to
that of \kwd{let} constraints. To begin, we extend the syntax of stacks with
so-called \emph{universal frames}:
%
$$\st ::= \ldots \mid \st[\forall\tvars.\chole]$$
%
Because existential quantifiers cannot, in general, be hoisted out of
universal quantifiers, rules \Rule{S-Ex-1} to \Rule{S-Ex-4} now allow floating
them up to the nearest enclosing \kwd{let} \emph{or universal} frame, if any,
or to the outermost level, otherwise. Thus, in our machine representation of
stacks, where rules \Rule{S-Ex-1} to \Rule{S-Ex-4} are applied in an eager
fashion, every universal frame carries a list of the type variables that are
existentially bound immediately after it, and integer ranks count not only
\kwd{let} frames, but also universal frames.

\begin{widefigure}
$$\begin{regles}
\regle{S-Solve-All}
  {\st;\mc;\forall\tvars.\co}
  {\st[\forall\tvars.\chole];\mc;\co}
  {\disjoint\tvars{\ftv\mc}}

\regle{S-AllEx}
  {\st[\forall\tvars.\exists\twars\tzars.\chole];\mc;\ctrue}
  {\st[\exists\twars.\forall\tvars.\exists\tzars.\chole];\mc;\ctrue}
  {\disjoint\tvars\twars \wedge
   \text{$\exists\tvars\tzars.\mc$ determines $\twars$}}

% TEMPORARY side condition \tvar\isdominatedstar\mc\tzar is wrong; should include the case where
% \tvar and \tzar are equated by U
\regle{S-All-Fail-1}
  {\st[\forall\tvars\tvar.\exists\twars.\chole];\mc;\ctrue}
  {\cfalse}
  {\tvar\not\in\twars \wedge \tvar\isdominatedstar\mc\tzar \wedge
   \tzar\not\in\tvar\twars}

\regle{S-All-Fail-2}
  {\st[\forall\tvars\tvar.\exists\twars.\chole];
   \tvar=\ttyp=\meq\wedge\mc;\ctrue}
  {\cfalse}
  {\tvar\not\in\twars \wedge \ttyp\not\in\tyvarset}

\regle{S-Pop-All}
  {\st[\forall\tvars.\exists\twars.\chole];\mc_1\wedge\mc_2;\ctrue}
  {\st;\mc_1;\ctrue}
  {\disjoint{\tvars\twars}{\ftv{\mc_1}} \wedge
   \exists\twars.\mc_2\logeq\ctrue}
\end{regles}$$
\bcpcaption{solver-univ-trs}{Solving universal constraints}
\end{widefigure}

The solver's specification is extended with the rules in
Figure~\ref{fig:solver-univ-trs}. \Rule{S-Solve-All}, a forward rule,
discovers a universal constraint and enters it, creating a new universal frame
to record its existence. \Rule{S-AllEx} exploits
Lemma~\ref{lemma-exists-forall} to hoist existential quantifiers out of the
universal frame. It is analogous to \Rule{S-LetAll}, and its implementation
may rely upon the same procedure (Exercise~\ref{exercise-det-proc}). The next
two rules detect failure conditions. \Rule{S-All-Fail-1} states that the
constraint $\forall\tvar.\exists\twars.\mc$ is false if the rigid
variable $\tvar$ is directly or indirectly \emph{dominated} by a \emph{free}
variable $\tzar$. Indeed, the value of $\tvar$ is then determined by that of
$\tzar$---but a universally quantified variable ranges over \emph{all} values,
so this is a contradiction. In such a case, $\tvar$ is commonly said to
\emph{escape its scope}. \Rule{S-All-Fail-2} states that the same constraint
is false if $\tvar$ is equated with a \emph{nonvariable}
term. Indeed, the value of $\tvar$ is then partially determined, since its
head constructor is known, which again contradicts its universal status.
Last, \Rule{S-Pop-All} splits the current unification constraint into two
components $\mc_1$ and $\mc_2$, where $\mc_1$ is made up entirely of
\emph{old} variables and $\mc_2$ constrains \emph{young} variables only. This
decomposition is analogous to that performed by \Rule{S-Pop-Let}. Then, it is
not difficult to check that $\forall\tvars.\exists\twars.(\mc_1\wedge\mc_2)$
is equivalent to $\mc_1$. So, the universal frame, as well as $\mc_2$, are
discarded, and the solver proceeds by examining whatever remains on top of the
stack $\st$.

It is possible to further extend the treatment of universal frames with two
rules analogous to \Rule{S-Compress} and \Rule{S-UnName}. In practice, this
improves the solver's efficiency, and makes it easier to share code between
the treatment of \kwd{let} frames and that of universal frames.

It is interesting to note that, as far as the underlying unification
algorithm is concerned, there is no difference between existentially and
universally quantified type variables. The algorithm solves whatever equations
are presented to it, without inquiring about the status of their variables.
Equations that lead to failure, because a rigid variable escapes its scope or
is equated with a nonvariable term, are detected only when the universal
frame is exited. A perhaps more common approach is to \emph{mark} rigid
variables as such, allowing the unification algorithm to signal failure as
soon as one of the two error conditions is encountered. In this approach, a
rigid variable may successfully unify only with itself or with flexible
variables fresher than itself. It is often called a \emph{Skolem constructor}
in the literature \cite{laod91,shields-peyton-jones-02}. An interesting
variant of this approach appears in Dowek, Hardin, Kirchner and Pfenning's
treatment of (higher-order) unification \citeyr{dowek-al-95,dowek-al-98}, where
flexible variables are represented as ordinary variables, while rigid
variables are encoded using De Bruijn indices.
%JNF -- are the paper's above different? why do we have two dowek citations?
% why not use Dowek96jicslp from bcp.bib

\begin{short}
The properties of our constraint solver are preserved by this extension: it is
possible to prove that Lemmas~\ref{lemma-solver-trs-normalizes},
\ref{lemma-solver-trs-correct}, and~\ref{lemma-solver-trs-normal-forms} remain
valid.
\end{short}
\begin{full}
The properties of our constraint solver are preserved by this extension, as
shown by the following lemmas, whose statements are identical to those of
Lemmas~\ref{lemma-solver-trs-normalizes}, \ref{lemma-solver-trs-correct},
and~\ref{lemma-solver-trs-normal-forms}.
%
\begin{lemma}
\label{lemma-solver-univ-trs-normalizes}
The reduction system $\red$ is strongly normalizing.
\end{lemma}
%
% TEMPORARY des volontaires pour la preuve?
%
\begin{lemma}
\label{lemma-solver-univ-trs-correct}
$\st;\mc;\co \red \st';\mc';\co'$ implies
$\st[\mc\wedge\co] \logeq \st'[\mc'\wedge\co']$.
\end{lemma}
%
\begin{Proof}
By examination of every rule.

\proofcase\Rule{S-Solve-All}. By Lemma~\ref{lemma-all-and}.

\proofcase\Rule{S-AllEx}. By Lemma~\ref{lemma-exists-forall}.

\demoreset\proofcase\Rule{S-All-Fail-1}. Let $\tvar\not\in\twars$~\dlabel{x}
and $\tvar\isdominatedstar\mc\tzar$~\dlabel{dom} and
$\tzar\not\in\tvar\twars$~\dlabel{free}. By \dref{dom}, there exists a path
$\chemin$ such that, for every ground assignment $\ga$ that satisfies $\mc$,
$\ga(\tvar)$ is $\ga(\tzar)/\chemin$, that is, the subtree of $\ga(\tzar)$
rooted at $\chemin$. By \dref{x} and \dref{free}, the same holds for every
ground assignment $\ga$ that satisfies $\exists\twars.\mc$. Let us now assume
that $\ga$ satisfies $\forall\tvar.\exists\twars.\mc$~\dlabel{sat}. By
\Rule{CM-Forall}, for every ground type $\gtyp$, we have
$\etend\ga\tvar\gtyp\satisfies\exists\twars.\mc$.  By \dref{free} and by the
above result, this implies $\gtyp=\ga(\tzar)/\chemin$. In a nondegenerate
model, this cannot possibly hold for every $\gtyp$, so the hypothesis
\dref{sat} is inconsistent.  We have shown that
$\forall\tvar.\exists\twars.\mc$ is false. The result follows by
Lemmas~\ref{lemma-false} and~\ref{lemma-all-all}.

\demoreset\proofcase\Rule{S-All-Fail-2}. Let $\tvar\not\in\twars$~\dlabel{x}
and $\ttyp\not\in\tyvarset$~\dlabel{t} and $\ga\satisfies
\forall\tvar.\exists\twars.(\tvar=\ttyp=\meq\wedge\mc)$~\dlabel{sat}. By
\dref{t}, $\ttyp$ has a head symbol $\tycon$. Then, by \dref{x},
\Rule{CM-Forall}, \Rule{CM-Exists}, \Rule{CM-And}, and \Rule{CM-Predicate},
\dref{sat} implies that every ground type $\gtyp$ has the head symbol
$\tycon$. In a nondegenerate model, this is a contradiction, so the hypothesis
\dref{sat} is inconsistent. We conclude as in the previous case.

\demoreset\proofcase\Rule{S-Pop-All}. Let
$\disjoint{\tvars\twars}{\ftv{\mc_1}}$~\dlabel{free} and
$\exists\twars.\mc_2\logeq\ctrue$~\dlabel{true}. We have
$\forall\tvars.\exists\twars.(\mc_1\wedge\mc_2) \logeq
 \forall\tvars.(\mc_1\wedge\exists\twars.\mc_2) \logeq
 \forall\tvars.\mc_1 \logeq 
 \mc_1$,
where the first equivalence is by \dref{free} and \Rule{C-ExAnd}, the second
one is by \dref{true}, and the last one follows from \dref{free} and
Lemma~\ref{lemma-forall-entail}.
\end{Proof}
%
\begin{lemma}
\label{lemma-solver-univ-trs-normal-forms}
A normal form for the reduction system $\red$ is one of
(i) $\st;\mc;\ccall\evid\ttyp$, where $\evid\not\in\dpv\st$;
(ii) $\st;\cfalse;\ctrue$; or
(iii) $\xc;\mc;\ctrue$, where $\xc$ is an existential constraint context
      and $\mc$ a satisfiable conjunction of multi-equations.
\end{lemma}
%
\begin{Proof}
It is clear that, thanks to \Rule{S-Solve-All}, the analysis of the structure
of the external constraint remains exhaustive. There remains to consider a
state of the form $\st[\forall\tvars.\exists\twars.\chole];\mc;\ctrue$, where
$\mc$ is a standard conjunction of multi-equations and, if the model is
syntactic, $\mc$ is acyclic. We may assume, \spdg, $\disjoint\tvars\twars$.
Let us write $\mc$ as $\mc_1\wedge\mc_2$, where
$\disjoint{\tvars\twars}{\ftv{\mc_1}}$, and where $\mc_1$ is maximal for this
criterion. Then, consider a multi-equation $\meq\in\mc$, one of whose variable
members is outside $\tvars\twars$. Because the state at hand is a normal form
with respect to \Rule{S-AllEx}, we may deduce, as in the proof of
Lemma~\ref{lemma-solver-trs-normal-forms}, that all variables in $\ftv\meq$
must be outside $\twars$. Furthermore, because it is a normal form with
respect to \Rule{S-All-Fail-1}, all variables in $\ftv\meq$ must be outside
$\tvars$ as well. By definition of $\mc_1$, this implies $\meq\in\mc_1$. By
contraposition, for every multi-equation $\meq\in\mc_2$, \emph{all variable
members of $\meq$ are in $\tvars\twars$}.  Last, if $\meq\in\mc_2$ and $\tvar$
is a variable member of $\meq$, where $\tvar\in\tvars$, then, because the
state at hand is a normal form with respect to \Rule{S-All-Fail-1} and
\Rule{S-All-Fail-2}, \emph{every member of $\meq$ is either $\tvar$ itself or
inside $\twars$}. Thus, every rigid variable $\tvar$ must be a minimal element
with respect to $\isdominated{\mc_2}$. Let us now recall that $\mc_2$ is a
standard conjunction of multi-equations and, if the model is syntactic,
$\mc_2$ is acyclic. We let the reader check that this implies
$\exists\twars.\mc_2\logeq\ctrue$; the proof is again a slight generalization
of the last part of that of Lemma~\ref{lemma-unif-trs-normal-forms}. Then, the
state at hand is reducible via \Rule{S-Pop-All}. This is a contradiction, so
this case cannot arise.
\end{Proof}
\end{full}

\subsection*{Type Annotations, Continued}

In \S\ref{section-calc-lang}, we introduced the expression form
$\ann\et$, allowing an expression $\et$ to be annotated with a type $\ttyp$
whose free variables $\tvars$ are \emph{locally} and \emph{existentially}
bound. It is now natural to introduce the symmetric expression form
$\fann\et$, where $\ttyp$ has kind $\normalkind$, $\tvars$ is bound within
$\ttyp$, and $\tvars$ contains $\ftv\ttyp$, as before. Its constraint
generation rule is as follows:
%
\begin{bnf}
\calcule{\fann\et}{\ttyp'}
\eq
\forall\tvars.\calcule\et\ttyp\wedge\exists\tvars.(\ttyp\leq\ttyp')
& \text{provided $\disjoint\tvars{\ftv{\et,\ttyp'}}$}
\end{bnf}%
%
The first conjunct requires $\et$ to have type $\ttyp$ for \emph{all} values
of $\tvars$. Here, the type variables $\tvars$ are \emph{universally} bound,
as expected. The second conjunct requires $\ttyp'$ to be \emph{some} instance
of the universal annotation $\dmscheme\tvars\ttyp$. Since $\ttyp'$ is only a
monotype, it seems difficult to think of another sensible way of constraining
$\ttyp'$. For this reason, the type variables $\tvars$ are still
\emph{existentially} bound in the second conjunct. This makes the
interpretation of the universal quantifier in type annotations a bit more
complex than that of the existential quantifier. For instance, when subtyping
is interpreted as equality, the constraint generation rule may be read:
\emph{a valid type for $\fann\et$ is of the form $\ttyp$, for \emph{some}
  choice of the type variables $\tvars$, provided $\et$ has type $\ttyp$ for
  \emph{all} choices of $\tvars$.}

We note that $\fann\et$ must be a new expression form: it cannot be encoded
by adding new constants to the calculus---whereas $\ann\et$ could---because
none of the existing constraint generation rules produce universally
quantified constraints. Like all type annotations, it has identity semantics.

What is the use of universal type annotations, compared with existential type
annotations? When a type variable is existentially bound, the typechecker is
free to assign it whatever value makes the program well-typed. As a result,
the expressions
$\eannot{\efun\evar{\evar\mathbin{\hat+}\hat1}}{\exists\tvar.\tvar\arw\tvar}$
and $\eannot{\efun\evar\evar}{\exists\tvar.\tvar\arw\tvar}$ are both
well-typed: $\tvar$ is assigned $\tint$ in the former case, and remains
undetermined in the latter. However, it is sometimes useful to be able to
insist that an expression should be polymorphic. This effect is naturally
achieved by using a universally bound type variable. Indeed,
$\eannot{\efun\evar{\evar\mathbin{\hat+}\hat1}}{\forall\tvar.\tvar\arw\tvar}$
is ill-typed, because $\forall\tvar.(\tvar=\tint)$ is false, while
$\eannot{\efun\evar\evar}{\forall\tvar.\tvar\arw\tvar}$ is well-typed.

\begin{exercise}[\QuickCheck]
Write down the constraints
$\exists\tzar.\calcule{\eannot{\efun\evar{\evar\mathbin{\hat+}\hat1}}
{\forall\tvar.\tvar\arw\tvar}}\tzar$ and
$\exists\tzar.\calcule{\eannot{\efun\evar\evar}
{\forall\tvar.\tvar\arw\tvar}}\tzar$, which tell whether these expressions are
well-typed. Check that the former is false, while the latter is satisfiable.
\solref{univannot}
\end{exercise}

A universal type annotation, as defined above, is nothing but a (closed)
Damas-Milner type scheme. Thus, the new construct $\fann\et$ gives us the
ability to ensure that the expression $\et$ admits the type scheme
$\dmscheme\tvars\ttyp$. This feature is exploited at the module level in
\MLlang, where it is necessary to check that the inferred type for a module
component $\et$ is more general than the type scheme $\dms$ that appears in
the module's signature.  In our view, this process simply consists of ensuring
that $\eannot\et\dms$ is well-typed.

In \S\ref{section-calc-lang}, we have pointed out that local (that is,
closed) type annotations offer limited expressiveness, because they cannot
share type variables. To lift this limitation, we have introduced the
expression forms $\exists\tvars.\et$ and $\eannot\et\ttyp$. The former binds
the type variables $\tvars$ within $\et$, making them available for use in
type annotations, and instructs the constraint generator to existentially
quantify them at this point. The latter requires $\et$ to have $\ttyp$. It is
natural to proceed in the same manner in the case of universal type
annotations. We now introduce the expression form $\forall\tvars.\et$, which
also binds $\tvars$ within $\et$, but comes with a different constraint
generation rule:
%
\begin{bnf}
\calcule{\forall\tvars.\et}\ttyp
\eq
\forall\tvars.\exists\tzar.\calcule\et\tzar \wedge
\exists\tvars.\calcule\et\ttyp 
& \text{provided $\disjoint\tvars{\ftv\ttyp} \wedge
\tzar\not\in\ftv\et$}
\end{bnf}%
%
This rule is a bit more complex than that associated with the expression form
$\exists\tvars.\et$. Again, this is due to the fact that we do not wish to
overconstrain $\ttyp$. The first exercise below shows that a more naive
version of the rule does not yield the desired behavior. The second
exercise shows that this version does. The third exercise clarifies an
efficiency concern.

\begin{exercise}[\QuickCheck]
Assume that $\calcule{\forall\tvars.\et}\ttyp$ is defined as
$\forall\tvars.\calcule\et\ttyp$, provided $\disjoint\tvars{\ftv\ttyp}$.
Write down the constraint
$\calcule{\forall\tvar.\eannot{\efun\evar\evar}{\tvar\arw\tvar}}\tzar$. Can
you describe its solutions? Does it have the intended meaning?
\solref{baduniv}
\end{exercise}

\begin{exercise}[\Easy]
Let $\tvars\supseteq\ftv\ttyp$ and $\disjoint\tvars{\ftv\et}$. Check that the
constraints $\calcule{\fann\et}{\ttyp'}$ and
$\calcule{\forall\tvars.\eannot\et\ttyp}{\ttyp'}$ are equivalent. In other
words, \emph{local} universal type annotations may also be expressed in terms
of the more complex constructs described above.
\solref{univsame}
\end{exercise}

\begin{exercise}[\Challenging, \nosolution]
The constraint generation rule that appears above compromises the linear time
and space complexity of constraint generation, because it duplicates the term
$\et$. It is possible to avoid this problem, but this requires a slight
generalization of the constraint language. Let us write
$\cxlet{\evid:\scheme{\underline\tvars\twars}{\co_1}\ttyp}{\co_2}$ for
$\forall\tvars.\exists\twars.\co_1\wedge
\cplet{\evid:\scheme{\tvars\twars}{\co_1}\ttyp}{\co_2}$. In this extended
\kwd{let} form, the underlined variables $\tvars$ are interpreted as
\emph{rigid}, instead of \emph{flexible}, while checking that $\co_1$ is
satisfiable. However, the type scheme associated with $\evid$ is not
affected. \emph{Check that the above constraint generation rule may now be
written as follows:}
%
\begin{bnf}
\calcule{\forall\tvars.\et}\ttyp
\eq
\cxlet{\evid:\scheme{\underline\tvars\tzar}{\calcule\et\tzar}\tzar}
      {\ccall\evid\ttyp}
& \text{provided $\tzar\not\in\ftv\et$}
\end{bnf}%
%
Roughly speaking, the new rule forms a most general type scheme for $\et$,
ensures that the type variables $\tvars$ are unconstrained in it, and checks
that $\ttyp$ is an instance of it. Furthermore, it does not duplicate $\et$.
To complete the exercise, \emph{extend the specification of the constraint
  solver (Figures~\ref{fig:solver-trs} and~\ref{fig:solver-univ-trs}), as well
  as its implementation,} to deal with this extension of the constraint
language.
\end{exercise}

To conclude, let us once again stress that, if $\ttyp$ has free type
variables, the effect of the type annotation $\eannot\et\ttyp$ depends on
\emph{how} and \emph{where} they are bound. The effect of \emph{how} stems
from the fact that binding a type variable universally, rather than
existentially, leads to a stricter constraint. Indeed, we let the reader check
that $\calcule{\forall\tvars.\et}\ttyp$ entails
$\calcule{\exists\tvars.\et}\ttyp$, while the converse does not hold in
general. The effect of \emph{where} has been illustrated, in the case of
existentially bound type variables, in \S\ref{section-calc-lang}. It is
due, in that case, to the fact that \kwd{let} and $\exists$ do not commute. In
the case of universally bound type variables, it may be imputed to the fact
that $\forall$ and $\exists$ do not commute. For instance,
$\efun\evar{\forall\tvar.\eannot\evar\tvar}$ is ill-typed, because
\emph{inside the $\lambda$-abstraction}, the program variable $\evar$ cannot
be said to have every type. However,
$\forall\tvar.\efun\evar{\eannot\evar\tvar}$ is well-typed, because the
identity function does have type $\tvar\arw\tvar$ for every $\tvar$.

\begin{exercise}[\QuickCheck]
Write down the constraints
$\exists\tzar.\calcule{\efun\evar{\forall\tvar.\eannot\evar\tvar}}\tzar$ and
$\exists\tzar.\calcule{\forall\tvar.\efun\evar{\eannot\evar\tvar}}\tzar$,
which tell whether these expressions are well-typed. Is the former
satisfiable? Is the latter?
\solref{noncom}
\end{exercise}

\label{page-disc-annotations}
In Standard ML and Objective Caml, the type variables that appear in type
annotations are \emph{implicitly} bound. That is, there is no syntax in the
language for the constructs $\exists\tvars.\et$ and $\forall\tvars.\et$. When
a type annotation $\eannot\et\ttyp$ contains a free type variable $\tvar$, a
fixed convention tells how and where $\tvar$ is bound. In Standard ML, $\tvar$
is \emph{universally} bound at the nearest @val@ binding that encloses all
related occurrences of $\tvar$ \cite{Milner&90}. The 1997 revision of Standard
ML \cite{SML97} slightly improves on this situation by allowing type variables
to be \emph{explicitly} introduced at @val@ bindings.  However, they still
must be universally bound. In Objective Caml, $\tvar$ is \emph{existentially}
bound at the nearest enclosing toplevel \kwlet binding; this behavior seems to
be presently undocumented. We argue that (i) allowing type variables to be
implicitly introduced is confusing; and (ii) for expressiveness, both
universal and existential quantifiers should be made available to programmers.
Surprisingly, these language design and type inference issues seem to have
received little attention in the literature, although they have most likely
been ``folklore'' for a long time. Peyton Jones and
Shields \citeyr{peyton-jones-shields-03} study these issues in the context of
Haskell, and concur with (i). Concerning (ii), they seem to think that the
language designer must choose between existential and universal type variable
introduction forms---which they refer to as ``type-sharing'' and
``type-lambda''---whereas we point out that they may and should coexist.

% For instance, typechecking the expression
% $\eannot{\efun{\evar_1}{\efun{\evar_2}{(\evar_1,\evar_2\mathbin{\hat+}\hat1)}}}
% {\dmscheme\tvar{\tvar\arw\twar\arw\tvar\times\twar}}$ involves ensuring that
% the function $\efun{\evar_1}{\efun{\evar_2}{(\evar_1,\evar_2\mathbin{\hat+}\hat1)}}$ is
% parametric with respect to $\evar_1$ and instantiating the free type variable
% $\twar$ to $\tint$.

\subsection*{Polymorphic Recursion}
\label{page-polymorphic-recursion}

\index{polymorphic recursion|(}

Example~\ref{example-fix} explains how the \kwletrec construct found in
\MLlang may be viewed as an application of the constant $\efix$, wrapped
inside a normal \kwlet construct. Exercise~\ref{ex:solfix} shows that this
gives rise to a somewhat restrictive constraint generation rule:
generalization occurs only \emph{after} the application of $\efix$ is
typechecked. In other words, in $\eletrec\efar\evar{\et_1}{\et_2}$, all
occurrences of $\efar$ within $\et_1$ must have the same (monomorphic)
\emph{type}. This restriction is sometimes a nuisance, and seems unwarranted:
if the function that is being defined is polymorphic, it should be possible to
use it at different types even inside its own definition.  Indeed,
\longcite{Mycroft84} extended Damas and Milner's type system with a more
liberal treatment of recursion, commonly known as \emph{polymorphic
recursion}. The idea is to only request occurrences of $\efar$ within $\et_1$
to have the same \emph{type scheme}. Hence, they may have different
\emph{types}, all of which are instances of a common type scheme that should
also be a valid type scheme for $\et_1$. It was later
shown that well-typedness in Mycroft's extended type system is
undecidable\index{undecidability!of
  type inference with polymorphic recursion}
 \cite{Henglein93,kfoury93type,Kfo+Tiu+Urz:IAC-1993-v102n1}. To
work around this stumbling 
block, one solution is to use a semi-algorithm, falling back to monomorphic
recursion if it does not succeed or fail in reasonable time. Although such a
solution might be appealing in the setting of an automated program analysis,
it is less so in the setting of a programmer-visible type system, because it
may become difficult to understand why a program is ill-typed. Thus, we
describe a simpler solution, which consists of requiring the programmer to
explicitly supply a type scheme for $\efar$. This is an instance of a
\emph{mandatory} type annotation.

To begin, we must change the status of $\efix$, because if $\efix$ remains a
constant, then $\efar$ must remain $\lambda$-bound and cannot receive a
polymorphic type scheme. We turn $\efix$ into a language construct, which
binds a program variable $\efar$, and annotates it with a \dm type scheme.
%
% TEMPORARY Expliquer pourquoi un schéma DM et pas un schéma contraint
%
The syntax of values and expressions is thus extended as follows:
%
$$\ev ::= \ldots \mid \ebfix\efar\dms\evar\et \qquad
  \et ::= \ldots \mid \ebfix\efar\dms\evar\et$$
%
Note that $\efar$ is bound within $\efun\evar\et$.
The operational semantics is extended as follows. 
%
\infax[R-Fix']{
\eapp{(\ebfix\efar\dms\evar\et)}\ev \reduces
\eapp{(\elet\efar{\ebfix\efar\dms\evar\et}{\efun\evar\et})}\ev
}
%
The type annotation $\dms$ plays no essential role in the reduction; it is
merely preserved.  It is now possible to define
$\eletrec{\efar:\dms}\evar{\et_1}{\et_2}$ as syntactic sugar for
$\elet\efar{\ebfix\efar\dms\evar{\et_1}}{\et_2}$.

We now give a constraint generation rule for $\efix$:
%
\begin{bnf}
\calcule{\ebfix\efar\dms\evar\et}\ttyp
\eq
\cxlet{\efar:\dms}{\calcule{\efun\evar\et}\dms} \wedge
\ccall\dms\ttyp
\end{bnf}%
%
The left-hand conjunct requires the function $\efun\evar\et$ to have type
scheme $\dms$, under the assumption that $\efar$ has type $\dms$. Thus, it is
now possible for different occurrences of $\efar$ within $\et$ to receive
different types.\label{page-calcule-poly}
If $\dms$ is $\dmscheme\tvars\ttyp$, where
$\disjoint\tvars{\ftv\et}$, then we write $\calcule\et\dms$ for
$\forall\tvars.\calcule\et\ttyp$. Indeed, checking the validity of a
polymorphic type annotation---be it mandatory, as is the case here, or
optional, as was previously the case---requires a universally quantified
constraint. The right-hand conjunct merely constrains $\ttyp$ to be an
instance of $\dms$.

Given the definition of $\eletrec{\efar:\dms}\evar{\et_1}{\et_2}$ as syntactic
sugar, the above rule leads to the following derived constraint generation
rule for \kwletrec:
%
\begin{bnf}
\calcule{\eletrec{\efar:\dms}\evar{\et_1}{\et_2}}\ttyp
\eq
\cxlet{\efar:\dms}{(\calcule{\efun\evar\et_1}\dms \wedge
\calcule{\et_2}\ttyp)}
\end{bnf}%
%
This rule is arguably quite natural. The program variable $\efar$ is assigned
the type scheme $\dms$ throughout its scope, that is, both inside and outside
of the function's definition. The function $\efun\evar\et_1$ must itself have
type scheme $\dms$. Last, $\et_2$ must have type $\ttyp$, as in every \kwlet
construct.
%
\begin{exercise}[\Easy]
Prove that the derived constraint generation rule above is indeed valid.
\solref{polrec}
\end{exercise}

It is straightforward to prove that the extended language still enjoys subject
reduction. The proof relies on the following lemma: if $\et$ has type scheme
$\dms$, then every instance of $\dms$ is also a valid type for $\et$.
%
\begin{lemma}
\label{lemma-calcule-poly}
$\calcule\et\dms \wedge \ccall\dms\ttyp \entails \calcule\et\ttyp$.
\end{lemma}
%
\begin{full}
\begin{Proof}
\demoreset Let us write $\dms$ as $\dmscheme\tvars{\ttyp'}$, where
$\disjoint\tvars{\ftv{\et,\ttyp}}$~\dlabel{f}. By \dref{f} and by definition,
$\calcule\et\dms \wedge \ccall\dms\ttyp$ stands for
$\forall\tvars.\calcule\et{\ttyp'} \wedge
\exists\tvars.(\ttyp'\subtype\ttyp)$~\dlabel{un}.
By \Rule{C-ExAnd} and by Lemma~\ref{lemma-forall-entail}, \dref{un} entails
$\exists\tvars.(\calcule\et{\ttyp'} \wedge \ttyp'\subtype\ttyp)$, which
by Lemma~\ref{lemma-calcule-covariant} entails
$\exists\tvars.\calcule\et\ttyp$. By \dref{f} and \Rule{C-Ex*}, this is
$\calcule\et\ttyp$.
\end{Proof}
\end{full}
%
\begin{theorem}[Subject reduction]
\label{theorem-sr-polrec}
$(\Rule{R-Fix'})\subseteq(\sr)$.
\end{theorem}
%
\begin{full}
\begin{Proof}
\demoreset The goal is to prove that
$\calcule{\eapp{(\ebfix\efar\dms\evar\et)}\ev}\ttyp$ entails
$\calcule{\eapp{(\elet\efar{\ebfix\efar\dms\evar\et}{\efun\evar\et})}\ev}
\ttyp$. By definition of constraint generation, the former is
$\exists\tvar.(\cxlet{\efar:\dms}{\calcule{\efun\evar\et}\dms} \wedge
\ccall\dms{\tvar\arw\ttyp} \wedge \calcule\ev\tvar)$, where $\tvar$ is fresh,
while the latter is $\exists\tvar.(\cxlet{\efar:\dms}{(\calcule{\efun\evar\et}\dms \wedge
\calcule{\efun\evar\et}{\tvar\arw\ttyp}) \wedge
\calcule\ev\tvar)}$. The result follows from \Rule{C-InAnd*} and
Lemma~\ref{lemma-calcule-poly}.
\end{Proof}
\end{full}

% TEMPORARY que dire de progress?

The programming language Haskell offers polymorphic recursion.
Interesting details about its typing rules may be found
in \longcite{jones-thih-99,faxen-02}.

It is worth pointing out that some restricted instances of type
inference in the presence of polymorphic recursion are decidable. This
is typically the case in certain program analyses, where a type
derivation for the program is already available, and the goal is only
to infer extra atomic annotations, such as binding time or strictness
properties. Several papers that exploit this idea are
\longcite{dhm95b}, % Space added [bcp]
\longcite{jensen-98}, and
\longcite{rehof-faehndrich-01}.

\index{polymorphic recursion|)}

\subsection*{Universal Types}

\MLtype enforces a strict stratification between types and type schemes, or,
in other words, allows only prenex universal quantifiers inside types. We have
pointed out earlier that there is good reason to do so: type inference for
\MLtype is decidable, while type inference for System~F, which has no such
restriction, is undecidable. Yet, this restriction comes at a cost in
expressiveness: it prevents higher-order functions from accepting polymorphic
function arguments, and forbids storing polymorphic functions inside data
structures. Fortunately, it is in fact possible to circumvent the problem by
requiring the programmer to supply additional type information.

The approach that we are about to describe is reminiscent of the way algebraic
data type definitions allow circumventing the problems associated with
equirecursive types (\S\ref{section-calc-lang}). Because we do not wish
to extend the syntax of types with universal types of the form
$\dmscheme\twars\ttyp$, we instead allow \emph{universal type definitions},
of the form
%
$$\datadef\tnam\tvarc{\dmscheme\twars\ttyp}$$
%
where $\tnam$ still ranges over data types. If $\tnam$ has {\tcsignature}
$\vec\kind\kindarrow\normalkind$, then the type variables $\tvarc$ must have
kind $\vec\kind$. The type $\ttyp$ must have kind $\normalkind$. The type
variables $\tvars$ and $\twars$ are considered bound within $\ttyp$, and the
definition must be closed, that is, $\ftv\ttyp\subseteq\tvars\twars$ must
hold. Last, the variance of the type constructor $\tnam$ must match its
definition---a requirement stated as follows:
%
\begin{definition}
\label{def-universal-variance}
Let $\datadef\tnam\tvarc{\dmscheme\twars\ttyp}$ and
$\datadef\tnam\tvarcp{\dmscheme{\twars'}{\ttyp'}}$ be two $\alpha$-equivalent
instances of a single universal type definition, such that
$\disjoint\twars{\ftv{\ttyp'}}$ and $\disjoint{\twars'}{\ftv\ttyp}$. Then,
$\tnam\,\tvarc\subtype\tnam\,\tvarcp \entails
\forall\twars'.\exists\twars.\ttyp\subtype\ttyp'$ must
hold.
\end{definition}

This requirement is analogous to that found in
Definition~\ref{def-algebraic-variance}. The idea is, if $\tnam\,\tvarc$ and
$\tnam\,\tvarcp$ are comparable, then their unfoldings $\dmscheme\twars\ttyp$
and $\dmscheme{\twars'}{\ttyp'}$ should be comparable as well. The comparison
between them is expressed by the constraint
$\forall\twars'.\exists\twars.\ttyp\subtype\ttyp'$, which may be read:
\emph{every instance of $\dmscheme{\twars'}{\ttyp'}$ is (a supertype of)
an instance of $\dmscheme\twars\ttyp$}. Again, when subtyping is interpreted
as equality, the requirement of Definition~\ref{def-universal-variance} is
always satisfied; it becomes nontrivial only in the presence of true
subtyping.

The effect of the universal type definition
$\datadef\tnam\tvarc{\dmscheme\twars\ttyp}$ is to enrich the programming
language with a new construct: $$\ev ::= \ldots \mid \eupack\tnam\ev \qquad
\et ::= \ldots \mid \eupack\tnam\et \qquad \ec ::= \ldots \mid
\eupack\tnam\ec$$ and with a new unary destructor $\euopen\tnam$. Their
operational semantics is as follows:
%
\infax[R-Open-All]{
\eapp{\euopen\tnam}{(\eupack\tnam\ev)} \reduces[\delta] \ev
}
%
Intuitively, $\kupack\tnam$ and $\euopen\tnam$ are the two coercions that
witness the isomorphism between $\tnam\,\tvarc$ and $\dmscheme\twars\ttyp$.
The value $\eupack\tnam\ev$ behaves exactly like $\ev$, except it is marked,
as a hint to the typechecker. As a result, the mark must be removed using
$\euopen\tnam$ before the value can be used.

What are the typing rules for $\kupack\tnam$ and $\euopen\tnam$? In
System~F, they would receive types $\forall\tvars.(\dmscheme\twars\ttyp)\arw
\tnam\,\tvarc$ and $\forall\tvars.\tnam\,\tvarc\arw\dmscheme\twars\ttyp$,
respectively. However, neither of these is a valid type scheme: both exhibit a
universal quantifier under an arrow.

In the case of $\kupack\tnam$, which
has been made a language construct rather than a constant, we work around the
problem by embedding this universal quantifier in the constraint generation
rule:
%
\begin{bnf}
\calcule{\eupack\tnam\et}{\ttyp'}
\eq
\exists\tvars.(
  \calcule\et{\dmscheme\twars\ttyp} \wedge
  \tnam\,\tvarc \subtype \ttyp'
)
\end{bnf}%
%
The rule implicitly requires that $\tvars$ be fresh for the left-hand side and
that $\datadef\tnam\tvarc{\dmscheme\twars\ttyp}$ be (an $\alpha$-variant of)
the definition of $\tnam$. The left-hand conjunct requires $\et$ to have type
scheme $\dmscheme\twars\ttyp$. The notation $\calcule\et\dms$ was
defined on page~\pageref{page-calcule-poly}. The right-hand conjunct states
that a valid type for $\eupack\tnam\et$ is (a supertype of) $\tnam\,\tvarc$.

We deal with $\euopen\tnam$ as follows. Provided $\disjoint\tvars\twars$, we
extend the initial environment $\ienv$ with the binding
$\euopen\tnam:\forall\tvars\twars.\tnam\,\tvarc\arw\ttyp$. We have simply
hoisted the universal quantifier outside of the arrow---a valid isomorphism in
System~F.

The proof of the subject reduction theorem must be extended with the following
new case:
%
\begin{theorem}[Subject reduction]
\label{theorem-sr-univ}
$(\Rule{R-Open-All})\subseteq(\sr)$.
\end{theorem}
%
\begin{Proof}
\demoreset We have
$$\begin{array}{:rll:}
& \cxlet\ienv{\calcule{\eapp{\euopen\tnam}{(\eupack\tnam\ev)}}{\ttyp_0}} \\
\logeq & \cxlet\ienv{\exists\tzar.(
           \ccall{\euopen\tnam}{\tzar\arw\ttyp_0} \wedge
           \calcule{\eupack\tnam\ev}\tzar
         )}
       & \dlabel{un} \\
\logeq & \cxlet\ienv{\exists\tzar.(
           \exists\tvars'\twars'.(
             \tnam\,\tvarcp\arw\ttyp' \subtype \tzar\arw\ttyp_0
           ) \wedge
           \exists\tvars.(
             \calcule\ev{\dmscheme\twars\ttyp} \wedge
             \tnam\,\tvarc \subtype \tzar
           )
         )}
       & \dlabel{deux} \\
\logeq & \cxlet\ienv{\exists\tvars\tvars'\twars'.(
           \calcule\ev{\dmscheme\twars\ttyp} \wedge
           \tnam\,\tvarc \subtype \tnam\,\tvarcp \wedge
           \ttyp'\subtype\ttyp_0
         )}
       & \dlabel{trois} \\
\entails & \cxlet\ienv{\exists\tvars\twars\tvars'\twars'.(
           \calcule\ev{\dmscheme\twars\ttyp} \wedge
           \ttyp\subtype\ttyp' \wedge
           \ttyp'\subtype\ttyp_0
         )}
       & \dlabel{quatre} \\
\entails & \cxlet\ienv{\exists\tvars\twars\tvars'\twars'.
           \calcule\ev{\ttyp_0}
         }
       & \dlabel{cinq} \\
\logeq & \cxlet\ienv{
           \calcule\ev{\ttyp_0}
         }
       & \dlabel{six} \\
\end{array}$$
where \dref{un} is by definition of constraint generation for applications and
for constants; $\tzar$ is fresh; \dref{deux} is by definition of constraint
generation for $\kupack\tnam$ and $\euopen\tnam$, where
$\datadef\tnam\tvarc{\dmscheme\twars\ttyp}$ and
$\datadef\tnam\tvarcp{\dmscheme{\twars'}{\ttyp'}}$ are two $\alpha$-equivalent
instances of the definition of $\tnam$; $\tvars$, $\twars$, $\tvars'$, and
$\twars'$ are fresh and satisfy $\disjoint\twars{\ftv{\ttyp'}}$ and
$\disjoint{\twars'}{\ftv\ttyp}$; \dref{trois} is by \Rule{C-ExAnd},
\Rule{C-Arrow}, and \Rule{C-ExTrans}, which allows
eliminating $\tzar$; \dref{quatre} is by
Definition~\ref{def-universal-variance}, Lemma~\ref{lemma-forall-entail}, and
\Rule{C-ExAnd}; \dref{cinq} is by Lemmas~\ref{lemma-calcule-poly}
and~\ref{lemma-calcule-covariant}; \dref{six} is by \Rule{C-Ex*}.
\end{Proof}

The proof of $(\Rule{R-Context})\subseteq(\sr)$ must also be extended with a
new subcase, corresponding the new production $\ec ::= \ldots\mid
\eupack\tnam\ec$. If the language is pure, this is straightforward. In the
presence of side effects, however, this subcase fails, because universal and
existential quantifiers in constraints do not commute. The problem is then
avoided by restricting $\kupack\tnam$ to values, as in
Definition~\ref{def-value-restriction}.

% TEMPORARY progress?

This approach to extending \MLtype with universal (or
existential---see below) types has been studied in
\longcite{laod91}, % space added [bcp]
\longcite{Remy94}, % space added [bcp]
\longcite{OderskyLaufer96},
and\longcite{shields-peyton-jones-02}.  Laüfer and Odersky have
suggested combining universal or existential type declarations with
algebraic data type definitions. This allows suppressing the
cumbersome $\kupack\tnam$ and $\euopen\tnam$ constructs; instead, one
simply uses the standard syntax for constructing and deconstructing
variants and records.

% TEMPORARY citer extensions au rang 2 (décidable?)
% et au rang n (Laufer-Odersky, Peyton Jones)
% (codable en termes de contraintes exists et forall? exo)

\subsection*{Existential Types}
\label{page-existential-types}

Existential types (\TAPLCHAPTER{24}) are close cousins of universal types,
and may be introduced into \MLtype in the same manner. 
Actually, existential types have been introduced in \MLtype before universal
types. We give a brief
description of this extension, insisting mainly on the differences with
the case of universal types. 

We now allow \emph{existential type definitions}, of the form
%
$\datadef\tnam\tvarc{\exists\twars.\ttyp}$.
%
The conditions required of a well-formed definition are unchanged, except the
variance requirement, which is dual:
%
\begin{definition}
\label{def-existential-variance}
Let $\datadef\tnam\tvarc{\exists\twars.\ttyp}$ and
$\datadef\tnam\tvarcp{\exists\twars'.\ttyp'}$ be two $\alpha$-equivalent
instances of a single existential type definition, such that
$\disjoint\twars{\ftv{\ttyp'}}$ and $\disjoint{\twars'}{\ftv\ttyp}$. Then,
$\tnam\,\tvarc\subtype\tnam\,\tvarcp \entails
\forall\twars.\exists\twars'.\ttyp\subtype\ttyp'$ must
hold.
\end{definition}

% The constraint $\forall\twars.\exists\twars'.\ttyp\subtype\ttyp'$ expresses
% the requirement that $\exists\twars.\ttyp$ be more general than
% $\exists\twars'.\ttyp'$. % TEMPORARY expliquer plus? pas facile

The effect of this existential type definition is to enrich the programming
language with a new unary constructor $\expack\tnam$ and with a 
new construct: $\et ::= \ldots \mid \exopen\tnam\et\et$ and
$\ec ::= \ldots \mid \exopen\tnam\ec\et \mid \exopen\tnam\ev\ec$.
Their operational semantics is as follows:
%
\infax[R-Open-Ex]{
\exopen\tnam{(\expack\tnam{\ev_1})}{\ev_2} \reduces \eapp{\ev_2}{\ev_1}
}
%
In the literature, the second argument of $\kxopen\tnam$ is often required to
be a $\lambda$-abstraction $\efun\evar\et$, so the construct becomes
$\exopen\tnam\et{(\efun\evar\et)}$, often written
$\eapp[5]{\kxopen\tnam}\et{\kwtt{as}}\evar{\kwtt{in}}\et$.

Provided $\disjoint\tvars\twars$, we extend the initial environment $\ienv$
with the binding $\expack\tnam:\forall\tvars\twars.\ttyp\arw\tnam\,\tvarc$.
The constraint generation rule for $\kxopen\tnam$ is as follows:
%
\begin{bnf}
\calcule{\exopen\tnam{\et_1}{\et_2}}{\ttyp'}
\eq
\exists\tvars.(
  \calcule{\et_1}{\tnam\,\tvarc} \wedge
  \calcule{\et_2}{\dmscheme\twars{\ttyp\arw\ttyp'}}
)
\end{bnf}%
%
The rule implicitly requires that $\tvars$ be fresh for the left-hand side,
that $\twars$ be fresh for $\ttyp'$, and that
$\datadef\tnam\tvarc{\dmscheme\twars\ttyp}$ be (an $\alpha$-variant of) the
definition of $\tnam$. The left-hand conjunct simply requires $\et_1$ to have
type $\tnam\,\tvarc$. The right-hand conjunct states that the function $\et_2$
must be prepared to accept an argument of type $\ttyp$, for \emph{any}
$\twars$, and produce a result of the expected type $\ttyp'$. In other words,
$\et_2$ must be a polymorphic function.

The type scheme of existential $\expack\tnam$ resembles that of universal
$\euopen\tnam$, while the constraint generation rule for existential
$\kxopen\tnam$ is a close cousin of that for universal $\kupack\tnam$. Thus,
the duality between universal and existential types is rather strong. The main
difference lies in the fact that the existential $\kxopen\tnam$ construct is
\emph{binary}, rather than unary, so as to limit the scope of the newly
introduced type variables $\twars$. The duality may be better understood by
studying the encoding of existential types in terms of universal
types \cite{REYNOLDS83} \TAPLSECTION{24.3}.

As expected, \Rule{R-Open-Ex} preserves types.
%
\begin{theorem}[Subject reduction]
\label{theorem-sr-ex}
$(\Rule{R-Open-Ex})\subseteq(\sr)$.
\end{theorem}
%
\begin{exercise}[\Easy, \nosolution]
Prove Theorem~\ref{theorem-sr-ex}. The proof is analogous, although not
identical, to that of Theorem~\ref{theorem-sr-univ}.
\end{exercise}

In the presence of side effects, the new production $\ec ::= \ldots \mid
\exopen\tnam\ev\ec$ is problematic. The standard workaround is to restrict
the second argument to $\kxopen\tnam$ to be a value.
\end{FULL}

\section{Rows}
\label{section-rows}

\index{row variables|see{type inference}}
\index{type inference!row variables|(}
\index{type inference!records|(}

In \S\ref{section-calc-lang}, we have shown how to extend \MLlang with
algebraic data types, that is, variant and record type definitions, which we
now refer to as \emph{simple}. This mechanism has a severe limitation: two
distinct definitions must define incompatible types. As a result, one cannot
hope to write code that uniformly operates over variants or records of
different shapes, because the type of such code is not even expressible.

For instance, it is impossible to express the type of the \emph{polymorphic
record access} operation, which retrieves the value stored at a particular
field $\elab$ inside a record, {regardless} of which other fields are
present. Indeed, if the label $\elab$ appears with type $\ttyp$ in the
definition of the simple record type $\tnam\,\tvarc$, then the associated
record access operation has type $\dmscheme\tvars{\tnam\,\tvarc\arw\ttyp}$. If
$\elab$ appears with type $\ttyp'$ in the definition of another simple record
type, say $\tnam'\,\tvarcp$, then the associated record access operation has
type $\dmscheme{\tvars'}{\tnam'\,\tvarcp\arw\ttyp'}$; and so on. The most
precise type scheme that subsumes all of these incomparable type schemes is
$\dmscheme{\tvar\twar}{\tvar\arw\twar}$. It is, however, not a sound type
scheme for the record access operation. Another powerful operation whose type
is currently not expressible is \emph{polymorphic record extension}, which
copies a record and stores a value at field $\elab$ in the copy, possibly
creating the field if it did not previously exist, again {regardless} of
which other fields are present. (If $\elab$ was known to previously exist,
the operation is known as \emphindex{polymorphic record update}\index{record
update, polymorphic}.)

In order to assign types to polymorphic record operations, we must do away
with record type definitions: we must replace {named} record types, such
as $\tnam\,\tvarc$, with {structural} record types that provide a direct
description of the record's domain and contents. (Following the analogy
between a record and a partial function from labels to values, we use the word
\emph{domain} to refer to the set of fields that are defined in a record.) For
instance, a product type is structural: the type $\ttyp_1\times\ttyp_2$ is the
(undeclared) type of pairs whose first component has type $\ttyp_1$ and whose
second component has type $\ttyp_2$. Thus, we wish to design record types that
behave very much like product types. In doing so, we face two orthogonal
difficulties. First, as opposed to pairs, records may have different
domains. Because the type system must statically ensure that no undefined
field is accessed, information about a record's domain must be made part of
its type. Second, because we suppress record type definitions, labels must now
be predefined. However, for efficiency and modularity reasons, it is
impossible to explicitly list {every} label in existence in every record
type.

In what follows, we explain how to address the first difficulty in the simple
setting of a finite set of labels. Then we introduce \emph{rows}, which allow
dealing with an infinite set of labels, and address the second difficulty. We
define the syntax and logical interpretation of rows, study the new constraint
equivalence laws that arise in their presence, and extend the first-order
unification algorithm with support for rows. Then we review several
applications of rows, including polymorphic operations on records, variants,
and objects, and discuss alternatives to rows.

Because our interest is in typechecking and type inference issues, we do not
address the compilation issue: how does one efficiently compile polymorphic
records or polymorphic variants? A few relevant papers
are \longcite{pugh-weddell-90}, \longcite{ohori-95}, and \longcite{garrigue-98}.
The problem of optimizing message dispatch in object-oriented
languages, which has received considerable attention in the literature, is
related.
% NOTE On pourrait citer un survey sur la compilation du message dispatch.

\subsection*{Records with Finite Carrier}
\label{section-finite-carrier}

\newcommand{\rcd}{\typi}
\newcommand{\trcd}[3]{\tapp[3]\rcd{#1}{#2}{#3}}
\newcommand{\option}{{\kwd{option}}}
\newcommand{\toption}[1]{\tapp\option {#1}}

Let us temporarily assume that $\rowlabels$ is finite. In fact, for the sake
of definiteness, let us assume that $\rowlabels$ is the three-element set
$\set {\elab_a, \elab_b, \elab_c}$.

To begin, let us consider only \emph{full} records, whose domain is exactly
$\rowlabels$---in other words, tuples indexed by $\rowlabels$. To describe
them, it is natural to introduce a type constructor $\rcd$ of {\tcsignature}
$\normalkind\kindprod\normalkind\kindprod\normalkind\kindarrow\normalkind$.
The type $\trcd{\ttyp_a}{\ttyp_b}{\ttyp_c}$ represents all records where the
field $\elab_a$ (respectively\ $\elab_b$, $\elab_c$) contains a value of type
$\ttyp_a$ (respectively\ $\ttyp_b$, $\ttyp_c$). Note that $\rcd$ is nothing
but a product type constructor of arity 3. The basic operations on records,
namely \emph{creation} of a record out of a default value, which is stored
into every field, \emph{update} of a particular field (say, $\elab_b$), and
\emph{access} to a particular field (say, $\elab_b$), may be assigned the
following type schemes:
$$\begin{array}{rl}
\inject\cdot :& \dmscheme\tvar{\tvar\arw\trcd\tvar\tvar\tvar} \\
\override{\elab_b}\cdot\cdot :&
   \dmscheme{\tvar_a\tvar_b\tvar'_b\tvar_c}
            {
      \trcd{\tvar_a}{\tvar_b}{\tvar_c} \arw \tvar'_b \arw
      \trcd{\tvar_a}{\tvar'_b}{\tvar_c} 
   } \\
\project{\elab_b}\cdot :&
   \dmscheme{\tvar_a\tvar_b\tvar_c}{\trcd{\tvar_a}{\tvar_b}{\tvar_c}
                                    \arw\tvar_b}
\end{array}
$$
Here, polymorphism allows updating or accessing a field without knowledge of
the types of the other fields. This flexibility stems from the key
property that all record types are formed using a single $\rcd$ type
constructor.

This is fine, but in general, the domain of a record is not necessarily
$\rowlabels$: it may be a subset of~$\rowlabels$. How may we deal with this
fact while maintaining the above key property? A naive approach consists of
encoding arbitrary records in terms of full records, using the standard
algebraic data type $\option$, whose definition is
$
\datadef\option\tvar{\tapp \pre \tvar + \abs}.
$
We use $\pre$ for \emphfull{present} and $\abs$ for \emphfull{absent}: indeed,
a field that is defined with value $\ev$ is encoded as a field with value
$\eapp\pre\ev$, while an undefined field is encoded as a field with value
$\abs$. Thus,
an arbitrary record whose fields, \emphfull{if present}, have types $\ttyp_a$,
$\ttyp_b$, and $\ttyp_c$, respectively, may be encoded as a full record of
type $\trcd{(\toption{\ttyp_a})}{(\toption{\ttyp_b})}{(\toption{\ttyp_c})}$.
This naive approach suffers from a serious drawback: record types still
contain no domain information. As a result, field access must involve
a dynamic check, so as to determine whether the desired field is present;
in our encoding, this corresponds to the use of $\ecase_\option$.

To avoid this overhead and increase programming safety, we must move this
check from runtime to compile time. In other words, we must make the type
system aware of the difference between $\pre$ and $\abs$. To do so, we replace
the definition of $\option$ by two separate algebraic data type definitions,
namely $\datadef{\pre}{\tvar}{\tapp\pre\tvar}$ and $\datadef{\abs}{}{\abs}$.
In other words, we introduce a unary type constructor $\pre$, whose only
associated data constructor is $\pre$, and a nullary type constructor $\abs$,
whose only associated data constructor is $\abs$. Record types now contain
domain information; for instance, a record of type
$\trcd\abs{(\tapp\pre{\ttyp_b})}{(\tapp\pre{\ttyp_c})}$ must have domain
$\set{\elab_b,\elab_c}$. Thus, the type of a field tells whether it is
defined. Since the type $\pre$ has no data constructors other than $\pre$, the
accessor $\edest\pre$, whose type is $\dmscheme {\tvar}{\tapp \pre \tvar \arw
\tvar}$, and which allows retrieving the value stored in a field, cannot fail.
Thus, the dynamic check has been eliminated.

To complete the definition of our encoding, we now define operations on
arbitrary records in terms of operations on full records. To distinguish
between the two, we write the former with angle braces, instead of curly
braces. The \emph{empty record} $\rnil$, where all fields are undefined, may
be defined as $\inject\abs$. \emph{Extension} at a particular field (say,
$\elab_b$) $\extend{\elab_b}\cdot\cdot$ is defined as $\efun \erar {\efun
\evar \override {\elab_b} {\eapp \pre \evar} \erar}$.  \emph{Access} at a
particular field (say, $\elab_b$) $\access{\elab_b}\cdot$ is defined as
$\efun\evar{\edest\pre{\project{\elab_b}\evar}}$. It is straightforward to
check that these operations have the following principal type schemes:
$$\begin{array}{rl}
\rnil :& \trcd\abs\abs\abs \\
\extend{\elab_b}\cdot\cdot :&
   \dmscheme {\tvar_a\tvar_b\tvar'_b\tvar_c} {
      \trcd {\tvar_a}{\tvar_b}{\tvar_c} \arw \tvar'_b \arw
      \trcd {\tvar_a}{(\tapp\tpre{\tvar'_b})}{\tvar_c} 
   }
\\
\access{\elab_b}\cdot :&
  \dmscheme{\tvar_a\tvar_b\tvar_c}{
       \trcd{\tvar_a}{(\tapp\tpre{\tvar_b})}{\tvar_c} \arw \tvar_b}
\end{array}
$$
It is important to notice that the type schemes associated with extension and
access at $\elab_b$ are polymorphic in $\tvar_a$ and $\tvar_c$, which now
means that \emphfull{these operations are insensitive, not only to the type, but
also to the presence or absence of the fields $\elab_a$ and $\elab_c$}.
Furthermore, extension is polymorphic in $\tvar_b$, which means that it is
insensitive to the presence or absence of the field $\elab_b$ in its
argument. The subterm $\tapp\pre{\tvar'_b}$ in its result type reflects the
fact that $\elab_b$ is defined in the extended record. Conversely, the
subterm $\tapp\pre{\tvar_b}$ in the type of the access operation reflects the
requirement that $\elab_b$ be defined in its argument.

Our encoding of arbitrary records in terms of full records was carried out for
pedagogical purposes. In practice, no such encoding is necessary: the
\emph{data} constructors $\pre$ and $\abs$ have no machine representation, and
the compiler is free to lay out records in memory in an efficient manner. The
encoding is interesting, however, because it provides a natural way of
introducing the \emph{type} constructors $\pre$ and $\abs$, which play an
important role in our treatment of polymorphic record operations.

Once we forget about the encoding, the arguments of the type
constructor $\rcd$ are expected to be either type variables or formed with
$\pre$ or $\abs$, while, conversely, the type constructors $\pre$ and $\abs$
are not intended to appear anywhere else. It is possible to enforce this
invariant using kinds.  In addition to $\normalkind$, let us introduce the
kind $\fieldkind$ of \emph{field types}. Then, let us adopt the following
{\tcsignature s}: $\tpre : \normalkind \kindarrow \fieldkind$, $\tabs :
\fieldkind$, and $\rcd : \fieldkind \kindprod \fieldkind \kindprod \fieldkind
\kindarrow \normalkind$.
%
\begin{exercise}[\Recommended, \QuickCheck, \nosolution]
Check that the three type schemes given above are well-kinded. What is the
kind of each type variable?
\end{exercise}
%
\begin{exercise}[\Recommended, \Easy]
\solref{naive-record-types}
Our $\rcd$ types contain information about every field, regardless of whether
it is defined: we encode definedness information within the type of each
field, using the type constructors $\pre$ and $\abs$. A perhaps more natural
approach would be to introduce a family of record type constructors, indexed
by the subsets of $\rowlabels$, so that the types of records with different
domains are formed with different constructors. For instance, the empty record
would have type $\set{}$; a record that defines the field $\elab_a$ only would
have a type of the form $\set {\elab_a : \ttyp_a}$; a record that defines the
fields $\elab_b$ and $\elab_c$ only would have a type of the form $\set
{\elab_b : \ttyp_b; \elab_c : \ttyp_c}$; and so on. Assuming that the type
discipline 
is Damas and Milner's (that is, assuming an equality-only syntactic model),
would it be possible to assign satisfactory type schemes to polymorphic record
access and extension? Would it help to equip record types with a
nontrivial subtyping relation?
\end{exercise}

\subsection*{Records with Infinite Carrier}
\label{section-infinite-carrier}

The treatment of records described above is
not quite satisfactory, from practical and theoretical points of view.  First,
in practice, the set $\rowlabels$ of all record labels that appear within a
program could be very large. Because {every} record type is just as large
as $\rowlabels$ itself, even if the record that it describes only has a few
fields, this is unpleasant. Furthermore, in a modular setting, the set of all
record labels that appear within a program cannot be determined until
{link} time, so it is still unknown at {compile} time, when each
compilation unit is separately typechecked. As a result, it may only be
assumed to be a subset of the infinite set of all syntactically valid record
labels. Resolving these issues requires coming up with a treatment of records
that does not become more costly as $\rowlabels$ grows and that, in fact,
allows $\rowlabels$ to be infinite. Thus, from here on, let us assume that
$\rowlabels$ is infinite.

As in the previous section, we first concentrate on {full} records, whose
domain is exactly $\rowlabels$. The case of arbitrary records, whose domain is
a subset of $\rowlabels$, will then follow in the same manner, by using the
type constructors $\tcpre$ and $\tcabs$ to encode domain information.

Of course, even though we have assumed that $\rowlabels$ is infinite, we must
ensure that every record has a finite representation. We choose to restrict
our attention to records that are \emph{almost constant}, that is, records
where all fields but a finite number contain the same value. Every such record
may be defined in terms of two primitive operations, namely (i)
\emph{creation} of a constant record out of a value; for instance,
$\inject\efalse$ is the record where every field contains the value $\efalse$;
and (ii) \emph{update} of a record at a particular field; for instance,
$\override\elab{1}{\inject\efalse}$ carries the value $1$ at field $\elab$ and
the value $\efalse$ at every other field. As usual, an \emph{access} operation
allows retrieving the contents of a field. Thus, the three primitive
operations are the same as in the previous subsection, only in
the setting of an infinite number of fields.

If we were to continue as before, we would
now introduce a type constructor $\rcd$, \emphfull{equipped with an infinite
family of type parameters}. Because types must remain finite objects, we
cannot do so. Instead, we must find a finite (and economical) representation
of such an infinite family of types. This is precisely the role played by
\emph{rows}.

A row is a type that denotes a function from labels to types or,
equivalently, a family of types indexed by labels. Its domain is
$\rowlabels$---the row is then \emph{complete}---or a cofinite subset of
$\rowlabels$---the row is then \emph{incomplete}. (A subset of $\rowlabels$
is cofinite if and only if its complement is finite. Incomplete rows are used
only as building blocks for complete rows.) Because rows must admit a finite
representation, we build them out of two syntactic constructions, namely % En fait il y a aussi les $\ttycon^\rowkind$.
(i) construction of a \emph{constant row} out of a type; for instance, the
notation $\rowall\tbool$ denotes a row that maps every label in its domain to
$\tbool$; and (ii) strict \emph{extension} of an incomplete row; for instance,
$(\rowat\elab\tint{\rowall\tbool})$ denotes a row that maps $\elab$ to $\tint$
and every other field in its domain to $\tbool$. Formally, $\rowall$ is a
unary type constructor, while, for every label $\elab$,
$(\rowat\elab\cdot\cdot)$ is a binary type constructor. These two
constructions are reminiscent of the two operations used above to build
records. There are, however, a couple of subtle but important
differences. First, $\rowall\ttyp$ may be a {complete or incomplete}
row. Second, $(\rowat\elab\ttyp{\ttyp'})$ is defined only if $\elab$ is not in
the domain of the row $\ttyp'$, so this construction is strict extension, not
update.  These aspects are made clear by a \emph{kinding discipline}, to be
introduced later on\iffull{(\S\ref{section-row-syntax})}.

It is possible for two syntactically distinct rows to denote the same function
from labels to types. For instance, according to the intuitive interpretation
of rows given above, the three complete rows
$(\rowat\elab\tint{\rowall\tbool})$,
$(\rowat\elab\tint{(\rowat{\elab'}\tbool{\rowall\tbool})})$, and
$(\rowat{\elab'}\tbool{(\rowat\elab\tint{\rowall\tbool})})$ denote the same
total function from labels to types. In the following, we define the logical
interpretation of types in such a way that the interpretations of these three
rows in the model are indeed equal.

We may now make the record type constructor $\typi$ a {unary} type
constructor, {whose parameter is a row}. Then, (say) $\tpi{(\rowat
\elab\tint{\rowall\tbool})}$ is a record type, and we intend it to be a valid
type for the record $\override\elab{1}{\inject\efalse}$. The basic operations
on records may be assigned the following type schemes:
$$\begin{array}{rl}
\inject\cdot :& \dmscheme\tvar{\tvar\arw\tpi{(\rowall\tvar)}} \\
\override\elab\cdot\cdot :&
   \dmscheme{\tvar\tvar'\twar}
            {
      \tpi{(\rowat\elab\tvar\twar)} \arw \tvar' \arw
      \tpi{(\rowat\elab{\tvar'}\twar)}
   } \\
\project\elab\cdot :&
   \dmscheme{\tvar\twar}{\tpi{(\rowat\elab\tvar\twar)}
                                    \arw\tvar}
\end{array}$$
These type schemes are reminiscent of those  given above.
However, in the previous section, the size of the type schemes was linear in the cardinal of
$\rowlabels$, whereas here it is constant, even though $\rowlabels$ is infinite. This is
made possible by the fact that record types no longer list all labels in existence; instead,
they use rows. In the type scheme assigned to record creation, the constant row $\rowall\tvar$
is used to indicate that all fields have the same type in the newly created record. In the
next two type schemes, the row $(\rowat\elab{\tvar_\elab}\tvar)$  is used to separate the
type $\tvar_\elab$, which describes the contents of the field $\elab$, 
and the row $\tvar$, which collectively describes the contents of all other fields.
Here, the type variable $\tvar$ stands for an arbitrary row; it is often referred to
as a \emph{row variable}. The ability of quantifying over row and type variables alike
confers great expressiveness to the type system.

We have explained, in an informal manner, how rows allow typechecking
operations on {full} records, in the setting of an infinite set of
labels. We return to this issue in Example~\ref{example-full-records}. To deal
with the case of arbitrary records, whose domain is finite, we rely on the
field type constructors $\tcpre$ and $\tcabs$, as explained previously. We
return to this point in Example~\ref{example-extensible-records}. In the
following, we give a formal exposition of rows. We begin with their syntax and
logical interpretation.  Then we give some new constraint equivalence laws,
which characterize rows, and allow extending our first-order unification
algorithm with support for rows. We conclude with several illustrations of the
use of rows and some pointers to related work.

\subsection*{Syntax of Rows}
\label{section-row-syntax}

In the following, the set of labels $\rowlabels$ is considered denumerable. We
let $\elabs$ range over finite subsets of~$\rowlabels$. When $\elab \notin
\elabs$ holds, we write $\elab.\elabs$ for $\set\elab \uplus \elabs$. Before explaining
how the syntax of types is enriched with rows, we introduce \emph{row kinds},
whose grammar is as follows:
%
$$\rowkind ::= \type \mid \row \elabs$$
%
Row kinds help distinguish between three kinds of types, namely ordinary
types, complete rows, and incomplete rows. While ordinary types are used to
describe expressions, complete or incomplete rows are used only as building
blocks for ordinary types. For instance, the record type $\tpi{(\rowat
\elab\tint{\rowall\tbool})}$, which was informally introduced above, is
intended to be an ordinary type, that is, a type of row kind $\type$. Its
subterm $(\rowat \elab\tint{\rowall\tbool})$ is a complete row, that is, a
type of row kind $\row\varnothing$. Its subterm $\rowall\tbool$ is an incomplete
row, whose row kind is $\row{\{\ell\}}$. Intuitively, a row of kind $\row
\elabs$ denotes a family of types whose domain is $\rowlabels \setminus
\elabs$. In other words, $\elabs$ is the set of labels that the row does not
define. The purpose of row kinds is to outlaw meaningless types, such as
$\tpi{(\tint)}$, which makes no sense because the argument to the record type
constructor $\typi$ should be a (complete) row, or
$(\rowat\elab{\ttyp_1}{\rowat\elab{\ttyp_2}{\rowall\tbool}})$, which makes no
sense because no label may occur twice within a row.

Let us now define the syntax of types in the presence of rows. As usual, it is
given by a signature $\sig$ (Definition~\ref{def-signature}), which lists all
type constructors together with their \tcsignature s. Here, for the sake of
generality, we do not wish to give a {fixed} signature $\sig$. Instead,
we give a {procedure} that builds $\sig$ out of two simpler signatures,
referred to as $\sig_0$ and $\sig_1$. The input signature $\sig_0$ lists the
type constructors that have nothing to do with rows, such as $\arw$, $\times$,
$\tint$, etc. The input signature $\sig_1$ lists the type constructors that
allow a row to be a subterm of an ordinary type, such as the record type
constructor $\typi$. In a type system equipped with extensible variant types
or with object types, there might be several such type constructors; see
the sections on Polymorphic Variants (p.~\pageref{section-variants}) and
Other Applications of Rows (p.~\pageref{section-other-apps}). Without loss of
generality, we assume that all type constructors in $\sig_1$ are unary. The
point of parameterizing the definition of $\sig$ over $\sig_0$ and $\sig_1$ is
to make the construction more general: instead of defining a {fixed} type
grammar featuring rows, we wish to explain how to enrich an {arbitrary}
type grammar with rows.

In the following, we let $\ttycon$ (respectively\ $\ptycon$) range over the type
constructors in $\sig_0$ (respectively\ $\sig_1$). We let $\kind$ range over the
kinds involved in the definition of $\sig_0$ and $\sig_1$, and refer to them
as \emph{basic kinds}. We let $\tycon$ range over the type constructors in
$\sig$. The kinds involved in the definition of $\sig$ are \emph{composite
kinds}, that is, pairs of a basic kind $\kind$ and a row kind $\rowkind$,
written $\kind.\rowkind$. This allows the kind discipline enforced by $\sig$
to reflect that enforced by $\sig_0$ and $\sig_1$ and  also to impose
restrictions on the structure and use of rows, as suggested above. For the
sake of conciseness, we write $\dk.\rowkind$ for the mapping $(d \mapsto
\dk(d).\rowkind)^{d \in \Dom \dk}$ and $(\dk \kindarrow \kind).\rowkind$ for
the (composite) kind signature $\dk.\rowkind \kindarrow \kind.\rowkind$. (In
other words, we let $.\rowkind$ distribute over basic signatures.)
We use symmetric notations to build a composite kind signature out of a basic
kind and a row kind signature.
%
% Définition de la signature S des types avec rangées.
%
\begin{definition}
The signature $\sig$ is defined as follows:
$$
#>\begin{array}{|c|l|l|}%l|}
\hline
\tycon \in \Dom {\sig} & \text {Signature} & 
\text{Conditions}
\\\hline
\ttycon^\rowkind
    & (\dk \kindarrow \kind). \rowkind
    & (\ttycon : \dk \kindarrow \kind) \in \sig_0
\\
\ptycon
    & \dk.\row \varnothing \kindarrow \kind.\type
    & (\ptycon: \dk \kindarrow \kind) \in \sig_1
\\
\rowall ^{\kind, \elabs} 
    & \kind . (\type \kindarrow \row \elabs)
&\\
\elab^{\kind, \elabs}
    & \kind . (\type \kindprod \row {\elab.\elabs}
        \kindarrow \row \elabs)
    & \elab \notin \elabs
\\\hline
\end{array}
%
% NOTE pour les deux dernières lignes de cette définition, on a un abus
% de langage: on n'a pas nommé les directions associées aux paramètres des
% constructeurs \rowat et \rowall.
%
$$
We sometimes refer to $\sig$ as the \emph{row extension of $\sig_0$ with $\sig_1$}.
\end{definition}
%
Examples~\ref{example-syntax-normal} and~\ref{example-syntax-normal-field} suggest
common choices of $\sig_0$ and $\sig_1$ and give a perhaps more concrete-looking
definition of the grammar of types that they determine. First, however, let us
explain the definition.
%
The type constructors that populate $\sig$ come in four varieties: they may be
(i) taken from $\sig_0$, (ii) taken from $\sig_1$, (iii) a unary row
constructor $\rowall$, or (iv) a binary row constructor
$(\rowat\elab\cdot\cdot)$. Let us review and explain each case.

Let us first consider case (i) and assume, for the time being, that $\rowkind$
is $\type$. Then, for every type constructor $\ttycon$ in $\sig_0$, there is a
corresponding type constructor $\ttycon^\type$ in $\sig$. For instance,
$\sig_0$ must contain an arrow type constructor $\arw$, whose {\tcsignature}
is $\{ \dirdomain\mapsto\normalkind, \dircodomain\mapsto\normalkind \}
\kindarrow\normalkind$. Then, $\sig$ contains a type constructor $\arw^\type$,
whose {\tcsignature} is $\{ \dirdomain\mapsto\normalkind.\type,
\dircodomain\mapsto\normalkind.\type \} \kindarrow\normalkind.\type$. Thus,
$\arw^\type$ is a binary type constructor whose parameters and result must
have basic kind $\normalkind$ and must have row kind $\type$; in other words,
they must be ordinary types, as opposed to complete or incomplete rows. The
family of all type constructors of the form $\ttycon^\type$, where $\ttycon$
ranges over $\sig_0$, forms a copy of $\sig_0$ at row kind $\type$: one might
say, roughly speaking, that $\sig$ \emphfull{contains} $\sig_0$. This is not
surprising, since our purpose is to \emphfull{enrich} the existing signature
$\sig_0$ with syntax for rows.

Perhaps more surprising is the existence of the type constructor
$\ttycon^\rowkind$, for every $\ttycon$ in $\sig_0$, \emphfull{and for every row
kind $\rowkind$}. For instance, for every $\elabs$, $\sig$ contains a type
constructor $\arw^{\row\elabs}$, whose {\tcsignature} is $\{
\dirdomain\mapsto\normalkind.{\row\elabs},
\dircodomain\mapsto\normalkind.{\row\elabs} \}
\kindarrow\normalkind.{\row\elabs}$. Thus, $\arw^{\row\elabs}$ is a binary
type constructor whose parameters and result must have basic kind
$\normalkind$ and must have row kind $\row\elabs$. In other words, this type
constructor maps a pair of rows that have a common domain to a row with the
same domain. Recall that a row is to be interpreted as a family of types.  Our
intention is that $\arw^{\row\elabs}$ maps two families of types to a family
of arrow types. This is made precise in the next subsection.  One
should point out that the type constructors $\ttycon^\rowkind$, with
$\rowkind\not=\type$, are required only in some advanced applications of rows;
Examples~\ref{example-record-apply} and~\ref{example-first-class-msg} provide
illustrations. They are not used when assigning types to the usual
primitive operations on records, namely creation, update, and access
(Examples~\ref{example-full-records} and~\ref{example-extensible-records}).

Case (ii) is simple: it simply means that $\sig$ \emph{contains} $\sig_1$. It
is only worth noting that every type constructor $\ptycon$ maps a parameter of
row kind $\row\varnothing$ to a result of row kind $\type$, that is, a complete
row to an ordinary type. Thanks to this design choice, the type
$\tpi{(\tint^\type)}$ is invalid: indeed, $\tint^\type$ has row kind $\type$,
while $\typi$ expects a parameter of row kind $\row\varnothing$.

Cases (iii) and (iv) introduce new type constructors that were not present
in $\sig_0$ or $\sig_1$ and allow forming rows. They were informally
described in the previous subsection.
%
First, for every $\kind$ and $\elabs$, there is a \emph{constant row
constructor} $\rowall^{\kind,\elabs}$. Its parameter must have row kind
$\type$, while its result has row kind $\row\elabs$; in other words, this type
constructor maps an ordinary type to a row. It is worth noting that the row
thus built may be complete or incomplete; for instance,
$\rowall^{\normalkind,\varnothing}\,\tbool$ is a complete row, and may be
used, for example, to build the type $\tpi{(\rowall^{\normalkind,\varnothing}\,\tbool)}$,
while $\rowall^{\normalkind,\{\elab\}}\,\tbool$ is an incomplete row, and may
be used, for example, to build the type
$\tpi{(\rowat\elab\tint{\rowall^{\normalkind,\{\elab\}}\,\tbool)}}$.
%
Second, for every $\kind$, $\elabs$, and $\elab\notin\elabs$, there is a
\emph{row extension constructor} $\elab^{\kind,\elabs}$. We usually write
$\rowat [\kind,\elabs] \elab {\ttyp_1} \ttyp_2$ for $\tapp[2] {\elab^{\kind,
\elabs}} {\ttyp_1} {\ttyp_2}$ and let this symbol be right associative so as
to recover the familiar list notation for rows. According to the definition of
$\sig$, if $\ttyp_2$ has row kind $\row{\elab.\elabs}$, then $\rowat
[\kind,\elabs] \elab {\ttyp_1} \ttyp_2$ has row kind $\row\elabs$. Thanks to
this design choice, the type $(\rowat[\normalkind,\elabs]\elab{\ttyp_1}
{\rowat[\normalkind,\elabs]\elab{\ttyp_2}{\rowall^{\normalkind,\elab.\elabs}\,\tbool}})$
is invalid; indeed, the outer $\elab$ expects a parameter of row kind
$\row{\elab.\elabs}$, while the inner $\elab$ produces a type of row kind
$\row\elabs$.

The superscripts carried by the type constructors $\ttycon$, $\elab$, and
$\rowall$ in the signature $\sig$ make all kind information explicit,
obviating the need for assigning several kinds to a single type constructor.
In practice, however, we often drop the superscripts and use
\emph{unannotated} types. No ambiguity arises because, given a type expression
$\ttyp$ of known kind, it is possible to reconstruct all superscripts in a
unique manner. This is the topic of the next example and exercises.
%
\begin{example}[Ill-kinded types]
Assume that $\sig_0$ contains type constructors $\tint$ and $\arw$, whose
{\tcsignature s} are respectively $\normalkind$ and
$\normalkind\kindprod\normalkind\kindarrow\normalkind$, and that $\sig_1$
contains a type constructor $\typi$, whose {\tcsignature} is $\normalkind
\kindarrow \normalkind$.

The unannotated type $\tvar \arw \typi (\tvar)$ is invalid. Indeed, because
$\typi$'s image row kind is $\type$, the arrow must be $\arw^\type$. Thus, the
leftmost occurrence of $\tvar$ must have row kind $\type$. On the other hand,
because $\typi$ expects a parameter of row kind $\row\varnothing$, its
rightmost occurrence must have row kind $\row\varnothing$---a
contradiction. The unannotated type $\tvar \arw \typi (\rowall\tvar)$ is,
however, valid, provided $\tvar$ has kind $\normalkind.\type$. In fact, it is
the type of the primitive record creation operation.

The unannotated type $(\rowat {\elab} \ttyp {\rowat \elab {\ttyp} {\ttyp''}})$
is also invalid: there is no way of reconstructing the missing superscripts so
as to make it valid. Indeed, the row $(\rowat \elab {\ttyp'} {\ttyp''})$ must
have row kind $\row \elabs$ for some $\elabs$ that does not contain $\elab$.
However, the context where it occurs requires it to also have row kind
$\row\elabs$ for some $\elabs$ that does contain $\elab$. This makes it
impossible to reconstruct consistent superscripts.

Any type of the form $\typi (\typi (\ttyp))$ is invalid, because the outer
$\typi$ expects a parameter of row kind $\row\varnothing$, while the inner
$\typi$ 
constructs a type of row kind $\type$. This is an intentional limitation:
unlike 
those of $\sig_0$, the type constructors of $\sig_1$ are not lifted to every
row kind $\rowkind$. (If they were, we would be led to work not only with
rows of ordinary types, but also with rows of rows, rows of rows of rows,
and so on. \longcite{Remy90:Thesis} explores this avenue.) 
% FIXME est-ce exact?
\end{example}
%
\begin{exercise}[\Recommended, \QuickCheck]
Consider the unannotated type $$\tvar \arw \typi
(\rowat {\elab} {\tint} (\twar \arw {\rowall \tvar})).$$ Can you guess the
kind of the type variables $\tvar$ and $\twar$, as well as the missing
superscripts, so as to ensure that this type has kind $\normalkind.\type$?
\solref{rowkindinfer}
\end{exercise}
%
\begin{exercise}[\Moderate, \nosolution]
Propose a {kind checking} algorithm that, given an unannotated type
$\ttyp$, given the kind of $\ttyp$, and given the kind of all type variables
that appear within $\ttyp$, ensures that $\ttyp$ is well-kinded, and
reconstructs the missing superscripts within $\ttyp$. Next, propose a
{kind inference} algorithm that, given an unannotated type $\ttyp$,
{discovers} the kind of $\ttyp$ and the kind of all type variables that
appear within $\ttyp$ so as to ensure that $\ttyp$ is well-kinded.
\solref{kinds-inference}
\end{exercise}
\iffalse
\begin{solution}{kind-inference}
We define $\sem{\ttyp}_{\kind.\rowkind}$ as the term $\ttyp$ annotated
with all superscript when $\ttyp$ is unannotated of kind $\kind.\rowkind$,
recursively defined as follows.
$$
\begin{array}{lr}
\sem {\tapp \ttycon \ttypc}_{\kind.\rowkind} =
\tapp {\ttycon^\rowkind} 
(\dird \mapsto \sem {\ttypc(\dird)}_{\dk(\dird).\rowkind})^{\dird \in \Dom \dk}
&\text {if $G : \dk \kindarrow \kind \in \sig_0$}
\\[2mm]
\sem {\tapp \ptycon \ttypc}_{\kind.\type} =
\tapp \ptycon
(\sem {\ttypc(\dird)}_{\dk(\dird).\row \varnothing})^{\dird \in \Dom \dk} 
&\text {if $H : \dk \kindarrow \kind' \in \sig_0$}
\\[2mm]
\sem {\rowall \ttyp}_{\kind.\row \elabs} = 
\rowall^{\kind.\elabs} {\sem {\ttyp}_{\kind.\type}}
\\[2mm]
\sem {\rowat \ell \ttyp {\ttyp'}}_{\kind. \row \elabs} = 
\rowat[\kind.\elabs] \ell {\sem {\ttyp}_{\kind.\type}}
        {\sem {\ttyp'}_{\kind.\row {\ell.\elabs}}}
& \ell \notin \elabs
\end{array}
$$
The function is partial, and undefined exactly when typed cannot be
annotated so as to be of the required kind. 
%
When the kind of the term is unspecified, there may be several annotations
that are well-kinded. For instance, if $\ttyp$ of kind $\kind.\type$ then
$\rowall [\kind.\elabs] \ttyp$ is well kinded for all set of labels $\elabs$. 
% 
So it is not possible to reconstruct the superscripts unambiguously. 
However, it is still possible to check the existence of at least one
annotation. For this purpose, one may introduce kind variables $x$ and 
label-set variables $l$, and apply the above transformation when kinds may
not be kind variables. Each side condition generates an equation of the form
$x = \kind$, $x = \rowkind$, or $l \notin \elabs$. In particular, expressions $\elabs$ 
may not be $\varnothing$, $l$ or $\ell.\elabs$. 

% TEMPORARY To be finished.
\end{solution}
\fi

We have given a very general definition of the syntax of types. In this view,
types, ranged over by the meta-variable $\ttyp$, encompass both ``ordinary''
types and rows: the distinction between the two is established only via the
kind system. In the literature, however, it is common to establish this
distinction by letting {distinct} meta-variables, say $\ttyp$ and $\rtyp$,
range over ordinary types and rows, respectively, so as to give the syntax a
more concrete aspect. The next two examples illustrate this style and suggest
common choices for $\sig_0$ and $\sig_1$.
%
\begin{example}
\label{example-syntax-normal}
Assume that there is a single basic kind $\normalkind$, that $\sig_0$ consists
of the arrow type constructor $\arw$, whose {\tcsignature} is
$\normalkind\kindprod\normalkind\kindarrow\normalkind$, and that $\sig_1$
consists of the record type constructor $\typi$, whose {\tcsignature} is
$\normalkind\kindarrow\normalkind$. Then, the composite kinds are
$\normalkind.\type$ and $\normalkind.\row\elabs$, where $\elabs$ ranges over
the finite subsets of $\rowlabels$. Let us employ $\ttyp$ (respectively\ $\rtyp$) to
range over types of the former (respectively\ latter) kind, and refer to them as
ordinary types (respectively\ rows). Then, the syntax of types, as defined by the
signature $\sig$, may be presented under the following form:
$$\begin{array}{rcl}
\ttyp & ::= & \tvar \mid \ttyp\arw\ttyp \mid \tpi\rtyp \\
\rtyp & ::= & \tvar \mid \rtyp\arw\rtyp \mid (\rowat\elab\ttyp\rtyp) \mid \rowall\ttyp
\end{array}$$
Ordinary types $\ttyp$ include ordinary type variables (that is, type
variables of kind $\normalkind.\type$), arrow types (where the type
constructor $\arw$ is really $\arw^\type$), and record types, which are formed
by applying the record type constructor $\typi$ to a row. Rows $\rtyp$ include
row variables (that is, type variables of kind $\normalkind.\row\elabs$ for
some $\elabs$), arrow rows (where the row constructor $\arw$ is really
$\arw^{\row\elabs}$ for some $\elabs$), row extension (whereby a row $\rtyp$
is extended with an ordinary type $\ttyp$ at a certain label $\elab$), and
constant rows (formed out of an ordinary type $\ttyp$). It would be possible
to also introduce a syntactic distinction between ordinary type variables and
row variables, if desired.

Such a presentation is rather pleasant, because the syntactic segregation
between ordinary types and rows makes the syntax less ambiguous. It does not
allow getting rid of the kind system, however: (row) kinds are still necessary
to keep track of the domain of every row.
\end{example}
%
\begin{example}
\label{example-syntax-normal-field}
\let\ottyp\ttyp
\let\ortyp\rtyp
\renewcommand{\ttyp}{{\ottyp^\normalkind}}
\renewcommand{\rtyp}{{\ortyp^\normalkind}}
\newcommand{\ftyp}{{\ottyp^\fieldkind}}
\newcommand{\frtyp}{{\ortyp^\fieldkind}}
Assume that there are two basic kinds $\normalkind$ and $\fieldkind$, that
$\sig_0$ consists of the type constructors $\arw$, $\tcabs$, and $\tcpre$,
whose respective {\tcsignature s} are
$\normalkind\kindprod\normalkind\kindarrow\normalkind$, $\fieldkind$, and
$\normalkind\kindarrow\fieldkind$, and that $\sig_1$ consists of the record
type constructor $\typi$, whose {\tcsignature} is
$\fieldkind\kindarrow\normalkind$. Then, the composite kinds are
$\normalkind.\type$, $\normalkind.\row\elabs$, $\fieldkind.\type$, and
$\fieldkind.\row\elabs$, where $\elabs$ ranges over the finite subsets of
$\rowlabels$. Let us employ $\ttyp$, $\rtyp$, $\ftyp$, and $\frtyp$,
respectively, to range over types of these four kinds. Then, the syntax of
types, as defined by the signature $\sig$, may be presented under the
following form:
$$\begin{array}{rcl}
\ttyp & ::= & \tvar \mid \ttyp\arw\ttyp \mid \tpi\frtyp \\
\rtyp & ::= & \tvar \mid \rtyp\arw\rtyp \mid (\rowat\elab\ttyp\rtyp) \mid \rowall\ttyp \\
\ftyp & ::= & \tvar \mid \tabs \mid \tpre\ttyp \\
\frtyp & ::= & \tvar\mid \tabs \mid \tpre\rtyp \mid (\rowat\elab\ftyp\frtyp) \mid \rowall\ftyp
\end{array}$$
Ordinary types $\ttyp$ are as in the previous example, except the record type
constructor $\typi$ must now be applied to a row of field types $\frtyp$.
Rows $\rtyp$ are unchanged. Field types $\ftyp$ include field type variables
(that is, type variables of kind $\fieldkind.\type$) and applications of the
type constructors $\tabs$ and $\tpre$ (which are really $\tabs^\type$ and
$\tpre^\type$). Field rows $\frtyp$ include field row variables (that is, type
variables of kind $\fieldkind.\row\elabs$ for some $\elabs$), applications of
the row constructors $\tabs$ and $\tpre$ (which are really
$\tabs^{\row\elabs}$ and $\tpre^{\row\elabs}$ for some $\elabs$), row
extension, and constant rows, where row components are field types
$\ftyp$.

In many basic applications of rows, $\tabs^{\row\elabs}$ and
$\tpre^{\row\elabs}$ are never required: that is, they do not appear in the
the type schemes that populate the initial environment. (Applications where
they {are} required appear in Pottier~[\citeyropen{pottier-njc-00}].) In that case, they
may be removed from the syntax. Then, the nonterminal $\rtyp$ becomes
unreachable from the nonterminal $\ttyp$, which is the grammar's natural entry
point, so it may be removed as well. In that simplified setting, the syntax of
types and rows becomes:
$$\begin{array}{rcl}
\ttyp & ::= & \tvar \mid \ttyp\arw\ttyp \mid \tpi\frtyp \\
\ftyp & ::= & \tvar \mid \tabs \mid \tpre\ttyp \\
\frtyp & ::= & \tvar\mid (\rowat\elab\ftyp\frtyp) \mid \rowall\ftyp
\end{array}$$
This is the syntax found in some introductory accounts of
rows \cite{Remy89,pottier-njc-00}.
\end{example}

\subsection*{Meaning of Rows}
\label{section-row-meaning}

We now give meaning to the type grammar defined in the previous section by
interpreting it within a model. We choose to define a regular tree model, but
alternatives exist; see Remark~\ref{remark-row-model} below. In this model,
every type constructor whose image row kind is $\type$ (that is, every type
constructor of the form $\ttycon^\type$ or $\ptycon$) is interpreted as
itself, as in a free tree model. However, every application of a type
constructor whose image row kind is $\row\elabs$ for some $\elabs$ receives
special treatment: it is interpreted as a family of types indexed by
$\rowlabels\setminus\elabs$, which we encode as an infinitely branching tree.
To serve as the root label of this tree, we introduce, for every $\kind$ and
for every $\elabs$, a symbol $\elabs^\kind$, whose arity is
$\rowlabels\setminus\elabs$. More precisely,

\begin{definition}
The model, which consists of a
set $\model_{\kind.\rowkind}$ for every $\kind$ and $\rowkind$, is the regular
tree algebra that arises out the following signature:
$$
#>\begin{array}{|l|l|l|}%l|}
\hline
\text{Symbol} & \text {Signature} & 
\text{Conditions}
\\\hline
\ttycon
    & (\dk \kindarrow \kind). \type
    & (\ttycon : \dk \kindarrow \kind) \in \sig_0
\\
\ptycon
    & \dk.\row \varnothing \kindarrow \kind.\type
    & (\ptycon: \dk \kindarrow \kind) \in \sig_1
\\
\elabs ^{\kind} 
    & \kind . (\type ^{\rowlabels \setminus \elabs} \kindarrow \row \elabs)
    & %\kind \in \kindset, \elabs \in \fpowerset \rowlabels
\\\hline
\end{array}
$$~\noendofpf
\end{definition}

The first two lines in this signature coincide with the definitions of
$\ttycon^\type$ and $\ptycon$ in the signature $\sig$. Indeed, as stated
above, we intend to interpret these type constructors in a syntactic manner,
so each of them must have a counterpart in the model.  The third line
introduces the symbols $\elabs^\kind$ hinted at above.

According to this signature, if $\gtyp$ is a ground type of kind $\kind.\type$
(that is, an element of $\modelk{\kind.\type}$), then its head symbol
$\gtyp(\epsilon)$ must be of the form $\ttycon$ or $\ptycon$. If $\gtyp$ is a
ground type of kind $\kind.\row\elabs$, then its head symbol must be
$\elabs^\kind$, and its immediate subtrees, which are indexed by
$\rowlabels\setminus\elabs$, are ground types of kind $\kind.\type$; in other
words, the ground row $\gtyp$ is effectively a family of ordinary ground types
indexed by $\rowlabels\setminus\elabs$. Thus, our intuition that rows denote
infinite families of types is made literally true.

We have defined the model; there remains to explain how types are mapped to
elements of the model.
%
\begin{definition}
\label{def-interpretation-rows}
The interpretation of the type constructors that populate $\sig$ is defined
as follows.
\begin{enumerate}
\item Let $(\ttycon : \dk \kindarrow \kind) \in \sig_0$. Then, $\ttycon^\type$
      is interpreted as the function that maps $\dgtyp \in \modelk{\dk.\type}$
      to the ground type $\gtyp \in \modelk{\kind.\type}$ defined by
      $\gtyp(\epsilon)=\ttycon$ and $\gtyp/\dird=\dgtyp(\dird)$ for every
      $\dird\in\Dom\dk$. This is a syntactic interpretation.

\item Let $(\ptycon : \dk \kindarrow \kind) \in \sig_1$. Then, $\ptycon$
      is interpreted as the function that maps $\dgtyp \in \modelk{\dk.\row\varnothing}$
      to the ground type $\gtyp \in \modelk{\kind.\type}$ defined by
      $\gtyp(\epsilon)=\ptycon$ and $\gtyp/\dird=\dgtyp(\dird)$ for every
      $\dird\in\Dom\dk$. (Because $\ptycon$ is unary, there is exactly one
      such $\dird$.) This is also a syntactic interpretation.

\item \label{interpretation-grow} Let $(\ttycon : \dk \kindarrow \kind) \in \sig_0$. Then,
      $\ttycon^{\row\elabs}$ is interpreted as the function that maps $\dgtyp
      \in \modelk{\dk.\row\elabs}$ to the ground type $\gtyp \in
      \modelk{\kind.\row\elabs}$ defined by $\gtyp(\epsilon)=\elabs^\kind$ and
      $\gtyp(\elab)=\ttycon$ and $\gtyp/(\elab\cdot\dird)=\dgtyp(\dird)/\elab$
      for every $\elab\in\rowlabels\setminus\elabs$ and
      $\dird\in\Dom\dk$. Thus, when applied to a family of rows, the type
      constructor $\ttycon^{\row\elabs}$ produces a row where every component
      has head symbol $\ttycon$. This definition may sound quite
      technical; its effect is summed up in a simpler fashion by the
      equations \Rule{C-Row-GD} and \Rule{C-Row-GL} in the next section.

% NOTE pour les deux cas qui suivent, dans la mesure où on n'a pas nommé
% les directions associées aux paramètres de ces constructeurs, le mieux est
% de ne pas employer la notation \dgtyp, ce qui nous obligerait à tirer des indices
% 1 et 2 de notre chapeau, mais de directement présenter l'interprétation comme
% une fonction des types grounds (ou des paires de types grounds) dans les types
% grounds.

\item Interpret $\rowall^{\kind,\elabs}$ as the function that maps
      $\gtyp_1 \in \modelk{\kind.\type}$ to the ground type $\gtyp \in
      \modelk{\kind.\row\elabs}$ defined by $\gtyp(\epsilon)=\elabs^\kind$ and
      $\gtyp/\elab=\gtyp_1$ for every $\elab\in\rowlabels\setminus\elabs$.
      Note that $\gtyp/\elab$ does not depend on $\elab$: $\gtyp$ is a constant
      ground row.

\item Let $\elab\notin\elabs$. Then, $\elab^{\kind,\elabs}$ is interpreted as
      the function that maps $(\gtyp_1,\gtyp_2) \in
      \modelk{\kind.\type}\times\modelk{\kind.\row{\elab.\elabs}}$ to the
      ground type $\gtyp \in
      \modelk{\kind.\row\elabs}$ defined by $\gtyp(\epsilon)=\elabs^\kind$ and
      $\gtyp/\elab=\gtyp_1$ and $\gtyp/\elab'=\gtyp_2(\elab')$ for every
      $\elab'\in\rowlabels\setminus\elab.\elabs$. This definition is precisely
      row extension; indeed, the ground row $\gtyp$ maps $\elab$ to $\gtyp_1$
      and coincides with the ground row $\gtyp_2$ at every other label $\elab'$.~\endpf
\end{enumerate}
\end{definition}
%
Defining a model and an interpretation allows our presentation of rows to fit
within the formalism proposed earlier in \this
(\S\ref{section-constraints}). It also provides a basis for the
intuition that rows denote infinite families of types. From a formal point of
view, the model and its interpretation allow proving several constraint
equivalence laws concerning rows, which are given and discussed in
the next subsection. Of course, it is also possible to accept
these equivalence laws as axioms and give a purely syntactic account of rows
without relying on a model; this is how rows were historically dealt
with \cite{Remy/start}.

\begin{remark}
\label{remark-row-subtyping}
We have not defined the interpretation of the subtyping predicate, because
much of the material that follows is independent of it. One common approach is
to adopt a nonstructural definition of subtyping
(Example~\ref{example-subtyping-models}), where every $\elabs^\kind$ is
considered covariant in every direction, and where the variances and relative
ordering of all other symbols ($\ttycon$ and $\ptycon$) are chosen at will,
subject to the restrictions associated with nonstructural subtyping and to
the conditions necessary to ensure type soundness.

Recall that the arrow type constructor $\arw$ is contravariant in its
domain and covariant in its codomain. The record type constructor $\typi$ is
usually covariant. These properties are exploited in proofs of the subject
reduction theorem. The type constructors $\arw$ and $\typi$ are usually
incompatible.  This property is exploited in proofs of the progress
theorem. In the case of Example~\ref{example-syntax-normal}, because no type
constructors other than $\arw$ and $\typi$ are present, these conditions imply
that there is no sensible way of interpreting subtyping other than
equality. In the case of Example~\ref{example-syntax-normal-field}, two
sensible interpretations of subtyping exist: one is equality, while the other
is the nonstructural subtyping order obtained by letting
$\tcpre\symleq\tcabs$. In the former interpretation, $\tcabs$ means
``definitely absent,'' while in the latter, it means ``possibly absent.''%
\end{remark}
%
\begin{remark}
\label{remark-row-model}
The model proposed above is a regular tree model. Of course, it is possible to
adopt a finite tree model instead. Furthermore, other interpretations of rows
are possible: for instance, \longcite{faehndrich-phd-99} extends the
set constraints formalism with rows. In his model, an ordinary type is
interpreted as a set of values, while a row is interpreted as a set of
functions from labels to values. While the definition of the model may vary,
the key point is that the characteristic laws of rows, which we discuss
next, hold in the model.% 
\end{remark}

\subsection*{Reasoning with Rows}
\label{section-reasoning-rows}

\begin{widefigure}
\TTtoprule
\vspace*{-2ex}
\begin{tabular*}{\linewidth}{.C.R<{={}},>{{}}L.R.}&
(\rowat {\elab_1} {\ttyp_1} \rowat {\elab_2} {\ttyp_2} \ttyp_3) &
(\rowat {\elab_2} {\ttyp_2} \rowat {\elab_1} {\ttyp_1} \ttyp_3)
&(\DefRule{C-Row-LL})
\\&
\rowall \ttyp  & (\rowat \elab \ttyp {\rowall \ttyp})
&(\DefRule{C-Row-DL})
\\&
\tapp[3] \ttycon {\rowall \ttyp_1} \ldots {\rowall \ttyp_n} &
\rowall (\tapp[3] \ttycon {\ttyp_1} \ldots {\ttyp_n})
&(\DefRule{C-Row-GD})
\\&
\tapp[3] \ttycon 
    {(\rowat \elab {\ttyp_1} {\ttyp_1'})} \ldots {(\rowat \elab {\ttyp_n}{\ttyp_n'})} & 
(\rowat \elab 
    {\tapp[3] \ttycon {\ttyp_1} \ldots {\ttyp_n}}
    {\tapp[3] \ttycon {\ttyp_1'} \ldots {\ttyp_n'}})
&(\DefRule{C-Row-GL})
\end{tabular*}
\par\smallskip
\vspace*{.3ex}
\TTbottomrule \\
\vspace*{-2ex}
\bcpcaption{row-equations}{Equational reasoning with rows}
\end{widefigure}

The interpretation presented in the previous section was designed  to
support the intuition that a row denotes an infinite family of types, indexed
by labels, that the row constructor $\rowat\elab\cdot\cdot$ denotes row
extension, and that the row constructor $\rowall$ denotes the creation of a
constant row. From a formal point of view, the definition of the model and
interpretation may be exploited to establish some reasoning principles
concerning rows. These principles take the form of equations between types
(Figure~\ref{fig:row-equations}) and  constraint equivalence laws
(Figure~\ref{fig:row-equivalences}), which we now explain and prove.
%
\begin{remark}
As stated earlier, we omit the superscripts of row constructors. We also
omit the side conditions that concern the kind of the type variables ($\tvar$)
and type meta-variables ($\ttyp$) involved. Thus, each equation in
Figure~\ref{fig:row-equations} really stands for the (infinite) family of
equations obtained by reconstructing the missing kind information in a
consistent way. For instance, the second equation may be read
$\rowall^{\elab.\elabs}\ttyp =
(\rowat[\kind,\elabs]\elab\ttyp{\rowall^\elabs\ttyp})$, where
$\elab\notin\elabs$ and $\ttyp$ has kind $\kind.\type$.
\end{remark}
%
\begin{exercise}[\Recommended, \QuickCheck, \nosolution]
Reconstruct all of the missing kind information in the equations of
Figure~\ref{fig:row-equations}.
\end{exercise}
%
\begin{remark}
There is a slight catch with the unannotated version of the second equation
in Figure~\ref{fig:row-equations}:
its left-hand side admits strictly {more} kinds than its right-hand side,
because the former has row kind $\row\elabs$ for every $\elabs$, while the
latter has row kind $\row\elabs$ for every $\elabs$ such that
$\elab\notin\elabs$ holds. As a result, while replacing the unannotated term
$(\rowat \elab \ttyp {\rowall \ttyp})$ with $\rowall \ttyp$ is always valid,
the converse is not: replacing the unannotated term $\rowall \ttyp$ with
$(\rowat \elab \ttyp {\rowall \ttyp})$ is valid only if it does not result in
an ill-kinded term.
\end{remark}

The first equation in Figure~\ref{fig:row-equations} states that rows are
equal up to commutation of labels. 
For the equation to be well-kinded, the labels $\elab_1$ and $\elab_2$ must be
distinct. The equation holds under our interpretation because extension of a
ground row at $\elab_1$ and extension of a ground row at $\elab_2$ commute.
The second equation states that $\rowall\ttyp$ maps every label within its
domain to $\ttyp$, that is, $\rowall^\elabs\ttyp$ maps every label
$\elab\not\in\elabs$ to $\ttyp$. This equation holds because $\rowall\ttyp$ is
interpreted as a constant row. The last two equations deal with the
relationship between the row constructors $\ttycon$ and the ordinary type
constructor $\ttycon$. Indeed, notice that their left-hand sides involve
$\ttycon^{\row\elabs}$ for some $\elabs$, while their right-hand sides involve
$\ttycon^\type$. Both equations state that it is equivalent to apply
$\ttycon^{\row\elabs}$ at the level of rows or to apply $\ttycon^\type$ at the
level of types. Our interpretation of $\ttycon^{\row\elabs}$ was designed 
to give rise to these equations; indeed, the application of
$\ttycon^{\row\elabs}$ to $n$ ground rows (where $n$ is the arity of
$\ttycon$) is interpreted as a pointwise application of $\ttycon^\type$ to the
rows' components (item~\ref{interpretation-grow} of
Definition~\ref{def-interpretation-rows}). Their use is illustrated in
Examples~\ref{example-record-apply} and~\ref{example-first-class-msg}.
%
\begin{lemma}
\label{lemma-sound-equations}
Each of the equations in Figure~\ref{fig:row-equations} is equivalent to $\ctrue$.
\end{lemma}
%
\begin{Proof}
Each equation can be considered independently.  It suffices to see that any
ground assignment $\ga$ sends both sides of the equation to the same element
in the model, which follows directly from the meaning of row types. Notice
that this fact only depends on the semantics of types and not on the
semantics of the subtyping predicate.
% TEMPORARY this is at best a proof sketch, not a proof!
\end{Proof}

The four equations in Figure~\ref{fig:row-equations} show that two types
with {distinct} head symbols may denote the {same} element of the
model. In other words, in the presence of rows, the interpretation of types is
no longer {free}: an equation of the form $\ttyp_1=\ttyp_2$, where
$\ttyp_1$ and $\ttyp_2$ have distinct head symbols, is not necessarily
equivalent to $\cfalse$.
% NOTE Ces équations forment la théorie équationnelle des rangées.
% Je ne le dis pas explicitement; c'est déjà suggéré à la fin de la
% sous-section précédente.
In Figure~\ref{fig:row-equivalences}, we give several
constraint equivalence laws, known as \emph{mutation} laws,
% FIXME peut-on expliquer pourquoi?
that concern such ``heterogeneous'' equations, and, when viewed as rewriting
rules, allow solving them. To each equation in Figure~\ref{fig:row-equations}
corresponds a mutation law. The soundness of the mutation law, that is, the
fact that its right-hand side entails its left-hand side, follows from the
corresponding equation. The completeness of the mutation law, that is, the
fact that its left-hand side entails its right-hand side, holds by design
of the model.
% NOTE La complétude des lois de mutation indique en quelque sorte
% qu'il n'y a pas d'autres équations que celles données par fig:row-equations.
% Peut-on clarifier cette intuition?

\begin{widefigure}
\TTtoprule
\vspace*{-2ex}
\renewcommand{\regle}[5][t]{&%
  \ifthenelse{\equal{#5}{}}%
    {#3 & \logeq &{}#4 & (\DefRule{#2})\\[3mm]}%
    {\ifthenelse{\equal{#1}{}}%
      {#3 & \logeq &{}#4  \;\;\hspace{-4em}\null\\&
        &      & \text{\footnotesize if $#5$}
                      & (\DefRule{#2})\\[3mm]}%
      {#3 & \logeq &{}#4 & (\DefRule{#2})\\&
        &      & \text{\footnotesize if $#5$}\\[3mm]}}%
}
\newcommand{\xprefix}{\exists \tvar_1\ldots\tvar_n, \tvar'_1\ldots\tvar'_n.}
\begin{regles}
% Mutation.
\regle{C-Mutate-LL}
  {(\rowat {\elab_1} {\ttyp_1} \ttyp_1') = 
   (\rowat {\elab_2} {\ttyp_2} \ttyp_2')}
  {\exists \tvar.(
   \ttyp_1' = (\rowat {\elab_2} {\ttyp_2} \tvar) \wedge 
   \ttyp_2' = (\rowat {\elab_1} {\ttyp_1} \tvar)
   )}
  {\disjoint \tvar {\ftv {\ttyp_1,\ttyp_1',\ttyp_2,\ttyp_2'}} \wedge
   \elab_1 \not=\elab_2 
   }

\regle{C-Mutate-DL}
  {\rowall \ttyp = 
   (\rowat \elab {\ttyp'} {\ttyp''})}
  {\ttyp = \ttyp' \wedge \rowall \ttyp = \ttyp''}
  {}

\regle{C-Mutate-GD}
  {\tapp[3] \ttycon {\ttyp_1}\ldots{\ttyp_n} = \rowall \ttyp}
  {\exists \tvar_1\ldots\tvar_n. (
     \tapp[3] \ttycon {\tvar_1}\ldots{\tvar_n} = \ttyp \wedge 
     \bigwedge_{i=1}^n
     (\ttyp_i = \rowall \tvar_i)
   )}
  {\disjoint {\tvar_1\ldots\tvar_n} {\ftv {\ttyp_1,\ldots,\ttyp_n,\ttyp}}}

\regle[]{C-Mutate-GL}
  {\tapp[3] \ttycon {\ttyp_1}\ldots{\ttyp_n} = (\rowat {\elab} {\ttyp}{\ttyp'})}
  {\xprefix(
      \tapp[3] \ttycon {\tvar_1}\ldots{\tvar_n} = \ttyp \;\wedge
      \\ & & & \phantom{\xprefix(} % this line to fix alignment
      \tapp[3] \ttycon {\tvar'_1}\ldots{\tvar'_n} = \ttyp' \;\wedge
      \\ & & & \phantom{\xprefix} % this line to fix alignment
      \bigwedge_{i=1}^n ( \ttyp_i = (\rowat \elab {\tvar_i} {\tvar'_i}) )
    )}
  {\disjoint {\tvar_1\ldots\tvar_n,\tvar'_1\ldots\tvar'_n} {\ftv {\ttyp_1,\ldots,\ttyp_n,\ttyp,\ttyp'}}}

\end{regles}
\vspace*{-1ex} \\
\TTbottomrule\\
\vspace*{-2ex}
\bcpcaption{row-equivalences}{Constraint equivalence laws involving rows}
\end{widefigure}
%
\begin{exercise}[\Recommended, \QuickCheck, \nosolution]
Reconstruct all of the missing kind information in the laws of
Figure~\ref{fig:row-equivalences}.
\end{exercise}
%
Let us now review the four mutation laws. For the sake of brevity, in the
following informal explanation, we assume that a ground assignment $\ga$ that
satisfies the left-hand equation is fixed, and write ``the ground type
$\ttyp$'' for ``the ground type $\ga(\ttyp)$.''
%
\Rule{C-Mutate-LL} concerns an equation between two rows, which are both given
by extension but exhibit distinct head labels $\elab_1$ and $\elab_2$. When
this equation is satisfied, both of its members must denote the same ground
row. Thus, the ground row $\ttyp'_1$ must map $\elab_2$ to the ground type
$\ttyp_2$, while, symmetrically, the ground row $\ttyp'_2$ must map $\elab_1$
to the ground type $\ttyp_1$. This may be expressed by two equations of the
form $\ttyp_1' = (\rowat {\elab_2} {\ttyp_2} \ldots)$ and $\ttyp_2' = (\rowat
{\elab_1} {\ttyp_1} \ldots)$. Furthermore, because the ground rows $\ttyp'_1$
and $\ttyp'_2$ must agree on their common labels, the ellipses in these two
equations must denote the same ground row. This is expressed by letting the
two equations share a fresh, existentially quantified row variable $\tvar$.
%
\Rule{C-Mutate-DL} concerns an equation between two rows, one of which is given
as a constant row, the other of which is given by extension. Then, because the
ground row $\rowall\ttyp$ maps every label to the ground type $\ttyp$, the
ground type $\ttyp'$ must coincide with the ground type $\ttyp$, while the
ground row $\ttyp''$ must map every label in its domain to the ground type
$\ttyp$. This is expressed by the equations $\ttyp = \ttyp'$ and $\rowall
\ttyp = \ttyp''$.
%
\Rule{C-Mutate-GD} and \Rule{C-Mutate-GL} concern an equation between two rows,
one of which is given as an application of a row constructor $\ttycon$, the
other of which is given either as a constant row or by extension. Again, the
laws exploit the fact that the ground row $\tapp[3] \ttycon
{\ttyp_1}\ldots{\ttyp_n}$ is obtained by applying the type constructor
$\ttycon$, pointwise, to the ground rows $\ttyp_1, \ldots, \ttyp_n$.  If, as
in \Rule{C-Mutate-GD}, it coincides with the constant ground row $\rowall\ttyp$,
then every $\ttyp_i$ must itself be a constant ground row, of the form
$\rowall\tvar_i$, and $\ttyp$ must coincide with $\tapp[3] \ttycon
{\tvar_1}\ldots{\tvar_n}$. \Rule{C-Mutate-GL} is obtained in a similar manner.
%
\begin{lemma}
\label{lemma-row-equivalence}
Each of the equivalence laws in Figure~\ref{fig:row-equivalences} holds.
\end{lemma}
%
\begin{Proof}
By examination of each of the laws.

\proofcase {\textsc{C-Mutate-LL}}:
\demoreset
Let $\disjoint \tvar {\ftv {\ttyp_1,\ttyp_1',\ttyp_2,\ttyp_2'}}$~\dlabel 0
and $\elab_1 \not=\elab_2$.  
Let $\row \elabs$ be the row kind of this equation. 
Let $\ga$ be a ground assignment that satisfies the constraint
$(\rowat {\elab_1} {\ttyp_1} \ttyp_1') = 
 (\rowat {\elab_2} {\ttyp_2} \ttyp_2')$. Then, 
$\ga$ maps both members of the equation to 
a ground type $\gtyp$ of row kind $\row \elabs$.
According to the interpretation of row extension, we must have
$\gtyp (\epsilon) = \elabs$, 
$\gtyp /\elab_1 = \ga(\ttyp_1) =  \ga(\ttyp'_2)/\elab_1$, 
$\gtyp/\elab_2 =  \ga(\ttyp_1')/\elab_2 = \ga(\ttyp_2)$,
and $\gtyp /\elab = \ga(\ttyp_2')/\elab = \ga(\ttyp_1')/\elab$
for every $\elab \in \rowlabels \setminus \elab_1.\elab_2.\elabs$. 
Now, let $\gtyp'$ be the ground row of row kind $\row{\elab_1.\elab_2.\elabs}$ defined by 
$\gtyp'(\epsilon) = \elab_1.\ell_2.\elabs$ and 
$\gtyp'/\elab = \gtyp/\elab$
for every $\elab \in  \rowlabels \setminus \elab_1.\elab_2.\elabs$. 
By construction and by \dref 0, $\etend\ga\tvar{\gtyp'}$ satisfies
both equations $\ttyp_1' = (\rowat {\elab_2} {\ttyp_2} \tvar)$ and 
$\ttyp_2' = (\rowat {\elab_1} {\ttyp_1} \tvar)$. 
Thus, by \Rule {CM-Exists} and \dref 0, $\ga$ satisfies $\exists \tvar.
(\ttyp_1' = (\rowat {\elab_2} {\ttyp_2} \tvar) \wedge  
\ttyp_2' = (\rowat {\elab_1} {\ttyp_1} \tvar))$. This proves that
\Rule{C-Mutate-LL} is complete.

Conversely, we have:
$$
\begin{tabular*}{\linewidth}{.C.R<{{}},>{{}}L.R.}&
&\exists \tvar.(
   \ttyp_1' = (\rowat {\elab_2} {\ttyp_2} \tvar) \wedge 
   \ttyp_2' = (\rowat {\elab_1} {\ttyp_1} \tvar)
   )
\\& \logeq&
\exists \tvar.
((\rowat {\elab_1} {\ttyp_1} \ttyp_1') = 
(\rowat {\elab_1} {\ttyp_1} \rowat {\elab_2} {\ttyp_2} \tvar) \wedge{}
\\& & \phantom{\exists \tvar.(}
(\rowat {\elab_2} {\ttyp_2} \ttyp_2') = 
(\rowat {\elab_2} {\ttyp_2} \rowat {\elab_1} {\ttyp_1} \tvar))
&\dlabel 6
\\& \entails&
\exists \tvar.
(\rowat {\elab_1} {\ttyp_1} \ttyp_1') = 
   (\rowat {\elab_2} {\ttyp_2} \ttyp_2')
&\dlabel 7
\\& \logeq&
(\rowat {\elab_1} {\ttyp_1} \ttyp_1') = 
   (\rowat {\elab_2} {\ttyp_2} \ttyp_2')
&\dlabel 8
\end{tabular*}
$$
where \dref 6 is by congruence; 
\dref 7 is by \Rule {C-Row-LL} and transitivity of equality;
and \dref 8 follows from \Rule {C-Ex*} and \dref 0. This
proves that \Rule{C-Mutate-LL} is sound.

\proofcases {\textsc {C-Mutate-DL}, \textsc{C-Mutate-GD}, and \textsc {C-Mutate-GL}}
are left to the reader. % TEMPORARY?
\end{Proof}

\subsection*{Solving Equality Constraints in the Presence of Rows}
\label{section-row-unification}

{
\begin{widefigure}
\TTtoprule
\vspace*{-2ex}
\newcommand{\xprefix}{\exists \tvar_1\ldots\tvar_n, \tvar'_1\ldots\tvar'_n.}
$$\begin{regles}
\regle{S-Mutate-LL}
  {(\rowat {\elab_1} {\tvar_1} \tvar_1') = 
   (\rowat {\elab_2} {\ttyp_2} \ttyp_2') = \meq}
  {\exists \tvar.(
   \tvar_1' = (\rowat {\elab_2} {\ttyp_2} \tvar) \wedge 
   \ttyp_2' = (\rowat {\elab_1} {\tvar_1} \tvar)
   ) \\&&&{} 
   \wedge (\rowat {\elab_1} {\tvar_1} \tvar_1') = \meq}
  {\elab_1 \not=\elab_2}

\regle{S-Mutate-DL}
  {\rowall \tvar = 
   (\rowat \elab \ttyp {\ttyp'}) = \meq}
  {\tvar = \ttyp \wedge \rowall \tvar = \ttyp'
   \wedge \rowall \tvar = \meq}
  {}

\regle{S-Mutate-GD}
  {\tapp[3] \ttycon {\ttyp_1}\ldots{\ttyp_n} =
   \rowall \tvar = 
   \meq}
  {\exists \tvar_1\ldots\tvar_n. (
     \tapp[3] \ttycon {\tvar_1}\ldots{\tvar_n} = \tvar \wedge
     \bigwedge_{i=1}^n
     (\ttyp_i = \rowall \tvar_i)
   ) \\&&&{} 
   \wedge \rowall \tvar = \meq}
  {}

\regle{S-Mutate-GL}
  {\tapp[3] \ttycon {\ttyp_1}\ldots{\ttyp_n} = (\rowat\elab\tvar{\tvar'}) = \meq}
  {\xprefix(
      \tapp[3] \ttycon {\tvar_1}\ldots{\tvar_n} = \tvar \;\wedge
      \\ & & & \phantom{\xprefix(} % this line to fix alignment
      \tapp[3] \ttycon {\tvar'_1}\ldots{\tvar'_n} = \tvar' \;\wedge
      \\ & & & \phantom{\xprefix} % this line to fix alignment
      \bigwedge_{i=1}^n ( \ttyp_i = (\rowat \elab {\tvar_i} {\tvar'_i}) )
    ) \quad\null \\&&&{} \wedge
    (\rowat \elab {\tvar} {\tvar'}) = \meq}
  {}

% Error.
\regle{S-Clash'}
  {\tycon\,\ttypc=\tycon'\,\ttypcp=\meq}
  {\cfalse}
  {\tycon\not=\tycon' \text{ and none of the four rules above applies}}
\end{regles}
$$ \\
\vspace*{-2.5ex}
\TTbottomrule\\
\vspace*{-1.5ex}
\bcpcaption{solver-unif-rcd}{Row unification (changes to Figure~\ref{fig:unif-trs})}
\end{widefigure}
}

We now extend the unification algorithm given in \S\ref{section-unification}
with support for rows. The extended algorithm is intended to solve unification
problems where the syntax and interpretation of types are as defined in
the discussions above of the syntax (p.~\pageref{section-row-syntax}) and
meaning (p.~\pageref{section-row-meaning}) of rows. Its
specification consists of the original rewriting rules of
Figure~\ref{fig:unif-trs}, minus \Rule {S-Clash}, which is removed and
replaced with the rules given in Figure~\ref{fig:solver-unif-rcd}.
Indeed, \Rule{S-Clash} is no longer valid in the presence of rows: not
all distinct type constructors are incompatible.

The extended algorithm features four {mutation} rules, which are in
direct correspondence with the mutation laws of
Figure~\ref{fig:row-equivalences}, as well as a weakened version of
\Rule{S-Clash}, dubbed \Rule{S-Clash'}, which applies when neither
\Rule{S-Decompose} nor the mutation rules are applicable. (Let us point out
that, in \Rule{S-Decompose}, the meta-variable $\tycon$ ranges over all type
constructors in the signature $\sig$, so that \Rule{S-Decompose} is applicable
to multi-equations of the form $\rowall\tvar=\rowall\ttyp=\meq$ or
$(\rowat\elab\tvar{\tvar'})=(\rowat\elab\ttyp{\ttyp'})=\meq$.)
%
Three of the mutation rules may allocate fresh type variables, which must be
chosen fresh for the rule's left-hand side.
% Je n'ai pas répété les conditions de fraîcheur, car ça fait du bruit
% et c'est une convention courante que de les omettre dans des règles de réécriture
% (par opposition à des règles d'équivalence).
The four mutation rules paraphrase the four mutation laws very closely. Two
minor differences are (i) the mutation rules deal with multi-equations, as
opposed to equations; and (ii) any subterm that appears more than once on the
right-hand side of a rule is required to be a type variable, as opposed to an
arbitrary type. Neither of these features is specific to rows: both may be
found in the definition of the standard unification algorithm
(Figure~\ref{fig:unif-trs}), where they help reason about sharing.
%
\begin{exercise}[\QuickCheck, \nosolution]
Check that the rewriting rules in Figure~\ref{fig:solver-unif-rcd} preserve
well-kindedness. Conclude that, provided its input constraint is well-kinded,
the unification algorithm needs not keep track of kinds.
\end{exercise}
%
\begin{full}
\begin{remark}
Like \Rule{S-Decompose}, the mutation rules accept a multi-equation with two
non-variable members. One of these members is discarded, making the original
multi-equation smaller; furthermore, new multi-equations may be created. One
novelty with respect to \Rule{S-Decompose} is that the two non-variable terms
at hand no longer necessarily have the same head symbol. Thus, \emph{which}
is discarded and \emph{which} is kept may make a difference. How do we resolve
this choice?

In the case of \Rule{S-Mutate-LL}, the two terms at hand have head symbols
$(\rowat{\elab_1}\cdot\cdot)$ and $(\rowat{\elab_2}\cdot\cdot)$, respectively.
Because a row label is a good as another, there does not seem, at first sight,
to be a meaningful way of choosing which term to keep and which to discard.
However, let us impose a fixed, arbitrary total order on labels, and let us
modify \Rule{S-Mutate-LL} by replacing the side condition $\elab_1\not=\elab_2$
with $\elab_1<\elab_2$. In other words, among two syntactic representations of
the same row, let us choose to keep the one that exhibits the smaller head
label. It is believed that, under such a strategy, rows tend to
``self-organize'' so that labels appear in increasing order. Under this
conjecture, two rows that are to be unified are more likely to be organized in
the same order, so that \Rule{S-Decompose}, which is cheap, is more likely to
be applicable than \Rule{S-Mutate-LL}, which is expensive, because it allocates
a fresh row varible as well as two new row terms. Thus, it is believed that
this strategy increases the efficiency of the unification algorithm. No
rigorous study of this phenomenon exists, however.

In the case of \Rule{S-Mutate-DL}, we choose to keep a term of the form
$\rowall\cdot$, rather than a term of the form $(\rowat\elab\cdot\cdot)$,
since the former is more concise.

In the case of \Rule{S-Mutate-GD} and \Rule{S-Mutate-GL}, the choice is between
favoring an application of $\ttycon$, on the one hand, or an application of
one the row constructors $\rowall$ or $\elab$, on the other hand. The rules in
Figure~\ref{fig:solver-unif-rcd} choose the latter. However, depending on the
arity of $\ttycon$, the former may be more concise, so a variation of these
two rules is conceivable.
\end{remark}
\end{full}

The properties of the unification algorithm are preserved by this extension,
as witnessed by the next three lemmas. Note that the termination of
reduction is ensured {only} when the initial unification problem is
well-kinded. The ill-kinded unification problem $\tvar = (\rowat {\elab_1}
\ttyp \twar) \wedge \tvar = (\rowat {\elab_2} \ttyp \twar)$, where $\elab_1$
and $\elab_2$ are distinct, illustrates this point.
%
\begin{lemma}
\label{lemma-solver-unif-rcd-normalizes}
The rewriting system $\red$ is strongly normalizing.
\end{lemma}
\begin{full}
% TEMPORARY preuve à revoir à cause des changements de notation dans la figure qui définit les règles.
\begin{Proof}
\newcommand{\cw}[1]{\kw{cw}(#1)}
\newcommand{\iw}[1]{\kw{iw}(#1)}
\newcommand{\wg}[1]{\kw{w}(#1)}
\newcommand{\kwg}[1]{\kw{kw}(#1)}
\newcommand{\wig}[1]{\delta(#1)}
We reuse the proof of Lemma~\ref{lemma-unif-trs-normalizes}.  We refine the
weight of type constructors as follows.  Let $\rowlabels_f$ be a finite
subsets of $\rowlabels$ that includes at least all labels appearing in the
constraints.  Note that labels are never created during reduction of
constraints, so $\rowlabels_f$ can be chosen before any reduction occurs.
We define the weight of a kind by $\kwg \type = 1$ and $\kwg {\row \elabs} = 2 +
\cardinal {\rowlabels_f \setminus \elabs}$.  
Finally, the weight of a type
constructor $F$ of row kind  $\rowkind_1 \times \ldots \rowkind_n \kindarrow \rowkind$ 
is a polynom in $X, Y$ with non negative integer coefficients,  defined
by $\cw{\tycon} = (X^{\kwg \rowkind}Y^{k(\tycon)} + 2)(2 + n)$ where 
$k(\tycon)a$ is, depending on $\tycon$, $0$ for $\rowall$ and $\typi$, $1$
for $\elab$, and $2$ for $\ttycon$. Polynoms are ordered in increasing power
of $X$,  then in increasing power of $Y$. Since this choice satisfies
the condition found in the proof of Lemma~\ref{lemma-unif-trs-normalizes},
it suffices to check that the rules 
\Rule {S-Mutate-LL}, 
\Rule {S-Mutate-LG}, 
\Rule {S-Mutate-DG}, 
\Rule {S-Mutate-DL}, 
and \Rule {S-Class-I} are also weight-decreasing. 
We write $\wig {\ttyp}$ for $\iw\ttyp - \wg\ttyp$, which is either $2$ or
$-1$, depending on whether $\ttyp \in \tyvarset$; thus, we always have
$\wig \ttyp \ge -1$. 
\proofcase {\Rule{S-Mutate-LL}}:
This amounts to replacing the term
$(\rowat {\elab_2} {\ttyp_2} \ttyp_2')$ by terms $\tvar_1'$, $(\rowat
{\elab_2} {\ttyp_2} \twar)$,  $\ttyp'_2$, and $(\rowat {\elab_1} {\tvar_1}
\twar)$. This decreases weight by (omitting internal weight of variables,
which are null):
\iffalse%\iftrue
$$
\cw{{\elab_2}^\elabs} + \iw {\ttyp_2} + \iw{\ttyp'_2}
- \wg{\tvar_1'} - \cw{{\elab_2}^{\elab_1.\elabs}} - \iw {\ttyp_2} 
- \wg{\ttyp'_2} - \cw{{\elab_1}^{\elab_2.\elabs}}.
$$
That is, 
\fi
$$
\cw{{\elab_2}^\elabs}  - \cw{{\elab_2}^{\elab_1.\elabs}} - \cw{{\elab_1}^{\elab_2.\elabs}}
+ \wig{\ttyp'_2} - 1.
$$
The monom of highest degree, which is $\cw{{\elab_2}^\elabs}$, has a positive
coefficient.  

\proofcase {\Rule{S-Mutate-LG}}:
This amounts to replacing the term $\tapp {\tycon^{\row \elabs}} {\ttyp_i\iI}$ by 
terms $\tvar$,  
$\tapp {\tycon^\type} \twar_i\iI$, $\tvar'$, 
$\tapp {\tycon^{\row {\elab.\elabs}}} \twar_i\iI$, 
$\ttyp_i\iI$, and $\rowat [\elabs] \elab {\twar_i} \twar'_i$. 
This decreases the weight by:
\iffalse%\iftrue
$$
\cw {\tycon^{\row {\elabs}}} + \Sigma\iI \iw {\ttyp_i}
- \wg \tvar
- \cw  {\tycon^{\row {\elab.\elabs}}} 
- \wg {\tvar'}
- \Sigma\iI \wg {\ttyp_i}
- \cw {\elab^\elabs}.
$$
That is 
\fi
$$
\cw {\tycon^{\row {\elabs}}} 
- \cw {\elab^\elabs}
- \cw  {\tycon^{\row {\elab.\elabs}}} 
+ \Sigma\iI \wig {\ttyp_i}
- 2.
$$
The monom of highest degree, which is the first one, 
has a positive coefficient. 

\proofcase {\Rule{S-Mutate-DG}}:
This amounts to replacing the term
$\tapp \ttycon \ttyp_i\iI$ by terms
$\tvar$, $\tapp \ttycon \twar_i\iI$, and $(\ttyp_i = \rowall \twar_i)\iI$. 
This decreases weight by: 
\iffalse%\iftrue
$$
\cw{\ttycon^{\row \elabs}} + \Sigma\iI \iw {\ttyp_i}
- \wg \tvar - \cw {\ttycon ^\type} 
- \Sigma\iI (\wg {\ttyp_i} + \cw {\rowall ^\elabs})
$$
That is,
\fi
$$
\cw{\ttycon^{\row \elabs}}
- \Sigma\iI (\cw {\rowall ^\elabs})
- \cw {\ttycon ^\type} 
+ \Sigma\iI \wig {\ttyp_i}
- 1
$$
The monom of highest degree, which is the first one, has a positive
coefficient.  

\proofcase {\Rule{S-Mutate-DL}}:
This amounts to replacing the term $(\rowat [\elabs] \elab \ttyp \ttyp'$
by the terms
$\tvar$, $\ttyp$,  $\rowall^{\elab.\elabs} \tvar$, and $\ttyp'$. 
This decreases weight by:
\iffalse%\iftrue
$$
\cw {\elab^\elabs} + \iw \ttyp + \iw {\ttyp'} 
- \wg{\tvar} - \wg{\ttyp} - \cw{\rowall^\elabs} - \wg{\ttyp'}
$$
That is,
\fi
$$
\cw {\elab^\elabs} - \cw{\rowall^\elabs}
 + \wig \ttyp + \wig {\ttyp'} -1
$$
The monom of highest degree, which is the first one, 
has a positive coefficient. 
\end{Proof}
\end{full}
%
\begin{lemma}
\label{lemma-solver-rcd-correct}
$\mc_1\red\mc_2$ implies $\mc_1\logeq\mc_2$.
\end{lemma}
\begin{Proof}
It suffices to check the property independently for each rule defining
$\red$.  The proof for rules of Figure~\ref{fig:unif-trs} but \Rule
{S-Clash} remain valid for row terms. For \Rule {S-Decompose}, it follows by
the invariance of all type constructors, which is preserved for row
terms\iffull{\space(Lemma~\ref {lemma-record-variance})}. For rule \Rule
{S-Class-I} it follows by Lemma~\ref{lemma-record-incomp} and for mutation
rules, it follows by Lemma~\ref{lemma-row-equivalence}.
\end{Proof}
%
\begin{lemma}
Every normal form is either $\cfalse$ or of the form $\xc[\mc]$, where $\xc$
is an existential constraint context, $\mc$ is a standard conjunction of
multi-equations and, if the model is syntactic, $\mc$ is acyclic. These
conditions imply that $\mc$ is satisfiable.
\end{lemma}
%
% \begin{Proof}
% TEMPORARY
% La preuve de Didier, qui consistait à noter que les formes normales
% sont les mêmes pour les deux systèmes, et à récupérer l'ancien lemme
% qui prouve que les formes normales sont satisfiables, est fausse.
% En effet, la preuve que les formes normales sont satisfiables a été
% effectuée vis-à-vis d'un modèle libre. Elle est à refaire ici, ou du
% moins à généraliser suffisamment pour être valide aussi bien dans un
% modèle libre que dans un modèle avec rangées.
% \end{Proof}
%

The time complexity of standard first-order unification is quasi-linear. What
is, then, the time complexity of row unification? Only a partial answer is
known. In practice, the algorithm given in \this is extremely efficient and
appears to behave just as well as standard unification. In theory, the
complexity of row unification remains unexplored and forms an interesting
open issue.
%
\begin{exercise}[\Moderate, \nosolution]
The unification algorithm presented above, although very efficient in
practice, does \emph{not} have linear or quasi-linear time complexity. Find a
family of unification problems $\mc_n$ such that the size of $\mc_n$ is linear
with respect to $n$ and the number of steps required to reach its normal form
is quadratic with respect to $n$.
\end{exercise}
% La solution est d'unifier deux rangées de longueur n n'ayant aucun label en commun.
% En fait, j'ai même noté (dans mon journal, je crois) qu'avec une légère variation
% sur cet exemple, on peut monter à une complexité cubique, en exploitant l'absence
% de mémoisation de l'algorithme (il découvre certaines relations entre variables,
% puis les oublie).
%
% TEMPORARY corriger l'exo; on peut aussi parler de l'exemple cubique
%
\begin{remark}
{Mutation} is a common technique for solving equations in a large class
of non-free algebras that are described by \emph{syntactic theories} \cite
{Kirchner-Klay/syntactic-theories}. The equations of Figure~\ref
{fig:row-equations} happen to form a syntactic presentation of an equational
theory. Thus, it is possible to derive a unification algorithm out of these
equations in a systematic way \cite{Remy/start}. Here, we have presented the
same algorithm in a direct manner, without relying on the apparatus of
syntactic theories.
\end{remark}

\subsection*{Operations on Records}

\index{operations on records|see{records}}
\index{concatenation|see{records}}

We now illustrate the use of rows for typechecking operations on records.
We begin with full records; our treatment follows \longcite{Remy!projective-ml}.
%
\begin{example}[Full records]
\label{example-full-records}
As before, let us begin with full records, whose
domain is exactly $\rowlabels$. The primitive operations are record creation
$\inject\cdot$, update $\override\elab\cdot\cdot$, and access
$\project\elab\cdot$.

Let $<$ denote a fixed strict
total order on row labels. For every set of labels $\elabs$ of cardinal $n$, let us
introduce a $(n+1)$-ary constructor $\{\}_\elabs$. We use the following syntactic sugar: we write $\inject{
\elab_1=\et_1; \ldots; \elab_n=\et_n; \et }$ for the application
$\tapp[4]{\{\}_\elabs}{\et_{i_1}}\ldots{\et_{i_n}}\et$, where $\elabs = \{
\elab_1,\ldots,\elab_n \} = \{ \elab_{i_1},\ldots,\elab_{i_n} \}$ and
$\elab_{i_1} < \ldots < \elab_{i_n}$ holds. The use of the total order $<$
makes the meaning of record expressions independent of the order in which
fields are defined; in particular, it allows fixing the order in which
$\et_1,\ldots,\et_n$ are evaluated. We abbreviate the record value
$\inject{\elab_1=\ev_1; \ldots; \elab_n=\ev_n; \ev}$ as $\inject{\evm;\ev}$,
where $\evm$ is the finite function that maps $\elab_i$ to $\ev_i$ for every
$i\in\{1,\ldots,n\}$.
%
% NOTE J'ai été tenté d'aller plus loin et d'écrire les records sous la forme
% { V }, où V est une fonction presque constante des labels dans les valeurs.
% L'avantage aurait été de n'avoir que deux règles de réduction au lieu de trois/quatre.
% Malheureusement, cela n'est pas possible. En effet, il ne faut pas identifier
% les records {v} et {v with l=v}. Deux raisons: (i) le premier a un type principal moins
% général que le second; ainsi une sémantique qui réduirait le second en le premier ne
% pourrait pas avoir subject reduction; et (ii) lorsqu'on définit la primitive map sur
% les records, le nombre de fois où f est appliquée n'est pas le même, et cela est
% observable en CBV. Moralité, dans les records avec défaut, l'ensemble des champs
% qui sont définis explicitement a une importance intrinsèque.

The operational semantics of the above three operations may now be defined in
the following straightforward manner. First, record creation $\inject\cdot$ is
precisely the unary constructor $\{\}_\varnothing$. Second, for every
$\elab\in\rowlabels$, let update $\override\elab\cdot\cdot$ and access
$\project\elab\cdot$ be destructors of arity 1 and 2, respectively, equipped
with the following reduction rules:
$$
\renewcommand{\regle}[4][]{#2&\reduces[\delta]&#3&#1&(\DefRule{#4})\\}
\begin{tabular*}{\linewidth}{RCL.LR}
\regle{\override\elab{\ev'}{\inject{\evm;\ev}}}{\inject{\extension\evm\elab{\ev'};\ev}}{R-Update}
\regle[(\elab\in\Dom\evm)]{\project\elab{\inject{\evm;\ev}}}{\evm(\elab)}{R-Access-1}
\regle[(\elab\notin\Dom\evm)]{\project\elab{\inject{\evm;\ev}}}{\ev}{R-Access-2}
\end{tabular*}
$$
In these rules, $\extension\evm\elab\ev$ stands for the function that maps $\elab$
to $\ev$ and coincides with $\evm$ at every other label, while $\evm(\elab)$ stands
for the image of $\elab$ through $\evm$. Because these rules make use of the  syntactic
sugar defined above, they are, strictly speaking, rule {schemes}: each of them
really stands for the infinite family of rules that would be obtained if the syntactic
sugar was eliminated.

Let us now define the syntax of types as in
Example~\ref{example-syntax-normal}. Let the initial environment $\env_0$
contain the following bindings:
%% \FIX{This is probably too much work to fix,
%%   but I see that the spacing in some of these types is strange: there is no
%%   space at all after the dot, but lots of space around the arrow...}
$$\begin{array}{rl}
\{\}_{\{ \elab_1, \ldots, \elab_n \}} :& \dmscheme{\tvar_1\ldots\tvar_n\tvar}
{\tvar_1\arw\ldots\arw\tvar_n\arw\tvar\arw\tpi{(\elab_1:\tvar_1;\ldots;\elab_n:\tvar_n;\rowall\tvar)}} \\
& \text{where $\elab_1 < \ldots < \elab_n$}\\
\override\elab\cdot\cdot :&
   \dmscheme{\tvar\tvar'\twar}
            {
      \tpi{(\rowat\elab\tvar\twar)} \arw \tvar' \arw
      \tpi{(\rowat\elab{\tvar'}\twar)}
   } \\
\project\elab\cdot :&
   \dmscheme{\tvar\twar}{\tpi{(\rowat\elab\tvar\twar)}
                                    \arw\tvar}
\end{array}$$
Note that, in particular, the type scheme assigned to record creation
$\inject\cdot$ is $\dmscheme\tvar{\tvar\arw\tpi{(\rowall\tvar)}}$. As a
result, these bindings are exactly as stated in
the discussion of records with infinite carrier (p.~\pageref{section-infinite-carrier}).

To illustrate how these definitions work together, let us consider the program
$\project{\elab_2}{\override{\elab_1}{\etrue}{\inject{0}}}$, which builds a
record, extends it at $\elab_1$, then accesses it at $\elab_2$. Can we build
an \hmx type derivation for it, under the constraint $\ctrue$ and the initial
environment $\ienv$? To begin, by looking up $\ienv$ and using
\Rule{hmx-Inst}, we find that $\inject\cdot$ has type
$\tint\arw\tpi{(\rowall\tint)}$. Thus, assuming that $0$ has type $\tint$, the
expression $\inject{0}$ has type $\tpi{(\rowall\tint)}$. Indeed, this
expression denotes a record all of whose fields hold an integer value.  Then,
by looking up $\ienv$ and using \Rule{hmx-Inst}, we find that
$\override{\elab_1}\cdot\cdot$ has type
$\tpi{(\rowat{\elab_1}\tint{\rowall\tint})} \arw \tbool \arw
\tpi{(\rowat{\elab_1}{\tbool}{\rowall\tint})}$. May we immediately use
\Rule{hmx-App} to typecheck the application of $\override{\elab_1}\cdot\cdot$
to $\inject{0}$? Unfortunately, no, because there is an apparent mismatch
between the expected type $\tpi{(\rowat{\elab_1}\tint{\rowall\tint})}$ and the
effective type $\tpi{(\rowall\tint)}$. To work around this problem, let us
recall that, by \Rule{C-Row-DL}, the equation $\tpi{(\rowall\tint)} =
\tpi{(\rowat{\elab_1}\tint{\rowall\tint})}$ is equivalent to $\ctrue$.  Thus,
\Rule{hmx-Sub} allows proving that $\inject{0}$ has type
$\tpi{(\rowat{\elab_1}\tint{\rowall\tint})}$. Assuming that $\etrue$ has type
$\tbool$, we may now apply \Rule{hmx-App} and deduce
%
$$\hmj\ctrue\ienv{\override{\elab_1}{\etrue}{\inject{0}}}
{\tpi{(\rowat{\elab_1}\tbool{\rowall\tint})}}.$$
%
We let the reader check that, in a similar manner involving
\Rule{C-Row-DL}, \Rule{C-Row-LL}, and \Rule{hmx-Sub}, one may prove that
$\project{\elab_2}{\override{\elab_1}{\etrue}{\inject{0}}}$ has type $\tint$,
provided $\elab_1$ and $\elab_2$ are distinct.
\end{example}
%
\begin{exercise}[\Easy, \nosolution]
Unfold the definition of the constraint
$\cxlet\ienv{\calcule{\project{\elab_2}{\override{\elab_1}{\etrue}{\inject{0}}}}}\tvar$,
which states that $\tvar$ is a valid type for the above program. Assuming that
subtyping is interpreted as equality, simulate a run of the constraint solver
(\S\ref{section-solver}), extended with support for rows, so as to solve
this constraint. Check that the solved form is equivalent to $\tvar=\tint$.
\end{exercise}
%
\begin{exercise}[\Moderate]
\solref{record-soundness} Check that the definitions of
Example~\ref{example-full-records} meet the requirements of
Definition~\ref{def-requirements}.
\end{exercise}
%
\begin{example}[Record application]
\label{example-record-apply}
Let us now introduce a more unusual primitive operation on full records. This
operation accepts two records, the first of which is expected to hold a
function in every field and produces a new record, whose contents are
obtained by applying, pointwise, the functions in the first record to the
values in the second record. In other words, this new primitive operation
lifts the standard application combinator (which may be defined as
$\efun\efar{\efun\evar{\eapp\efar\evar}}$), pointwise, to the level of
records. For this reason, we refer to it as $\rapply$. Its operational
semantics is defined by making it a binary destructor and equipping it
with the following reduction rules:
$$
\renewcommand{\regle}[3]{#1&\reduces[\delta]&#2&(\DefRule{#3})\\}
\renewcommand{\sidecond}[1]{&&\text{if $#1$}\\}
\begin{tabular*}{\linewidth}{RCL.R}
\regle{\erapply{\inject{\evm;\ev}}{\inject{\evm';\ev'}}}
      {\inject{\eapp\evm{\evm'};\eapp\ev{\ev'}}}
      {R-Apply-1}

\regle{\erapply{\inject{\evm;\ev}}{\inject{\evm';\ev'}}}
      {\erapply{\inject{\evm;\ev}}{\inject{\extension{\evm'}\elab{\ev'};\ev'}}}
      {R-Apply-2}
\sidecond{\elab\in\Dom\evm\setminus\Dom{\evm'}}

\regle{\erapply{\inject{\evm;\ev}}{\inject{\evm';\ev'}}}
      {\erapply{\inject{\extension\evm{\elab'}\ev;\ev}}{\inject{\evm';\ev'}}}
      {R-Apply-3}
\sidecond{\elab'\in\Dom{\evm'}\setminus\Dom\evm}
\end{tabular*}
$$
In the first rule, $\eapp\evm{\evm'}$ is defined only if $\evm$ and $\evm'$
have a common domain; it is then defined as the function that maps $\elab$
to the expression $\eapp{\evm(\elab)}{\evm'(\elab)}$.
% Il y a ici un (très léger) abus de notation, dans la mesure où nous n'avons
% défini cette notation que pour les records de valeurs, et nous avons ici un
% record d'expressions.
The second and third rules, which are symmetric, deal with the case where some
field is explicitly defined in one input record but not in the other; in that
case, the field is made explicit by creating a copy of the record's default
value.

The syntax of types remains as in Example~\ref{example-full-records}. We extend
the initial environment $\env_0$ with the following binding:
$$\begin{array}{rl}
\rapply :& \dmscheme{\tvar\twar}{\tpi{(\tvar \arw \twar)} \arw \tpi\tvar \arw \tpi\twar}
\end{array}$$
To understand this type scheme, recall that the principal type scheme of the
standard application combinator (which may be defined as
$\efun\efar{\efun\evar{\eapp\efar\evar}}$) is $\dmscheme{\tvar\twar}{(\tvar
\arw \twar) \arw \tvar \arw \twar}$. The type scheme assigned to $\rapply$ is
very similar; the most visible difference is that both arguments, as well as
the result, are now wrapped within the record type constructor $\typi$. A more
subtle, yet essential change is that $\tvar$ and $\twar$ are now row
variables: their kind is $\normalkind.\row\varnothing$. As a result, the
leftmost occurrence of the arrow constructor is really
$\arw^{\row\varnothing}$. Thus, we are exploiting the presence of type
constructors of the form $\ttycon^\rowkind$, with $\rowkind\not=\type$,
in the signature $\sig$.

\newcommand{\enot}{\mathtt{not}}
\newcommand{\esucc}{\mathtt{succ}}
%
\demoreset To illustrate how these definitions work together, let us consider
the program $\erapply{\inject{\elab=\enot; \esucc}}{\inject{\elab=\etrue;0}}$,
where the terms $\enot$ and $\esucc$ are assumed to have types
$\tbool\arw\tbool$ and $\tint\arw\tint$, respectively. Can we build an \hmx
type derivation for it, under the constraint $\ctrue$ and the initial
environment $\ienv$? To begin, it is straightforward to derive that the record
$\inject{\elab=\enot; \esucc}$ has type
$\tpi{(\rowat\elab{\tbool\arw\tbool}{\rowall{(\tint\arw\tint)}})}$~\dlabel{type}. In
order to use $\rapply$, however, we must prove that this record has a type of
the form $\tpi{(\rtyp_1\arw\rtyp_2)}$, where $\rtyp_1$ and $\rtyp_2$ are rows.
This is where \Rule{C-Row-GD} and \Rule{C-Row-GL}
(Figure~\ref{fig:row-equations}) come into play. Indeed, by \Rule{C-Row-GD},
the type $\rowall{(\tint\arw\tint)}$ may be written
$\rowall\tint\arw\rowall\tint$. So, \dref{type} may be written
$\tpi{(\rowat\elab{\tbool\arw\tbool}{\rowall\tint\arw\rowall\tint})}$~\dlabel{type2},
which by \Rule{C-Row-GL} may be written
$\tpi{((\rowat\elab{\tbool}{\rowall\tint})\arw(\rowat\elab{\tbool}{\rowall\tint}))}$~\dlabel{type3}.
Thus, \Rule{hmx-Sub} allows deriving that the record $\inject{\elab=\enot;
\esucc}$ has type \dref{type3}. We let the reader continue and conclude that
the program has type $\tpi{(\rowat\elab{\tbool}{\rowall\tint})}$ under the
constraint $\ctrue$ and the initial environment $\ienv$.

This example illustrates a very important use of rows, namely to {lift}
an operation on ordinary values so as to turn it into a {pointwise}
operation on records. Here, we have chosen to lift the standard application
combinator, giving rise to $\rapply$ on records. The point is that, thanks to
the expressive power of rows, we were also able to lift the standard
combinator's {type scheme} in the most straightforward manner, giving
rise to a suitable type scheme for $\rapply$.
\end{example}
%
\begin{exercise}[\Moderate\fullsolution]
Check that the definitions of
Example~\ref{example-record-apply} meet the requirements of
Definition~\ref{def-requirements}.
\iffull{\solref{record-apply}}
\end{exercise}

The previous examples have illustrated the use of rows to typecheck operations
on full records. Let us now move to records with finite domain.  As explained
in the discussion above of records with finite carrier
(p.~\pageref{section-finite-carrier}), they may be either encoded in terms
of full 
records, or given a direct definition. The latter approach is illustrated
below.
%
\begin{example}[Finite records]
\label{example-extensible-records}
For every set of labels $\elabs$ of cardinal $n$, let us introduce a $n$-ary
constructor $\rnil_\elabs$. We define the notations $\rinject{ \elab_1=\et_1;
\ldots; \elab_n=\et_n }$ and $\rinject\evm$, where $\evm$ is a finite mapping
of labels to values, in a manner similar to that of
Example~\ref{example-full-records}.

The three primitive operations on finite records, namely the empty record
$\rnil$, extension $\extend\elab\cdot\cdot$, and access $\access\elab\cdot$,
may be defined as follows. First, the empty record $\rnil$ is precisely the
nullary constructor $\rnil_\varnothing$. Second, for every
$\elab\in\rowlabels$, let extension $\extend\elab\cdot\cdot$ and access
$\access\elab\cdot$ be destructors of arity 1 and 2, respectively, equipped
with the following reduction rules:
$$
\renewcommand{\regle}[4][]{#2&\reduces[\delta]&#3&#1&(\DefRule{#4})\\}
\begin{tabular*}{\linewidth}{RCL.LR}
\regle{\extend\elab\ev{\rinject\evm}}{\rinject{\extension\evm\elab\ev}}{R-Extend}
\regle[(\elab\in\Dom\evm)]{\access\elab{\rinject\evm}}{\evm(\elab)}{R-Access}
\end{tabular*}
$$

Let us now define the syntax of types as in
Example~\ref{example-syntax-normal-field}. Let the initial environment
$\env_0$ contain the following bindings:
$$\begin{array}{r<{{}}.l}
\rnil_{\{ \elab_1, \ldots, \elab_n \}} :& \dmscheme{\tvar_1\ldots\tvar_n}
{\tvar_1\arw\ldots\arw\tvar_n\arw\tpi{(\elab_1:\tpre{\tvar_1};\ldots;\elab_n:\tpre{\tvar_n};\rowall\tabs)}} \\
& \text{where $\elab_1 < \ldots < \elab_n$}
\\
\extend \elab \cdot \cdot :& 
   \dmscheme {\tvar \tvar' \twar}
        {\tpi{(\rowat \elab \tvar \twar)} \arw \tvar' \arw
         \tpi{(\rowat \elab {\tpre{\tvar'}} \twar)}}
\\
\access \elab \cdot :& 
   \dmscheme {\tvar\twar}
        {\typi (\rowat \elab {\tpre \tvar} \twar) \arw \tvar}
\\
\end{array}
$$ Note that, in particular, the type scheme assigned to the empty
record $\rnil$ is $\tpi{(\rowall \tabs)}$. 
\end{example}
%
% TEMPORARY en version longue, on pourrait noter que nous employons la notation
% \rowall\tabs, et non \tabs tout court, de façon à éviter d'utiliser inutilement
% les $\ttycon^\rowkind$ pour $\rowkind\not=\type$.
%
\begin{exercise}[\Recommended, \QuickCheck, \nosolution]
Reconstruct all of the missing kind information in the type schemes
given in Example~\ref{example-extensible-records}.
\end{exercise}
%
\begin{exercise}[\Recommended, \Easy, \nosolution]
\label{encoding-finite-full-records}
Give an encoding of finite records in terms of full records, along the lines
of the discussion of records with finite carrier
(p.~\pageref{section-finite-carrier}). Check that the principal type schemes
associated, via the encoding, with the three operations on finite records are
precisely those given in Example~\ref{example-extensible-records}.
\end{exercise}
%
\begin{exercise}[\Recommended, \QuickCheck]
\solref{record-minus} The extension operation, as defined above, may either
change the value of an existing field or create a new field, depending on
whether the field $\elab$ is or isn't present in the input record. This flavor
is known as {free} extension. Can you define a {strict} flavor of
extension that is not applicable when the field $\elab$ already exists? Can
you define (free and strict flavors of) a {restriction} operation that
removes a field from a record?
\end{exercise}
%
\begin{exercise}[\Recommended, \QuickCheck]
Explain why, when $\tcpre\symleq\tcabs$ holds, subsumption allows a record
with {more} fields to be supplied in a context where a record with
{fewer} fields is expected. This phenomenon is often known as \emph{width
subtyping}. Explain why such is not the case when subtyping is interpreted as
equality. \solref{record-width-subtyping}
\end{exercise}
%
\begin{exercise}[\Moderate\fullsolution]
Check that the definitions of
Example~\ref{example-extensible-records} meet the requirements of
Definition~\ref{def-requirements}.
\iffull{\solref{record-extensible-soundness}}
\end{exercise}

% TEMPORARY en version longue, on pourrait noter qu'en présence de sous-typage
% non structurel, on peut en fait se dispenser de \tpre et \tabs: il suffit
% de coder l'absence d'un champ par \top (pour les records) ou par \bot (pour
% les variantes)

\begin{full}
\begin{example}[Refinement of record types]
% TEMPORARY l'argument qui suit est trop restreint; cette idée a un intérêt
% aussi dans le cadre du sous-typage structurel.
In an equality-only model, records with more fields cannot be used in place
of records with fewer fields. However, this may be partially recovered by a
small refinement of the structure of types.  The presence of fields can
actually be split form their types, thus enabling some polymorphism over the
presence of fields while type of fields  themselves remains fixed.
%
Let $\flagkind$ be a new basic kind. Let type constructors $\tabs$ and
$\tpre$ be both of kind $\flagkind$ and let $\tdot$ be a new type
constructor of kind $\flagkind \kindprod \normalkind \kindarrow \fieldkind$.
Let $\ienv$ contain the following typing assumptions:
$$
\begin{array}{rl}
\rnil :& 
   \dmscheme {\tvar}{\typi {(\rowall (\tabs \tdot \tvar))}}
\\
\extend \elab \cdot \cdot :& 
   \dmscheme {\tzar \tvar \tvar' \twar}
        {\;\typi (\rowat \elab \tvar \twar) \arw \tvar' \arw
         \typi (\rowat \elab {\tzar \tdot \tvar'} \twar)}
\\
\access \elab \cdot :& 
   \dmscheme {\tvar\twar}
        {\;\typi (\rowat \elab {\tcpre \tdot \tvar} \twar) \arw \tvar}
\\
\end{array}
$$
The semantics of records remain unchanged.  The new signature strictly
generalizes the previous one (strictly more programs can be typed) while
preserving type soundness. Here is a program that can now be typed and
that could not be typed before:
$$
\access \ell {(\eif a 
{\extend \elab 1 {\extend {\elab'} \etrue \rnil}}
{\extend \elab 2 \rnil})}
$$
Notice however, that when a present field is forgotten, the type of the
field is not. Therefore two records defining the same field but with
incompatible types can still not be mixed, which is possible in the subtyping
model. 
\end{example}

\begin{example}[Refined subtyping]
\label{example-refined-subtyping} 
The previous refinement for an equality-only model is not much interesting
in the case of a subtyping model. 

The subtyping assumption $\tcpre \symleq \tcabs$ makes $\abs$ play
the role of $\top$ for fields. That is, $\tcabs$ encodes the absence of
information and not the information of absence. In other words, a value
whose field $\ell$ has type $\abs$ may either be undefined or defined on field
$\ell$; in the latter case, the fact that field $\ell$ is actually defined
has just been forgotten. Thus, types only provides
a lower approximation of the actual domain of records.  This is a lost of
accuracy by comparison with the equality-only model, where a record domain
is known from its type. As a result, some
optimizations in the representation of records that are only possible when
the exact domain of a record is statically known are lost.

Fortunately, there is a way to recover such accuracy. A conservative
solution could of course to drop the inequality $\tcpre \symleq \tcabs$.
Notice that this would still be more expressive than using an equality model
since, for instance $\typi (\rowat \ell {\tapp \tpre {(\ttyp_1 \arw
\ttyp_2)}} \ttyp) \subtype  \typi (\rowat \ell {\tapp \tpre \top} \ttyp)$ 
would still hold, as long as ${\arw} \subtype {\top}$ does hold.  This
solution is known as depth-only subtyping for records, while the previous
one provided both depth and width record subtyping.  Conversely, one could
also keep width subtyping and disallow depth subtyping, by preserving the
relation $\tcpre \symleq \tcabs$ while requiring $\tpre$ to be invariant; in
this case, presence of fields can be forgotten as a whole, but the types of
fields cannot be weakened as long as fields remain visible.

Another more interesting solution consists of introducing another type
constructor $\tceither$ of signature $\fieldkind$ and assuming that $\tcpre
\symleq \tceither$ and $\tcabs \symleq \tceither$ (but $\tcpre \not \symleq
\tcabs$). Here, $\tceither$ plays the role of $\top$ for fields
and means either \emph{present} (and forgotten) or \emph{absent}. while
$\tabs$ really means \emph{absent}. The accuracy of typechecking can be
formally stated as the fact that a record value of type $\typi {(\rowat \ell
{\tabs} \ttyp)}$ cannot define field $\ell$.
\end{example}

\begin{example}[mixed subtyping]
\demoreset
It is tempting to mix all variations of Example~\ref
{example-refined-subtyping} together.  As a first attempt, we may assume
that the basic signature $\sig_0$ contains covariant type constructors
$\tpre$ and $\tcmaybe$ and invariant type constructors $\tcpre_=$ and
$\tcmaybe_=$, all of kind $\normalkind \kindarrow \fieldkind$ and two type
constructors $\tabs$ and $\tceither$ of kind $\fieldkind$, and that the
subtype ordering $\symleq$ is defined by the following diagram:
$$
\begin{tabular*}{\linewidth}{.CC,CCC.C}
&&\Rnode{either}{\tceither}&&
\\[1em]
&&\Rnode{maybe}{\tcmaybe}&& 
\\[1em]
&\Rnode{pre}{\tcpre}\quad\null&&\Rnode{maybe=}{\tcmaybe_=}
\\[1em]
&&\Rnode{pre=}{\tpre_=}&& \Rnode{abs}{\tabs}
\\
\end{tabular*}
#>\psset{arrows=->,nodesep=2pt}
\ncline{pre=}{pre}\ncline{pre}{maybe}\ncline{maybe}{either}
\ncline{pre=}{maybe=}\ncline{maybe=}{maybe}
\ncline{abs}{maybe=}
$$
Intuitively, we wish that $\tcpre_=$ and $\tcmaybe_=$ be \emph{logically}
invariant,  $\tcpre$ and $\tcmaybe$ be \emph{logically} covariant, and 
the equivalences
$\tpre_= \ttyp \subtype \tcmaybe_= \ttyp' 
\logeq \ttyp = \ttyp'$ and
$$
\tpre_= \ttyp \subtype \tpre \ttyp' 
\logeq
\tpre \ttyp \subtype \tcmaybe \ttyp'
\logeq
\tcmaybe_= \ttyp \subtype \tcmaybe \ttyp'
\logeq
\ttyp \subtype \ttyp'
\eqno \dlabel 1
$$
simultaneously hold.  However, \dref 1 requires, for instance, type
constructors $\tcpre_=$ and $\tcpre$ to have the same direction, which is
not currently possible since they do not have the same variance.
Interestingly, this restriction may be relaxed by assigning variances 
of directions on a per type constructor basis and define structural subtyping
accordingly (See Exercise~\ref {exercice-variances}).
%
Then, replacing all occurrences of $\tcpre$ by $\tcpre_=$ in $\ienv$
preserves type soundness and allows for both accurate record types and
flexible subtyping in the same setting.
\end{example}

% TEMPORARY Ne faudrait-il pas un pointeur vers Palsberg? (read-only fields)
\begin{exercise}[Relaxed variances, \Moderate, \nosolution]
\label{exercice-variances}
Let $\varnothing$ be allowed as a new variance, let extend the composition of
variances defined in Example~\ref{example-subtyping-models} with $\nu 
\varnothing = \varnothing$, and let $\symleq^\varnothing$ stands for the full
relation on type constructors.  Let each type constructor $\tycon$ of
signature $\dk \kindarrow \kind$ now come with a mapping $\vartheta(\tycon)$
from $\Dom \dk$ to variances.
%
Let $\vartheta(\gtyp, \gtyp', \chemin)$ be the variance of two ground types
$\gtyp$ and $\gtyp'$ at a path $\chemin$ recursively defined by 
$\vartheta(\gtyp, \gtyp', d \cdot \chemin) =
\big(\vartheta(\gtyp(\epsilon)) (d) \cap \vartheta(\gtyp'(\epsilon)) (d)\big)
\;%\comp 
\vartheta(\gtyp/d, \gtyp'/d, \chemin)$ and $\vartheta(\gtyp, \gtyp',
\epsilon) = +$.  
%
Then define the interpretation of subtyping as follows: if $\gtyp, \gtyp' \in
\model_\kind$, let $\gtyp \subtype \gtyp'$ hold if and only if for all path
$\chemin \in \Dom {\gtyp} \cap \Dom {\gtyp'}$, $\gtyp (\chemin)
\symleq^{\vartheta(\gtyp, \gtyp', \chemin)}
\gtyp' (\chemin)$ holds.

Check that the relation $\subtype$ remains a partial ordering.  Check that a
type constructor whose direction $\dird$ has been syntactically declared
covariant (respectively contravariant, invariant) is still logically
covariant (respectively contravariant, invariant) in~$\dird$.
\end{exercise}

\subsection*{Record Concatenation}
\label{section-concatenation}

Record concatenation takes two records and combines them into a new record
whose fields are taken from whatever argument defines them. Of course, there
is an ambiguity when the two records do not have disjoint domains and a
choice should be made to disambiguate such cases. \emph{Symmetric}
concatenation let concatenation be undefined in this
case \cite{HarperPierce91}, while
\emph{asymmetric} concatenation let one-side (usually the right side)
always take priority.
%
For instance, concatenation with a right-priority semantics
may be described by the following reduction rules, where $\econcat$ 
of arity $2$ stands for concatenation.  

\infax[er-Concat-R]{
\ew \econcat \extend {\ell} {\ev} \ew
\reduces[\delta]
\extend \ell \ev {\ew \econcat \ew'}
}

\infax[er-Concat-Nil-R]{
\ew \econcat \rnil \reduces[\delta] \ew
}

\infax[er-Concat-Nil-L]{
\rnil \econcat \ew \reduces[\delta] \ew
}\relax
%
Despite a rather simple semantics, record concatenation remains hard to type
(with either a strict or a priority semantics).  
%
% Solutions to type inference for record concatenation may be found, for
% instance, in \cite {Wand89,Remy92,pottier-njc-00}.
% TEMPORARY {Citation Palsberg? Other citations?}
%
To tackle this difficulty,
let us consider an indirect semantics via encoding into full
records. Moreover, we may consider records with a single field, or even
simpler restrict our attention to field concatenation.  That is, fields are
of either form $\eapp \pre \ev$ (defined with value $v$) or $\abs$
(undefined) and the concatenation primitive $\econcat$ on fields is given by
the two following reduction rules:
$$
\ev \econcat \tapp \pre \ew \reduces[\delta] \tapp \pre \ew
\hskip 0.1\linewidth \text{and}\hskip 0.1\linewidth
\ev \econcat \abs \reduces[\delta] \ev
$$
Each rule suggest a different typing for $\econcat$.
$$
\dmscheme {\tvar\twar}{\tvar \arw \tpre \twar \arw \tpre \twar}
\hskip 0.1\linewidth \text{or}\hskip 0.1\linewidth
\dmscheme {\tvar}{\tvar \arw \tabs \arw \tvar}
$$
Indeed, both types are sound for concatenation. That is, both types should
be instances of the typing assumption for $\econcat$.  Unfortunately, both
types cannot be instances of a same sound type scheme for concatenation (the
smallest type scheme that generalizes both of them being $\dmscheme
{\tvar\tvar'\tvar''}{\tvar \arw \tvar' \arw \tvar''}$).  Notice that strict
concatenation is not any easier, and would lead to the following two cases:
$\dmscheme {\twar}{\tabs \arw \tpre \twar \arw \tpre \twar}$ and $\dmscheme
{\twar}{\tpre \twar \arw \tabs \arw \tpre \twar}$.  A simple solution would
be to replace type schemes with sets---or equivalently conjunctions---of
type schemes. Then, the conjunction of the two types schemes above would be a
good typing assumption for field concatenation \cite {Wand89}.  Such an
extension should raise no theoretical problem, but serious practical issues:
a naive solver may in general require exponential time and produce solutions
exponential in the size of the input. However, a restricted form of
disjunction could also be considered and may lead to better performance, for
instance by delaying resolution of constraints much as can be done for
conditional constraints \cite{pottier-njc-00}.

\newcommand{\cc}[5][]{#2\symleq#3\Rightarrow#4 \subtype^{#1}#5} 

Indeed, in the particular case of concatenation, disjunctions can be
factored out with conditional type constraints as described in the
example~\ref {example-conditional-constraints}. Conditional constraints must
now be interpreted in the row model as follows:
% For every type constructor $\ttycon$ of $\sig$ of
% kind $\kind$, for every row kind $\rowkind$ and every mixed-kind $\kind'.\rowkind'$, 
% let  $\cc[\kind'.\rowkind']{\ttycon} {^{\kind.\rowkind}\cdot} \cdot \cdot$ be a
% predicate of signature $\kind.\rowkind \kindprod \kind'.\rowkind' \kindprod \kind'.\rowkind'
% \kindarrow \cdot$.  
If $\gtyp_0 \in \model_{\kind.\rowkind}$ and $\ttyp_1, \ttyp_2 \in
\model_{\kind'.\rowkind'}$,  then $\cc {\ttycon^\rowkind} {\ttyp_0}{\ttyp_1}{\ttyp_2}$
holds, if and only if (i) $\rowkind = \type$ and $\tycon = \gtyp_0(\epsilon)$
implies $\gtyp_1 = \gtyp_2$ or (ii) $\rowkind = \row \elabs$ and for all $\ell \in
\rowlabels \setminus \elabs$, $\tycon = \gtyp_0(\ell)$ implies $\gtyp_1/\ell =
\gtyp_2/\ell$.
\begin{exercise}[\Recommended, \Challenging, \nosolution] 
Give an algorithm to solve conditional constraints and 
prove its correctness.
\end{exercise}
With conditional constraints field concatenation can be given the unique
type scheme:  
$$
\scheme {\tvar_1\tvar_2\tvar}{
\cc \tpre {\tvar_2} {\tvar_2} \tvar \wedge
\cc \tabs {\tvar_2} {\tvar_1} \tvar 
}
{\tvar_1 \arw \tvar_2 \arw \tvar}
$$
Lifting field concatenation to record concatenation, we get following
principal type scheme for record concatenation:
$$
\scheme {\tvar_1\tvar_2\tvar}{
\cc \tpre {\tvar_2} {\tvar_2} \tvar \wedge
\cc \tabs {\tvar_2} {\tvar_1} \tvar 
}
{%\hskip 10em plus -1fil\null \eqno 
 \typi (\tvar_1) \arw \typi (\tvar_2) \arw \typi (\tvar)}
$$
Notice that the conditional constraints above applies to rows of kind $\row
\varnothing$: it will be distributed to fields by the solver (to be defined)
as the row variable $\tvar_2$ gets instantiated.
\begin{exercise}[\Recommended, \Challenging, \nosolution] 
Prove type soundness for record concatenation with the conditional constraint
typing given above. 
\end{exercise}

% TEMPORARY The following example should probably be skipped, even in the long
% version. 
\begin{example}
\newcommand{\emap}{\kwtt{map}}
A solution to typing record concatenation without  uses is based 
on a wrapper interpretation of records: a record field can be seen as a
function that given a field value $\ev$ as argument 
returns the field concatenation of $\ev$ with itself. 
Using the $\emap$ primitive of Exercise~\ref{ex:record-map}, 
let the new interpretation of records be defined by: 
\let \Langle \langle
\let \Rangle \rangle
\def \llangle {\Langle\!\Langle}
\def \rrangle {\Rangle\!\Rangle}
\newcommand {\lift}[1]{\begingroup \let \langle \llangle\let \rangle
\rrangle #1\endgroup} 
\newcommand{\encoding}[1]{[\![#1]\!]}
$$
\begin{tabular*}{\linewidth}{.C.R<{\eqdef{}},>{{}}L.R.}&
\lift \rnil & \inject {\efun \erar \erar}
\\&\omit$\hfil :{}$ & 
    \dmscheme {\tvar}{\typi (\tvar \arw \tvar)}
\\&\lift{\extend \elab \cdot \cdot} & 
    \efun \evar {\efun {\evar'} {\override \elab {\efun \erar {\eapp \pre
    \evar'}} {\evar}}} 
\\&\omit$\hfil :{}$  & 
  \dmscheme {\tvar\tvar'\twar\twar'}
    \typi (\rowat \ell {\twar'} \twar) \arw \tvar \arw 
    \typi (\rowat \ell {\tapp \tpre (\tvar' \arw \tvar)}  \twar)
\\&\lift{\access \elab \cdot} & 
    \efun \evar {\eapp {(\project (\elab \evar))} \abs}
\\&\omit$\hfil :{}$ & 
  \dmscheme{\twar\tvar\tvar'}
    \typi (\rowat \ell {\tabs \arw \tpre \twar} \tvar \arw \tvar') \arw
    \twar 
\\&
\lift {\econcat} & 
    \eapp \emap
    {(\efun \evar {\efun {\evar'}
        {\efun \erar {\tapp \evar (\tapp {\evar'} \erar)}}})}
\\&\omit$\hfil :{}$ & 
  \dmscheme {\tvar\tvar'\tvar''}
    \typi (\tvar \arw \tvar') \arw
    \typi (\tvar' \arw \tvar'') \arw
    \typi (\tvar \arw \tvar'')
\end{tabular*}
$$
Intuitively, the new interpretation models the old interpretation in the
sense that any record value $\ev$ obtained of the new interpretation
can be turned  into a record value for the old interpretation 
by reducing $\eapp[2] \emap \ev {\inject \abs}$. 
% 
However, rather than proving such a property, it is simpler to take the 
types  above as typing assumptions for extensible records
and verify the requirements of Lemma~\ref{def-requirements}.
\end{example}
\end{full}

%%%%%%%%%%%%%%%%

\subsection*{Polymorphic Variants}
\label{section-variants}

\index{polymorphic variants|(}
\index{variants, polymorphic|(}
\index{type inference!polymorphic variants|(}

% TEMPORARY noter que la sémantique n'est pas dans le même style

So far, we have emphasized the use of rows for flexible typechecking of
operations on records. The record type constructor $\typi$ expects one
parameter, which is a row; informally speaking, one might say that it is a
{product} constructor of infinite arity. It appears natural to also
define {sums} of infinite arity. This may be done by introducing a
new unary type constructor $\tysigma$, whose parameter is a row.

As in the case of records, we use a nullary type constructor $\tcabs$ and a
unary type constructor $\tcpre$ in order to associate information with every
row label. Thus, for instance, the type
$\tsigma{(\rowat{\elab_1}{\tpre{\ttyp_1}}{\rowat{\elab_2}{\tpre{\ttyp_2}}{\rowall\tabs}})}$
is intended to contain values of the form $\etag{\elab_1}{\ev_1}$, where
$\ev_1$ has type $\ttyp_1$, or of the form $\etag{\elab_2}{\ev_2}$, where
$\ev_2$ has type $\ttyp_2$. The type constructors $\tcabs$ and $\tcpre$ are
{not} the same type constructors as in the case of records. In
particular, their subtyping relationship, if there is one, is
reversed. Indeed, the type
$\tsigma{(\rowat{\elab_1}{\tpre{\ttyp_1}}{\rowat{\elab_2}{\tabs}{\rowall\tabs}})}$
is intended to contain only values of the form $\etag{\elab_1}{\ev_1}$, where
$\ev_1$ has type $\ttyp_1$, so it is safe to make it a subtype of the above
type; in other words, it is safe to allow $\tabs\subtype\tpre{\ttyp_2}$. In
spite of this, we keep the names $\tcabs$ and $\tcpre$ by tradition.

The advantages of this approach over algebraic data types are the same as in
the case of records. The namespace of data constructors becomes global, so it
becomes possible for two distinct sum types to share data constructors. Also,
the expressiveness afforded by rows allows assigning types to new operations,
such as \emph{filtering} (see below), which allows functions that perform case
analysis to be incrementally extended with new cases. One disadvantage is that
it becomes more difficult to understand what it means for a function defined
by pattern matching to be {exhaustive}; this issue is, however, out of
the scope of \this.
%
\begin{example}[Polymorphic variants]
\label{example-polymorphic-variants}
For every label $\elab\in\rowlabels$, let us introduce a unary constructor
$\elab$ and a ternary destructor $\appexcase \elab \cdot \cdot \cdot$. We
refer to the former as a \emph{data constructor}, and to the latter as a
\emph{filter}. Let us also introduce a unary destructor $\evoid$. We equip
these destructors with the following reduction rules:
$$
\renewcommand{\regle}[4][]{#2&\reduces[\delta]&#3&#1&(\DefRule{#4})\\}
\begin{tabular*}{\linewidth}{RCL.LR}
\regle{\appexcase \elab \ev {\ev'} {(\etag \elab \ew)}}
        {\eapp \ev \ew}
        {R-Filter-1}
\regle[\text{if $\elab \not=\elab'$}]
        {\appexcase \elab \ev {\ev'} {(\etag {\elab'} \ew)}}
        {\eapp {\ev'} {(\etag {\elab'} \ew)}}
        {R-Filter-2}
\end{tabular*}
$$

Let us define the syntax of types as follows. Let there be two basic kinds
$\normalkind$ and $\vfieldkind$. Let $\sig_0$ consist of the type constructors
$\arw$, $\tcabs$, and $\tcpre$, whose respective {\tcsignature s} are
$\normalkind\kindprod\normalkind\kindarrow\normalkind$, $\vfieldkind$, and
$\normalkind\kindarrow\vfieldkind$. Let $\sig_1$ consist of the record type
constructor $\tysigma$, whose {\tcsignature} is
$\vfieldkind\kindarrow\normalkind$. Note the similarity with the case
of records (Example~\ref{example-syntax-normal-field}).

Subtyping is typically interpreted in one of two ways. One is equality. The
other is the nonstructural subtyping order obtained by letting $\arw$ be
contravariant in its domain and covariant in its codomain, $\tysigma$ be
covariant, $\arw$ and $\tysigma$ be incompatible, and letting
$\tcabs\symleq\tcpre$. Compare this definition with the case of
records (Remark~\ref{remark-row-subtyping}).

To complete the setup, let the initial environment $\ienv$ contain the
following bindings:
$$
\begin{array}{rl}
\etag \elab \cdot :& 
   \dmscheme {\tvar\twar}
        {\tvar \arw \tsigma {(\rowat \elab {\tpre \tvar} \twar)}}
\\
\appexcase \elab \cdot \cdot \cdot :& 
   \dmscheme {\tvar \tvar' \twar\twar'}
        { (\tvar \arw \twar) \arw 
          (\tsigma {(\rowat \elab {\tvar'} \twar')} \arw \twar) \arw
           \tsigma {(\rowat \elab {\tpre \tvar} \twar')} \arw \twar }
\\
\evoid :& 
   \dmscheme \tvar
   {\tsigma {(\rowall \tabs)} \arw \tvar}
\end{array}$$
%
The first binding means, in particular, that if $\ev$ has type $\ttyp$, then a
value of the form $\etag\elab\ev$ has type
$\tsigma{(\rowat\elab{\tpre\ttyp}{\rowall\tabs})}$. This is a sum type with
only one branch labeled $\elab$, hence a very precise type for this
value. However, it is possible to instantiate the row variable $\twar$ with
rows other than $\rowall\tabs$. For instance, the value $\etag\elab\ev$ also
has type
$\tsigma{(\rowat\elab{\tpre\ttyp}{\rowat{\elab'}{\tpre{\ttyp'}}{\rowall\tabs}})}$.
This is a sum type with two branches, hence a somewhat less precise type, but
still a valid one for this value. It is clear that, through this mechanism,
the value $\etag\elab\ev$ admits an infinite number of types. The point is
that, if $\ev$ has type $\ttyp$ and $\ev'$ has type $\ttyp'$, then both
$\etag\elab\ev$ and $\etag{\elab'}{\ev'}$ have type
$\tsigma{(\rowat\elab{\tpre\ttyp}{\rowat{\elab'}{\tpre{\ttyp'}}{\rowall\tabs}})}$,
so they may be stored together in a homogeneous data structure, such as a
list.

Filters are used to perform case analysis on variants, that is, on values of a
sum type. According to \Rule{R-Filter-1} and \Rule{R-Filter-2}, a filter
$\excase \elab \ev {\ev'}$ is a function that expects an argument of the form
$\etag {\elab'} \ew$ and reduces to $\eapp \ev \ew$ if $\elab'$ is $\elab$ and
to $\eapp {\ev'} {(\etag {\elab'} \ew)}$ otherwise. Thus, a filter defines a two-way branch,
where the label of the data constructor at hand determines which branch is
taken. The expressive power of filters stems from the fact that they may be
organized in a sequence, so as to define a multi-way branch. The inert filter
$\evoid$, which does not have a reduction rule, serves as a terminator for
such sequences. For instance, the composite filter $\excase \elab \ev {\excase
{\elab'} {\ev'} \evoid}$, which may be abbreviated as $[\,\elab :\ev \mid
\elab' : \ev'\,]$, may be applied either to a value of the form
$\etag\elab\ew$, yielding $\eapp\ev\ew$, or to a value of the form
$\etag{\elab'}{\ew'}$, yielding $\eapp{\ev'}{\ew'}$. Applying it to a value
$\ew$ whose head symbol is not $\elab$ or $\elab'$ would lead to the term
$\eapp\evoid\ew$, which is stuck, since $\evoid$ does not have a reduction
rule.

For the type system to be sound, we must ensure that every
application of the form $\eapp\evoid\ew$ is ill-typed. This is achieved by the
third binding above: the domain type of $\evoid$ is $\tsigma{(\rowall
\tabs)}$, a sum type with zero branches, which contains no values. The return
type of $\evoid$ may be chosen at will, which is fine; since it can never be
invoked, it can never return. The second binding above means that, if $\ev$
accepts values of type $\ttyp$ and $\ev'$ accepts values of type
$\tsigma{(\rowat \elab {\ttyp''}{\ttyp'})}$, then the filter $\excase \elab
\ev {\ev'}$ accepts values of type
$\tsigma{(\rowat\elab{\tpre\ttyp}{\ttyp'})}$. Note that any choice of
$\ttyp''$ will do, including, in particular, $\tabs$. In other words, it is
okay if $\ev'$ does not accept values of the form $\etag\elab\ew$. Indeed, by
definition of the semantics of filters, it will never be passed such a value.
\end{example}
%
\begin{exercise}[\Moderate, \nosolution]
Check that the definitions of Example~\ref{example-polymorphic-variants} meet
the requirements of Definition~\ref{def-requirements}.
\end{exercise}
%
\begin{remark}
It is interesting to study the similarity between the type schemes assigned to
the primitive operations on polymorphic variants and those assigned to the
primitive operations on records (Example~\ref{example-extensible-records}).
The type of $\evoid$ involves the complete row $\rowall\tabs$, just like the
empty record $\rnil$. The type of $\appexcase\elab\cdot\cdot\cdot$ is pretty
much identical to the type of record extension $\extend\elab\cdot\cdot$,
provided the three continuation arrows $\arw\twar$ are dropped. Last, the type
of the data constructor $\elab$ is strongly reminiscent of the type of record
access $\access\elab\cdot$. With some thought, this is hardly a surprise.
Indeed, records and variants are \emph{dual}: it is possible to encode the
latter in terms of the former and vice-versa. For instance, in the encoding of
variants in terms of records, a function defined by cases is encoded as a
record of ordinary functions, in continuation-passing style. Thus, the
encoding of $\evoid$ is $\efun\efar{\eapp\efar\rnil}$, the encoding of
$\excase\elab\ev{\ev'}$ is $\efun\efar{\eapp\efar{\extend\elab\ev{\ev'}}}$,
and the encoding of $\etag\elab\ev$ is
$\efun\erar{\eapp{\access\elab\erar}\ev}$. The reader is encouraged to study
the type schemes that arise out of this encoding and how they relate to the
type schemes given in Example~\ref{example-polymorphic-variants}.
% TEMPORARY Là, je botte en touche... peut-on en dire plus?
\end{remark}
%
\begin{example}[First-class messages]
\label{example-first-class-msg}
\newcommand{\esend}{\#}
%
In a programming language equipped with both records and variants, it is
possible to make the duality between these two forms of data explicit by
extending the language with a primitive operation $\esend$ that turns a record
of ordinary functions into a single function, defined by cases. More
precisely, $\esend$ may be introduced as a binary destructor, whose reduction
rule is
%
$$
\renewcommand{\regle}[4][]{#2&\reduces[\delta]&#3&#1&(\DefRule{#4})\\}
\begin{tabular*}{\linewidth}{RCL.LR}
\regle{\eapp[2]\esend\ev{(\etag\elab\ew)}}
      {\eapp{\access\elab\ev}\ew}
      {R-Send}
\end{tabular*}
$$
%
What type may we assign to such an operation? In order to simplify the answer,
let us assume that we are dealing with full records
(Example~\ref{example-full-records}) and full variants; that is, we have a
single basic kind $\normalkind$, and do not employ $\tcabs$ and
$\tcpre$. Then, a suitable type scheme would be
%
$$\dmscheme{\tvar\twar}{\tpi{(\tvar\arw\rowall\twar)}\arw\tsigma\tvar\arw\twar}$$
%
In other words, this operation accepts a record of functions, all of which
have the same return type $\twar$, but may have arbitrary domain types, which
are given by the row $\tvar$. It produces a function that accepts a parameter
of sum type $\tsigma\tvar$ and returns a result of type $\twar$. The fact that
the row $\tvar$ appears both in the $\tysigma$ type and in the $\typi$ type
reflects the operational semantics. Indeed, according to \Rule{R-Send}, the
label $\elab$ carried by the value $\etag\elab\ew$ is used to extract, out of
the record $\ev$, a function, which is then applied to $\ew$. Thus, the domain
type of the function stored at $\elab$ within the record $\ev$ should match
the type of $\ew$. In other words, at every label, the domain of the contents
of the record and the contents of the sum should be type compatible. This is
encoded by letting a single row variable $\tvar$ stand for both of these
rows. Note that the arrow in $\tvar\arw\rowall\twar$ is really
$\arw^{\row\varnothing}$; once again, we are exploiting the presence of type
constructors of the form $\ttycon^\rowkind$, with $\rowkind\not=\type$, in the
signature $\sig$.

If the record of functions $\ev$ is viewed as an \emph{object}, and if
the variant $\etag\elab\ew$ is viewed as a \emph{message} $\elab$
carrying a parameter $\ew$, then \Rule{R-Send} may be understood as
\emph{(first-class) message dispatch}, a common feature of
object-oriented languages. (The \emph{first-class} qualifier refers to
the fact that the message name $\elab$ is not statically fixed, but is
discovered at runtime.) The issue of type inference in the presence of
such a feature has been studied by \longcite{Nishimura:POPL98},
\longcite{MuellerNishimura:98}, and \longcite{pottier-njc-00}. These
papers address two issues that are not dealt with in the above
example, namely (i) accommodating finite (as opposed to full) record
and variants and (ii) allowing distinct methods to have distinct
result types. This is achieved via the use of subtyping and of some
form of conditional constraints.
\end{example}
%
\begin{exercise}[\Moderate, \nosolution]
Check that the definitions of Example~\ref{example-first-class-msg} meet
the requirements of Definition~\ref{def-requirements}.
\end{exercise}
% TEMPORARY First class labels in ML-Art... (labels codés par leur fonction d'accès)

The name \emph{polymorphic variants} stems from the highly polymorphic type
schemes assigned to the operations on variants
(Example~\ref{example-polymorphic-variants}). A row-based type system for
polymorphic variants was first proposed by \longcite{Remy89}. A somewhat
similar, constraint-based type system for polymorphic variants was then
studied by Garrigue \citeyr{garrigue-98,garrigue-00,garrigue-02} and
implemented by him as part of the programming language Objective Caml.

\index{polymorphic variants|)}
\index{variants, polymorphic|)}
\index{type inference!polymorphic variants|)}

\subsection*{Other Applications of Rows}
\label{section-other-apps}

Typechecking records and variants is the best-known application of rows. Many
variations of it are conceivable, some of which we have illustrated, such as
the choice between {full} and {finite} records and
variants. However, rows may also be put to other uses, of which we now list a
few.

First, since objects may be viewed as records of functions, at least from a
typechecking point of view, rows may be used to typecheck object-oriented
languages in a structural style \cite{Wand94,Remy94}. This is, in particular,
the route followed in Objective Caml \cite{RemyVouillon97}. There, an object
type consists of a row of method types, and gives the object's interface. Such
a style is considered {structural}, as opposed to the style adopted by
many popular object-oriented languages, such as C\verb|++|, Java, and
C\verb|#|, where an object type consists of the {name} of its
class. Thanks to rows, method invocation may be assigned a polymorphic type
scheme, similar to that of record access
(Example~\ref{example-extensible-records}), making it possible to invoke a
specific method (say, $\elab$) without knowing which class the receiver object
belongs to.

Rows may also be used to encode sets of properties within types or to encode
type refinements, with applications in type-based program analysis. Some
instances worth mentioning are soft typing \cite{CartwrightFagan90,
WrightCartwright94}, exception
analysis \cite{Leroy:2000:TBA,pottier-simonet-toplas-03}, and static
enforcement of an access control policy \cite{pottier-skalka-smith-01}.
BANE \cite{faehndrich-phd-99}, a versatile program analysis toolkit, also
implements a form of rows.

\begin{full}
The key to rows is to decompose the set of row labels into a class of finite
partitions that is closed by some operations. Here, those partitions are
composed of singleton labels and cofinite sets of labels; the operations
are merging (or conversely splitting) a singleton label and a cofinite set
of labels.  Other decompositions are possible, for instance, one could
imagine to consider labels in a two-dimensional space. More generally,
labels might also be given internal structure, for instance, one might
consider automatons as labels.  Notice also that record types are
stratified, since rows, that is, expressions of kind $\row \elabs$, may not
themselves contain records---constructors of $\sig_1$ are only given the
image row kind $\type$.  This restriction can be partially relaxed leading
to rows of increasing degrees \cite {Remy!projective-ml} \ldots and
complexity! Yet more intriguing are type-indexed rows where labels are
suppressed \cite {ShieldsPOPL01}.
\end{full}

\subsection*{Variations on Rows}

A type system may be said to have {rows}, in a broad sense, if mappings
from labels to types may be (i) defined incrementally, via some syntax for
extending an existing mapping with information about a new label and (ii)
abstracted by a type variable. In \this, which follows Rémy's
ideas \citeyr{Remy/start,Remy!mleth,Remy!projective-ml}, the former feature is
provided by the row constructors $(\rowat\elab\cdot\cdot)$, while the latter
is provided by the existence of row variables, that is, type variables of row
kind $\row\elabs$ for some $\elabs$. There are, however, type systems that
provide (i) and (ii) while departing significantly from the one presented
here. These systems differ mainly in how they settle some important design
choices:
%
\begin{enumerate}
\item Does a row denote a {finite} or an {infinite} mapping from
      labels to types?
\item Is a row with duplicate labels considered well-formed? If not,
      by which mechanism is it ruled out?
\end{enumerate}
%
In Rémy's approach, every row denotes an infinite (in fact, cofinite) mapping
from labels to types. The type constructors $\tcabs$ and $\tcpre$ are used to
encode domain information within field types. A row with duplicate labels,
such as $(\rowat\elab{\ttyp_1}{\rowat\elab{\ttyp_2}{\ttyp_3}})$, is ruled out
by the kind system.
%
%% (In fact, provided all programmer-supplied type annotations are well-kinded,
%% constraint generation must produce a well-kinded constraint. Since constraint
%% solving preserves well-kindedness, the constraint solver needs not be aware of
%% kinds at all.)
%
Below, we mention a number of type systems that make different design choices.

The first use of rows for typechecking operations on records, including record
extension, is due to Wand \citeyr{Wand87,Wand88a}. In Wand's approach, rows
denote finite mappings. Furthermore, rows with duplicate labels are considered
legal; row extension is interpreted as function extension, so that, if a label
occurs twice, the later occurrence takes precedence. This leads to a
difficulty in the constraint solving process: the constraint
$(\rowat\elab{\ttyp_1}{\rtyp_1}) = (\rowat\elab{\ttyp_2}{\rtyp_2})$ entails
$\ttyp_1=\ttyp_2$, but does {not} entail $\rtyp_1 = \rtyp_2$, because
$\rtyp_1$ and $\rtyp_2$ may have different domains---indeed, their domains may
differ at $\elab$. Wand's proposed solution \citeyr{Wand88a} introduces a
four-way disjunction, because each of $\rtyp_1$ and $\rtyp_2$ may or may not
define $\elab$. This gives type inference exponential time complexity.

Later work \cite{Berthomieu/tagged-types, berthomieu-sagazan-95} interprets
rows as {infinite} mappings but sticks with Wand's interpretation of row
extension as function extension, so that duplicate labels are allowed. The
constraint solving algorithm rewrites the problematic constraint
$(\rowat\elab{\ttyp_1}{\rtyp_1}) = (\rowat\elab{\ttyp_2}{\rtyp_2})$ to
$(\ttyp_1=\ttyp_2) \wedge (\rtyp_1 =_{\set\elab} \rtyp_2)$, where the new
predicate $=_\elabs$ is interpreted as row equality \emph{outside
$\elabs$}. Of course, the entire constraint solver must then be extended to
deal with constraints of the form $\ttyp_1=_\elabs\ttyp_2$. The advantage of
this approach over Wand's lies in the fact that no disjunctions are ever
introduced, so that the time complexity of constraint solving apparently
remains polynomial.
%
% NOTE Bien que l'idée de Berthomieu me semble viable, je me demande si
% l'algorithme décrit dans son article n'est pas incomplet. En effet, il
% semble résoudre $\tvar =_\elabs \twar$ en unifiant tout bonnement $\tvar$
% et $\twar$.

Several other works make opposite choices, sticking with Wand's
interpretation of rows as finite mappings but forbidding duplicate labels.
No kind discipline is imposed: some other mechanism is used to ensure that
duplicate labels do not arise. In \longcite{Jategaonkar&88} and \longcite{Jategaonkar89},
somewhat {ad hoc} steps are taken to ensure that, if the row
$(\rowat\elab\ttyp\tvar)$ appears anywhere within a type derivation, then
$\tvar$ is never instantiated with a row that defines $\elab$.
%
% jones-92 : prédicats lacks et has + syntaxe d'extension, restriction de rangées. Cet appareil ne semble pas minimal
% et a été simplifié dans les articles suivants de Jones, donc je ne le cite pas ici. Il est cité ailleurs pour les
% qualified types.
%
\newcommand{\placks}{\;\kwd{lacks}\;}
In \longcite{gaster-jones-96}, \longcite{gaster-98}, and \longcite{jones-peyton-jones-99}, explicit
constraints prevent duplicate labels from arising. This line of work uses
\emphindex{qualified types} \cite{jones94qualified}, a constraint-based
type system that bears strong similarity with \hmx. For every label $\elab$, a
unary predicate $\cdot\placks\elab$ is introduced; roughly
speaking, the constraint $\rtyp\placks\elab$ is considered to hold
if the (finite) row $\rtyp$ does not define the label $\elab$. The constrained
type scheme assigned to record access is
%
$$\access\elab\cdot:\scheme{\tvar\twar}{\twar\placks\elab}{\tpi{(\rowat\elab\tvar\twar)}\arw\tvar}.$$
%
The constraint $\twar\placks\elab$ ensures that the row
$(\rowat\elab\tvar\twar)$ is well-formed. Although interesting, this approach
is not as expressive as that described in \this.  For instance, although it
accommodates record update (where the field being modified is known to exist
in the initial record) and {strict} record extension (where the field is
known {not} to initially exist), it cannot express a suitable type scheme
for {free} record extension, where it is {not known} whether the
field initially exists.
% TEMPORARY on peut faire clarifier ce point en exercice.
This approach has been implemented as the ``Trex'' extension to Hugs \cite{hugs}.

\newcommand{\phas}{\;\kwd{has}\;}
It is worth mentioning a line of type systems \cite{OhoriBuneman88:TIDBPL,
OhoriBuneman89:STIPC, ohori-95} that do {not} have rows, because they
lack feature (i) above, but are still able to assign a polymorphic type scheme
to record access. One might explain their approach as follows. First, these
systems are equipped with ordinary, structural record types, of the form
$\set{\elab_1:\ttyp_1;\ldots;\elab_n:\ttyp_n}$. Second, for every label
$\elab$, a binary predicate $\cdot\phas\elab:\cdot$ is
available. The idea is that the constraint $\ttyp\phas\elab:\ttyp'$
holds if and only if $\ttyp$ is a record type that contains the field
$\elab:\ttyp'$. Then, record access may be assigned the constrained type
scheme
%
$$\access\elab\cdot:\scheme{\tvar\twar}{\tvar\phas\elab:\twar}{\tvar\arw\twar}.$$
%
This technique also accommodates a restricted form of record update,
where the field being written must initially exist and must keep its
initial type; it does not, however, accommodate any form of record
extension, because of the absence of row extension in the syntax of
types. Although the papers cited above employ different terminology,
we believe it is fair to view them as constraint-based type
systems. In fact, \fulllongcite{Sulzmann&97} prove that Ohori's system
\citeyr{ohori-95} may be viewed as an instance of \hmx. Sulzmann
\citeyr{sulzmann-00} proposes several extensions of it, also presented
as instances of \hmx, which accommodate record extension and
concatenation using new, {ad hoc} constraint forms in addition to
$\cdot\phas\elab$.

In the \emph{label-selective
$\lambda$-calculus} \cite{Garrigue:Label,garrigue-furuse-95}, the arrow type
constructor carries a label, and arrows that carry distinct labels may
{commute}, so as to allow labeled function arguments to be supplied in
any order. Some of the ideas that underlie this type system are closely
related to rows.

\longcite{pottier-lics03} describes an instance of \hmx where rows are
{not} part of the syntax of types: equivalent expressive power is
obtained via an extension of the constraint language. The idea is to work with
constraints of the form $\rtyp_1 \subtype_\elabs \rtyp_2$, where $\elabs$ may
be finite or cofinite, and to interpret such a constraint as row subtyping
\emph{inside $\elabs$}. In this approach, no new type variables need be
allocated during constraint solving; contrast this with \Rule{S-Mutate-LL},
\Rule{S-Mutate-GD}, and \Rule{S-Mutate-GL} in
Figure~\ref{fig:solver-unif-rcd}. One benefit is to simplify the complexity
analysis; another is to yield insights that lead to generalizations of rows.

Even though rows were originally invented with type inference in mind, they
are useful in explicitly typed languages as well; indeed, other approaches to
typechecking operations on records appear quite complex \cite{CardelliMitchell89}.
%
% NOTE Cette phrase est bateau, douteuse, et pas vraiment à sa place, mais ce serait
% ennuyeux de supprimer cette citation.
%
% NOTE je ne vois pas trop pourquoi Cardelli91:records était cité ici.
% C'est un codage d'un système avec rangées vers un système sans rangées.
% Dans la mesure où il semble évident qu'un tel codage est possible, mais
% non modulaire, je n'en vois pas vraiment l'intérêt. Mais je n'ai pas lu
% cet article en détail.

\index{type inference!row variables|)}
\index{type inference!records|)}
\index{type inference|)}
\index{type inference!HM(X)|)}

\index{ML|)}
\index{programming languages!ML|)}

\endinput

Voici une liste de choses à faire, un jour, idéalement.

Vérifier que l'index est bien fait.

Ajouter encore beaucoup d'EXEMPLES (exemples de dérivations de typage,
de programmes bien typés, de résolution de contraintes, etc. etc.)

Clarifier ce qui est nécessaire pour le cas du lambda-calcul simplement
typé? Pourquoi pas extraire la `tranche' simplement typée de tout notre
bazar et en faire un première section introductive, en tête, très
concrète, afin que le lecteur sache un peu où on va avant de se lancer
dans le grand développement abstrait?

Ajouter une section concernant les abréviations de types (en particulier
récursives) et leur emploi dans Objective Caml.

Dire quelque chose des types intersection et du lien avec notre approche.
Dire un mot du système développé dans la thèse de Damas?

Parler des multi-méthodes et des type classes.

