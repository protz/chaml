\documentclass[10pt,a4paper,twoside,titlepage,twocolumn]{article}
\usepackage{euler}
\usepackage{dblfloatfix}
\usepackage{fontspec}
\usepackage{url}
\usepackage[naturalnames,breaklinks]{hyperref}
\usepackage{graphicx}
\hypersetup{pdftitle={Jonathan Protzenko - Translating OCaml into System F}
pdfauthor={Jonathan Protzenko},
bookmarks=false}
\usepackage{varioref}
\usepackage{algorithm2e}
\usepackage{bussproofs}
\usepackage[svgnames]{xcolor}
\input xelatexstylesheet.tex
\usepackage{xunicode}
\usepackage{xltxtra}
\usepackage{multicol}
%\usepackage{microtype}
\usepackage{float}


% ATTAPL
\usepackage{usefulstuff/tinker}
\usepackage{usefulstuff/bcprules}
%\usepackage{usefulstuff/bcptheorem}
\usepackage{mlrow}
%\usepackage{usefulstuff/attapl-misc}
\usepackage{amsmath,amssymb}
\usepackage{stmaryrd}


\usepackage{tikz} 
\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,fit,positioning,shapes.symbols,chains,shapes.misc}

\newcommand{\code}[1]{\textbf{\texttt{#1}}}

\newcommand{\forallvec}{\ensuremath{\overline{\forall}}}

\defaultfontfeatures{Mapping=tex-text}
\setmainfont[SmallCapsFont={Fontin SmallCaps}]{Fontin}
\setsansfont[Scale=.95]{Aller}
\setmonofont[Scale=.88]{DejaVu Sans Mono}

%two column float page must be 90% full
\renewcommand\dblfloatpagefraction{.90}
%two column top float can cover up to 80% of page
\renewcommand\dbltopfraction{.80}
%float page must be 90% full
\renewcommand\floatpagefraction{.90}
%top float can cover up to 80% of page
\renewcommand\topfraction{.80}
%bottom float can cover up to 80% of page
\renewcommand\bottomfraction{.80}
%at least 10% of a normal page must contain text
\renewcommand\textfraction{.1}

\newcommand{\wf}[1]{{\fontspec[Colour=888888, Ligatures=Rare, Fractions=On, Contextuals={WordFinal}]{MEgalopolis Extra}#1}}
\title{Translating a Subset of OCaml into System F}
\author{Jonatha\wf{n} Protzenk\wf{o}}
\date{\today}

\begin{document}
{\fontspec[Colour=888888, Ligatures=Rare, Fractions=On]{MEgalopolis Extra}
\maketitle}
\tableofcontents

\part{Some background}

Type-checking functional languages à la ML is a long-time topic that has been
well-studied throughout the past 30 years
(\cite{gordon1978metalanguage,damas1982principal}). More
recently, there has been a surge of interest regarding the translation of rich,
complex, higher languages into simpler, core languages.

These core languages are well-typed, and the combination of simplicity and rich
type information makes them well-suited for program analysis, program
transformation and compilation.

Indeed, Haskell now features ``System FC'' \cite{sulzmann2007system}, a core
language that is both minimalistic and powerful. We applied the same design
principles to a translation from OCaml \cite{ocaml} to System $F_{\eta}$, that
is, System F with coercions \cite{mitchell-88}.

Our contribution is twofold: firstly, a strategy that builds upon previous work by
François Pottier \emph{et. al.} \cite{pottier2005essence} and leverages
constraint solving to build an explicitly typed representation of the original
program; secondly, a translation process that makes all the subtyping
relations explicit and transforms the original, fully-featured program into a System
$F_{\eta}$ term, where all coercions have been made explicit, all redundant
constructs eliminated, and well-typedness preserved. This final representation
can be type-checked before going any further, to ensure the consistency of the
process.

This report is structured as follows. In the next paragraphs, we briefly
describe how our translation process works, introduce the main concepts and our
original motivation. The following part is devoted to our first contribution:
adapting constraint generation to build a decorated term that corresponds to the
original program with full type annotations. Part \ref{part:translation} is all
about our translation process from the decorated AST to a simpler, core,
fully-explicit System F term. Finally, we reflect on our work and possible
extensions.


\begin{figure*}[t!]
  \begin{center}
    \begin{tikzpicture}
      [node distance = 1cm, auto,font=\footnotesize,
      every node/.style={node distance=3cm},
      pass/.style={shade, shading=axis, rounded rectangle, draw, fill=black!10, inner sep=5pt, text width=2cm,
        text badly centered, minimum height=1.2cm, font=\bfseries\footnotesize\sffamily},
      tool/.style={rectangle, draw, dashed, fill=black!10, inner sep=5pt, text width=1.5cm,
        text badly centered, minimum height=.8cm, font=\bfseries\footnotesize\sffamily}] 

      \node [pass] (source) {\code{.ml}};
      \node [pass, below of=source] (ast) {AST};
      \node [below of=ast] (phantom) {};
      \node [tool, below=3cm of ast] (unifier) {unifier};
      \node [pass, left of=phantom] (constraint) {Constraint};
      \node [pass, right of=phantom] (term1) {CamlX ``with holes''};
      \node [tool, left=2.2cm of constraint] (solver) {solver};
      \node [pass, below of=term1] (term2) {CamlX};
      \node [pass, below of=term2] (core) {Core};
      \node [tool, left of=core] (typechecker) {type checker};
      \node [below=1cm of core] (end) {Rest of the compilation process…};

      \draw [->, thick] (ast)
        --
        node [text width=2cm] {Pre-allocate solver structures}
        (term1);

      \draw [->, thick] (source)
        --
        node [text width=2cm] {We reuse the OCaml frontend}
        (ast);
      \draw [->, thick] (ast)
        --
        node [text width=2cm, above, left] {Generate a set of constraints}
        (constraint);
      \draw [<->, thick] (constraint) -- (unifier);
      \draw [<->, thick] (unifier) -- (term1);
      \draw [->, thick] (term1)
        --
        node [text width=2cm, right] {Remove union-find structures, switch to De Bruijn for types}
        (term2);
      \draw [->, thick] (term2)
        --
        node [text width=2cm, right] {Remove syntactic sugar, generate
        coercions, use atoms for terms}
        (core);
      \draw [->, very thick] (solver)
        --
        node [text width=2cm] {Fill holes with union-find information}
        (constraint);
      \draw [->, very thick] (typechecker) -- (core);

      \draw [->, thick] (core) -- (end);

    \end{tikzpicture} 

  \end{center}
  \caption{The overall structure of our translator\label{fig:structure}}
\end{figure*}

\section{Motivations}

OCaml is an old, feature-rich language. It features many extensions to the
original ML language, such as
\begin{itemize}
  \item a full module system, including first-class modules, recursive modules,
    functors,
  \item structural objects, with polymorphic methods, classes, class types,
  \item polymorphic variants, with private types,
  \item and a few more ``dark corners''.
\end{itemize}

Even if we consider a small subset of the language, some features are somehow
unusual: for instance, OCaml tries to generalize identifiers \emph{inside}
\code{let}-patterns, unlike Haskell.

The current trend is to prove as much as possible of the compilation process
\cite{compcert}.  Indeed, it is difficult to fully trust OCaml, because of its
many features and possibly complex interactions between all the extra features.

Garrigue has been working in this direction (\cite{garrigue-certified}) and has
successfully extracted a program from a Coq proof that runs a small subset of
OCaml, but it seems hard to prove a full, real-world type inferencer. Specifying
and proving a union-find algorithm has been done before, but specifying and
proving a whole type-inferencer is another order of magnitude.

To strike a middle ground, \textbf{we propose to translate any OCaml program
into an equivalent program written in a System F-like core language}.

The idea is to decompose complex, ``hard-to-trust'' features into simpler, core
building blocks. As an example, the value restriction \emph{à la} Garrigue
\cite{garrigue2004relaxing} relies on \emph{subtyping}. If a type variable only
appears in covariant position, than it can be safely replaced with any of its
supertypes. Garrigue introduces a supertype called \code{zero}, which then can
be subtyped into \code{'a} wherever needed. This is precisely the kind of
operations we would like to translate in an explicit manner, with very basic
constructs, so that the type-checker can later assert the correctness of what we
did.

Ultimately, what we want to do is, instead of proving the well-typedness of a
program written in full OCaml, is type-check \emph{a posteriori} an equivalent,
\emph{core} program. And because we trust this core language, we can be sure
that the program we are about to compile is, if not semantically correct, at
least type-sound and won't crash.

The motto is, to quote Milner \cite{milner1978theory}, ``Well-typed programs
can't go wrong'', and our aim is to enforce that.

\section{A high-level description of our translator}

Naturally, the trade-off is that as expressions become simpler in the ``CoreML''
language, types become possibly more complex. However, this is not considered to
be a problem, and although we end up with complex types, the type-checker for
$F_\eta$ remains simple and elegant.

Our process is made of the following steps (in chronological order)~:
\begin{itemize}
  \item we lex and parse the original OCaml source code, in order to obtain a
    parse tree;
  \item we generate constraints, and build an AST decorated with empty
    ``slots'': this is the CamlX (as in ``eXplicit Caml'') term ``with holes'';
  \item we solve the constraints; as a side-effect, this fills the empty
    ``slots'' in the CamlX term with relevant type information;
  \item we translate the ``impure'', decorated CamlX term into a pure CamlX term
    where ``slots'' have disappeared and types are now represented using De
    Bruijn indices;
  \item we desugar this complex CamlX AST into a CoreML one, and generate
    coercions to justify some steps on-the-fly;
  \item we type-check the resulting CoreML term and ensure well-typedness before
    going further.
\end{itemize}

Diagram \vref{fig:structure} gives a graphical overview of the whole process. We
will detail these steps in more detail.

We decided to reuse the OCaml parser and lexer for our tool. Because parsing
OCaml is painful\footnote{We're thinking about the numerous parsing subtleties:
\code{let x, y =} instead of \code{let (x, y) =}, and many more.}, and because
we needed to compare the output of our tool with OCaml's, we decided to just use
OCaml's \code{parsing/} subdirectory. All the other pieces of code in our tool
are original work.

\begin{TTCOMPONENT}{Our syntax for CamlX\label{fig:camlx}}{!b}
  \let \\ \TTSyntaxAlternative%

  \TTSyntaxCategoryNamed{e}{}                                       {expression} \\
  {\hspace{-1ex}\begin{array}{l}\kwd{let}\; p_1: \sigma_1 = e_1\;\\
    \quad\kwd{and}\; …\\
    \quad\kwd{and}\;p_n: \sigma_n = e_n\;\\
    \quad\kwd{in}\;e\end{array}}                                         {let binding} \\
  {\hspace{-1ex}\begin{array}{l}
    \kwd{function}\\
    \quad|\;p_1: \tau \to e_1\\
    \quad|\; …\\
    \quad|\;p_n: \tau \to e_n\end{array}}                                {function binding}\\
  {x\;[\tau_1,…,\tau_n]}                                            {instanciate $x$}\\
  {e\;e_1\;…\;e_n}                                                  {application} \\
  {\hspace{-1ex}\begin{array}{l}
    \kwd{match}\;e:\sigma\;\kwd{with}\\
    \quad|\;p_1: \sigma_1 \to e_1\\
    \quad|\;…\\
    \quad|\;p_n: \sigma_n \to e_n\end{array}}                            {pattern-matching} \\
  {(e_1, …, e_n)}                                                   {$n$-ary tuple} \\
  {…}  

  \columnbreak

  \TTSyntaxCategoryNamed{x}{}       {identifier} \\
  {s} {a string}

  \TTSyntaxCategoryNamed{p}{}       {pattern} \\
  {x}                               {identifier} \\
  {\_}                              {wildcard} \\
  {p_1\;|\;p_2}                     {or pattern} \\
  {(p_1, \dots, p_n)}               {tuple}

  \TTSyntaxCategoryNamed{\tau}{}       {type} \\
  {F (\tau_1, \dots, \tau_n)}          {type constructor} \\
  {\alpha}                             {a type variable (an integer)}

  \TTSyntaxCategoryNamed{\sigma}{}     {type scheme} \\
  {\forallvec.\;\tau}                  {universal quantification}

  \extraspacehack{.07in}
\end{TTCOMPONENT}

\part[1]{Constraint solving and decorated ASTs}

\begin{TTCOMPONENT}{Syntax of type schemes and constraints\label{fig:constraints_syntax}}{ht}
  \let \\ \TTSyntaxAlternative%

  \TTSyntaxCategoryNamed{\ts}{}        {type scheme} \\
  {\scheme\tvars\co\ttyp}              {}

  \TTSyntaxCategoryNamed{\co,\cp}{}    {constraint} \\
  {\ctrue}                             {truth} \\
  {\cfalse}                            {falsity} \\
  {\predicate\,\ttyp_1\,\ldots\ttyp_n} {predicate application} \\
  {\co \wedge \co}                     {conjunction} \\
  {\cexists\tvars\co}                  {existential quantification} \\
  {\cplet{\evid:\ts}\co}               {type scheme introduction} \\
  {\ccall\evid\ttyp}                   {type scheme instantiation}

  \columnbreak % Wastes some space, but avoids a widow.

  \TTSyntaxCategoryNamed{\co,\cp}{}     {Syntactic sugar for constraints} \\
  {\ldots}                              {As before} \\
  {\ccall{\ts}{\ttyp}}                  {\code{T} is an instance of $\sigma$} \\
  {\cxlet {\evid:\ts}\co}               {def + \code{x} has an instance} \\
  {\exists \ts}                         {$\sigma$ has an instance} \\
  {\cplet {\env}\co}                    {as before} \\
  {\cxlet {\env}\co}                    {as before} \\
  {\exists \env}                        {as before} 

  \extraspacehack{.07in}
\end{TTCOMPONENT}

\begin{figure*}[t!]
  \TTtoprule
  \vspace*{-2ex}
  \begin{bnf}
      \calcule\evid\ttyp
  \eq \ccall\evid\ttyp
  \\
  \calcule{\efun\evar\et}\ttyp
  \eq \exists\tvar_1\tvar_2.(
        \clambda\evar{\tvar_1}{\calcule\et{\tvar_2}}
        \wedge
        \tvar_1\arw\tvar_2\subtype\ttyp 
  ) \\
  \calcule{\eapp{\et_1}{\et_2}}\ttyp
  \eq \exists\tvar_2.(
         \calcule{\et_1}{\tvar_2\arw\ttyp} 
         \wedge
         \calcule{\et_2}{\tvar_2}
  ) \\
  \calcule{\elet\evar{\et_1}{\et_2}}\ttyp
  \eq \cxlet{\evar:\scheme\tvar{\calcule{\et_1}\tvar}\tvar}{\calcule{\et_2}\ttyp}
  \end{bnf}%
  \vspace*{-3ex}
  \TTbottomrule
  \vspace*{-1ex}
  \caption{\label{fig:constraint_generation}Constraint generation}
\end{figure*}

The first requirement for our translation process to work properly is to build a
full AST of the original program, \emph{with explicit type annotations}. This is
the CamlX AST.  Type annotations are indeed required: given that type inference
in System F is undecidable, the only opportunity we have to perform type
inference is when we're still in ML. Once we have that explicit type annotation,
we can send our program into System F, Curry-style, and then perform
type-checking only.

We did \emph{not} reuse the original OCaml further. Indeed, one of the side
goals of this work was to show that constraint-based type-inferencing could be
applied to OCaml, and possibly build a minimalistic replacement for the
type-checker of OCaml. We are currently far from that goal, but fundamentally,
the aim was to try something new, which is why we did not hack the OCaml
typer\footnote{Moreover, the OCaml type-inferencer is known to be hard to
tackle, so as our goal was to build a prototype, it seemed reasonable to us
\emph{not} to try to reuse it.}.

Another reason for not reusing OCaml's type-inferencer is that the process of
generating a fully annotated AST is closely linked to the way the type-inferencer
works. We had no guarantee that OCaml's type-inferencer was well-suited for this
task, which is why it seemed better to us to start from scratch for the
type-inferencer.

In our tool, the part that performs type inferencing corresponds roughly to the
left half of figure \ref{fig:structure}. We perform type inferencing using
constraints \cite{pottier2005essence}: the constraint generator walks the AST to
generate a set of constraints, and another component, called the \emph{solver},
runs a second pass to solve the constraints. The final result is an environment
with all the top-level bindings and their types, or an error if the program
cannot be type-checked.

This is nothing like a fully annotated AST, and some work was needed in order to
transform this process into one suitable for generating a fully annotated AST.
This is the first part of our work: adapting constraint-solving to our needs.

\section{Constraint-based type inference}

The main advantage of constraint-based type inference is the clean, elegant
separation between constraint generation and constraint solving. Whereas
traditional unification algorithms generate constraints and solve them
on-the-fly, constraint-based type inference first generates constraints and then
solves them in a separate pass. The key component that makes it possible is the
\code{let}-binding on type variables.

This design allows one to change the semantics of the original program without
ever touching the solver. For instance, the \code{match} is not generalizing in
OCaml. Turning it into a generalizing \code{match} was simply a matter of
changing a few lines in the constraint generator to insert a
\code{let}-constraint in the right place. This clean separation is a feature we
have tried to keep in our tool.

The reference paper for constraint-based type inference is
\cite{pottier2005essence}. The whole process is described in great detail, and
this is what our implementation used as a reference. Without giving too much
detail, let us just show the syntax of constraints (figure
\vref{fig:constraints_syntax}) and the rules for constraint generation (figure
\vref{fig:constraint_generation}).

The issue with constraint-based type inference is that \emph{this process
answers yes or no}. It does not add any annotation to the original AST, it does
not build another AST, it discards information as it goes, and when it reaches
the end of the constraint solving process, \emph{the best it can do is print the
inferred types for all the top-level bindings}.

This is in contradiction with our goal, which is to build an explicitly typed
representation of the original AST. The following section details how we dealt
with this issue.

\section{Generating an explicitly-typed term}

The key question is: \textbf{how do we obtain an annotated AST?} One might think
of examining the generated constraints to rebuild the original AST. But because
the syntax of constraints does not mirror that of the expressions, we have no
way to recover the original term from a constraint. For instance, a \code{fun x
->} expression in the OCaml AST, a \code{match}, a \code{let}
all result in a \code{let}-constraint (see figure
\vref{fig:constraint_generation}).

\subsection{Encoding the term in the syntax of constraints}

The syntax of constraints has a strong notion of \emph{scope}: a type variable is
defined through a \code{let}-binding, and is only available below it. Similarly,
type schemes are defined through a \code{let}-binding, and are available below
this binding. The natural extension of this design is to include the expressions
in this scope as well, so that we can annotate them with the right types.

That is to say, \emph{we want constraints and annotated expressions to share the
same typing scope}.

In the original design from \cite{pottier2005essence}, the solver is an
automaton, with a set of rules that describe the process of solving a
constraint. The state of the solver is a triple, which consists in a
\emph{stack}, a \emph{unification environment} and a \emph{constraint}. The
final state is as follows:

$$\cexists\tvars[\dots]; U; \ctrue$$

The first component is the environment with all the top-level bindings, $U$ is
the unification knowledge obtained so far, and $\ctrue$ is the empty constraint,
meaning there is nothing left to solve.

In this new scenario, the solver recursively solves constraints and returns
\emph{terms}. For instance, when solving a \underline{\smash{application}}-constraint,
the solver obtains two terms annotated with the right types, and returns an AST
node that applies the first one to the second one. When solving a
\underline{instance}-constraint, it returns an identifier annotated with the
right types. 

That way, instead of just obtaining the type of the top-level
bindings at the end of the process, we end up with:

$$\cexists\tvars[\dots]; U; CT$$

Where $CT$ is the final constraint-as-a-term that corresponds to our annotated AST.

This idea felt natural for a number of reasons:
\begin{itemize}
  \item although tedious, the formalization had clear semantics and scoping
    rules, and consisted in an extension of the previous design from
    \cite{pottier2005essence};
  \item the whole process, unlike figure \vref{fig:structure}, was not split in
    two but linear: the decorated AST was obtained as the output of the solver;
  \item this allows the solver to \emph{hide its internal data structures} and
    never reveal union-find implementation details to the outside.
\end{itemize}

Figure \vref{fig:bad_idea} develops this idea and describes a tentative syntax
for these ``extended constraints''. The \underline{underlined} constraints are
those that generate a term.

However, this solution was discarded. The main reason was that while attractive
in theory, this design had no simple implementation. The number of
term-constraints kept growing, because as we recognized more features, we had to
extend the syntax of constraints. This led us to abandon this design, and we
chose a more tractable process instead.


\begin{TTCOMPONENT}{Alternative encoding of expressions in constraints\label{fig:bad_idea}}{!b}
  \let \\ \TTSyntaxAlternative%

  \TTSyntaxCategoryNamed{C}{}       {constraint} \\
  {\co \wedge \co}                     {conjunction} \\
  {\cexists\tvars\co}                  {existential quantification} \\
  {\ccall\evid\ttyp}                   {type scheme instantiation}\\
  {\ttyp = \ttyp}                      {type equality} \\
  {…}                                  {as before}

  \columnbreak % Wastes some space, but avoids a widow.

  \TTSyntaxCategoryNamed{CT}{}     {constraint with term} \\
  {\cexists\tvars CT}                  {existential quantification} \\
  {CT \wedge\co}                  {mix with a regular constraint} \\
  {\co\wedge CT}                  {mix with a regular constraint} \\
  {\underline{\evid}}                  {\underline{identifier}} \\
  {\ccall{\underline{\evid}}\ttyp}                  {\underline{instance}} \\
  {\underline{\efun\evar\ttyp}.\,CT}                  {\underline{function}} \\
  {\underline{CT}\,\,\underline{CT}} {\underline{\smash{application}}} \\
  {\underline{\smash{\cxlet{\evar:\scheme\tvar{CT}\tvar}{CT}}}} {\underline{\smash{\code{let}-binding}}}

  \extraspacehack{.07in}
\end{TTCOMPONENT}

\subsection{Digging holes in terms}

This new idea is the one we implemented in our tool. While we have been unable
to come up with a formal description of it, we believe it to be well thought-out
and, in our experience, easy to implement.

The basic idea is for the constraint generator to pre-allocate solver
structures\footnote{In the implementation, the constraint generator is a functor
that is parameterized over the solver types and some solver helpers to allocate
new ``slots''.}: for instance, when it generates the constraints for a
\code{fun}-binding, it pre-allocates a slot $\sigma$ for the scheme of the
argument, creates a \code{let}-constraint with a pointer to this slot, and
creates a \code{fun}-expression with a pointer to the \emph{exact same slot}.

The difference with our previous approach is that now the decorated AST
and the constraint tree are separate structures. They are generated in parallel
but are distinct entities; they only share pointers to common structures (see
figure \vref{fig:good_idea}).

\begin{figure}[h!]
  \small
    \begin{tabular}{l|l}
      %\textbf{Original AST} &
      \textbf{Constraint} & \textbf{Decorated AST} \\
      \hline

      %$\cxlet {\evar = e_1} {e_2}$ &

      $\cxlet{
          \forall\tvar[C_1]: \underbrace{\evar\mapsto\tvar}_{\sigma}
        }{C_2}$ &
        
      $\cxlet {\evar:\ts = E_1} {E_2}$ \\

      \hline

      %$\kwd{fun}\;\evar \rightarrow e$ &

      $\exists\tvar_1\tvar_2.(
        \kwd{let}\;\underbrace{\evar\mapsto\tvar_1}_\sigma
        \kwd{in}\;{C_1}
        \wedge
        \tvar_1\arw\tvar_2\subtype\ttyp)$ &

      $\kwd{fun}\;(\evar: \sigma) \to E_1$ \\

      \hline

      \dots & \dots

    \end{tabular}
    (\centering $\sigma$ allows one to recover the union-find structures for $\evar$)
  \caption{Sample output of our modified constraint generator}
  \label{fig:good_idea}
\end{figure}

Right after the constraint has been generated, the common structures $\sigma$
are empty, because the constraint generator only allocated them. However, as
figure \vref{fig:good_idea} shows, the type scheme $\sigma$ is \emph{shared} by
the constraint and the CamlX AST ``with holes''.

What happens next is solving: the driver feeds the solver with the constraint.
When the solver encounters a \code{let}-constraint, it solves it, generalizes
variables as it can, and then fills the slot with all the information gathered
at this \code{let}.

We are voluntarily vague regarding the contents of this slot -- this is the
topic of the next section. Let us just say, for the moment, that because the
slot is mutable, once the constraint solving is done, and although the solver
never heard about the term with ``holes'', the holes are now filled with all the
information required.

\subsection{What are the slots?}

The slots are mutable structures that are shared between the constraint and the
CamlX term ``with holes''. The solver fills them in one side, in the constraint.
They end up on the other side, in the CamlX AST. But what do they contain
exactly?

There are two kinds of slots that decorate the AST:
\begin{itemize}
  \item instantiation slots, that describe all the variables used to
    instantiate a type scheme;
  \item type scheme slots: this can be the type scheme of a whole pattern,
    as in \code{let (x, y) = f e1 e2}, or only the type scheme associated to
    \code{x}.
\end{itemize}

These two kinds of slots are attached to different nodes in the AST and in the
constraint, as is shown in the table below.

\begin{center}
  \small
  \begin{tabular}{l|p{2.5cm}|p{2.5cm}}
    \textbf{Kind of slot} & Attached to… (\textbf{in the CamlX term}) & Attached
    to… (\textbf{in the constraints}) \\
    \hline
    type scheme & \code{let}, \code{fun}, \code{function}, \code{match} & \code{let} \\
    \hline
    instance    & \code{instance} & \code{instance}
  \end{tabular}
\end{center}

To understand why we do this, one must think of what is needed to build an
explicitly typed CoreML term. We will need type annotations in
$\lambda$-bindings. We will also need to coerce patterns at \code{let}-bindings
and \code{function}-bindings.  We might need to coerce generalizing
\code{match}es as well (more on this later).

In order to properly generate all the coercions and type annotations, the
translator must access as much type information as it can, which is why we
decorate the AST. The next part is all about this translation process.

\subsection{About this method}

This method has proved to be easy to implement and work with, although it lacks
some formal clarity. When used carefully, it provides a very flexible way to
forward some information to further parts of the translation process.

The trick is simply to think of what will be needed for the next steps,
pre-allocate it, and then store it somewhere in the CamlX term, so
that the next passes can find it and use it.

One might wonder what is the exact syntax of CamlX. It is very close to the
original OCaml language, except that is has explicit type annotations, as we
said. The full syntax of CamlX is describe in figure \vref{fig:camlx}.


\section{Verifying our work}

Rewriting a type-inferencer and adapting it to our needs is a rather big task, and
implementing it properly requires some care. To make sure our constraint
generator and solver were reliable, we successfully developed a series of tests
to check both the inferred types and the generated constraints.

The authors of \cite{pottier2005essence} developed a prototype implementation of
their work, called \code{mini}. This prototype has a constraint parsing
facility, which we took advantage of. One of the first series of test we ran
consisted in pretty-printing the constraint tree, and having the prototype
implementation solve it. This allowed us to spot a few bugs in \code{mini}, but
also to ensure our results were correct.

Another series of tests took advantage of the original OCaml. After the solver
does his job, we obtain an environment with all the top-level
\code{let}-bindings and their types in it. We wrote a quick parser for the
output of \code{ocamlc -i}\footnote{This prints the inferred signature of a
compilation unit.}, and we have a series of tests that compares the output of
our solver with that of OCaml\footnote{\code{make tests} in the working
directory will run the series of tests.}.

A complexity test is also featured. Boris Yakobowski wrote a prototype
implementation of MLF \cite{boris2008}, that was supposedly faster than the
prototype implementation for \cite{pottier2005essence}. We compared our
tool\footnote{\code{make benchs}, assuming you have the relevant packages,
should plot some nice data} with the two others, and we managed to confirm the
issue reported by B. Yakobowski and fix the corresponding complexity bug in
\code{mini}.

\part{\label{part:translation}System F and coercions…}

\begin{TTCOMPONENT}{Our syntax for System F\label{fig:systemf}}{ht}
  \let \\ \TTSyntaxAlternative%

  \TTSyntaxCategoryNamed{x}{}       {identifier} \\
  {a} {an atom}

  \TTSyntaxCategoryNamed{e}{}                      {expression} \\
  {\Lambda. e}                                                      {type abstraction} \\
  {e[\tau]}                                                         {type application}\\
  {e \blacktriangleright c}                                         {coercion application} \\
  {\lambda (x: \tau). e}                                            {$\lambda$-abstraction} \\
  {e_1\;e_2}                                                        {application} \\
  {\kwd{let}\; x = e_1\;\kwd{in}\; e_2}                             {\code{let}-binding} \\
  {x}                                                               {instanciation} \\
  {\kwd{match}\; e\;\kwd{with}\;p_1 \to e_1\;|\;\dots\;|\;p_n \to e_n}  {\code{match}} \\
  {(e_1, \dots, e_n)}                                              {$n$-ary tuple} \\
  {…}  

  \TTSyntaxCategoryNamed{p}{}       {pattern} \\
  {x}                               {identifier} \\
  {\_}                              {wildcard} \\
  {p_1\;|\;p_2}                     {or pattern} \\
  {(p_1, \dots, p_n)}               {tuple} \\
  {p \blacktriangleright c}         {coerce when matching this pattern}

  \columnbreak

  \TTSyntaxCategoryNamed{c}{}       {coercion} \\
  {\kwd{id}}                        {identity} \\
  {c_1; c_2}                        {composition} \\
  {\forall}                         {$\forall$ introduction} \\
  {\bullet[\tau]}                   {$\forall$ elimination} \\
  {\forall[c]}                      {$\forall$ covariance} \\
  {\forall\times}                   {$\forall$ distributivity} \\
  {\times_i[c]}                          {coercion projection on the $i$-th component}

  \TTSyntaxCategoryNamed{\tau}{}       {type} \\
  {\forall.\,\tau}                        {universally quantified type}\\
  {F (\tau_1, \dots, \tau_n)}          {type constructor} \\
  {\alpha}                             {a type variable (an integer)}

  \extraspacehack{.07in}
\end{TTCOMPONENT}

System F \cite{reynolds1974systemf,girard1972systemf} was introduced at the
beginning of the '70s. Since then, many extensions have been derived: System
$F_{<:}$ (``System F-sub'') with delimited polymorphism, System $F_\mu$ for
recursive types, System $F_\omega$ with functions from types to types, and
System $F_\eta$ \cite{mitchell-88}.

The previous part was all about running type inference on the original OCaml
program, and building a decorated CamlX AST with all the required type information.
This part is about translating the CamlX AST into the CoreML language, close to
System $F_\eta$, which is described in figure \vref{fig:systemf}. 

Our translation is targeting the $F_\eta$ language. $F_\omega$ and $F_\mu$ might
be relevant later on for us, but the subset of OCaml which have chosen has not
required more power than $F_\eta$.

\section{An overview of the translation}

\subsection{Our version of System F}

We're using our own flavour of System $F_\eta$, called ``CoreML'', and
there are a few important features.
\begin{itemize}
  \item We're using patterns, and these patterns are used only in
    \code{match}es.
  \item We're using coercions \emph{inside patterns} -- this will be discussed
    later on.
  \item The only type annotations are at $\lambda$-bindings: the type of the
    arguments is provided. The types of the arguments do not use $\forall$:
    indeed, since \emph{we're in ML}, the arguments cannot have polymorphic
    types.
  \item $\tau_1 \to \tau_2$ is understood to be the application of the ``arrow''
    type constructor; the same goes for product types. Constant types
    (\code{int}, \code{unit}, …) are type constructors with arity 0.
  \item Types are represented using De Bruijn indices, which is why type
    variables are integers.
\end{itemize}

\subsection{The steps to System F}

If one recalls the previous section, the output of the constraint generator is a
term whose type information consists in filled ``slots'', that is to say,
union-find equivalence classes, with mutable structures and a lot of sharing. We
cannot work with such a representation, which is why a first ``cleanup'' step is
needed.

We're using types represented as De Bruijn indices. That is, type variable
$i$ refers to the $i$-th enclosing $\Lambda$ above the term. This is one of the
available representations for types and felt natural in our case.

The first step transforms the union-find structures into these De Bruijn types.
We still have a syntax of expressions (not described here) that is more or less
equivalent to that of the original OCaml AST. We're not trying to desugar
anything. Simply, we're cleaning up the representation of types. This is the
step that goes from CamlX ``with holes'' to CamlX in figure \ref{fig:structure}.

The second step is where all the work is actually performed. A number of OCaml
constructs were redundant: \code{function}, \code{let \dots\;and}, \code{let
pattern = \dots}. We eliminate all those, and sometimes introduce temporary
identifiers, or change the original expression a little bit. We also guarantee
that identifiers are globally unique in the result of this transformation;
they're \emph{atoms}. The syntax of this ``CoreML'' language is described in
figure \vref{fig:systemf}. This step corresponds to the final arrow from CamlX
to CoreML in figure \ref{fig:structure}.

Once we've obtained a System $F_\eta$ term, we can type-check this term to
ensure the consistency before proceeding with the rest of the compilation
process.

\section{Generating coercions}

One feature of OCaml is that, as we said previously, patterns are generalizing.
We discard the value restriction for the sake of clarity in the following
examples.

\subsection{The core issue}

Let us consider the following example.

\begin{verbatim}
let (l, f) = (fun x -> x)([], fun x -> x);;
val l : 'a list = []
val f : 'a -> 'a = <fun>
\end{verbatim}

If now think of F-terms, we'll be matching the expression
$$\Lambda\Lambda \big(\Lambda.\;\lambda (x: 0).\; x\big)
\;\big[(\kwd{list}[1], 0\to0)\big]
\;\big(\kwd{Nil}[1], (\Lambda.\;\lambda (x: 0).\; x) [0]\big)
$$
against the pattern \code{(l, f)}.

In order to assign type schemes to the identifiers of the pattern, we must
compute the type of the expression above. We thus obtain:
$$\forall\forall(\kwd{list}[1],0\to0)$$
that is to say,
$$\forall\alpha\beta.\; (\alpha\;\kwd{list},\;\beta\to\beta)$$

The pattern-matching will fail because the pattern on the left-hand
side has a type that starts with head symbol ``tuple'', and the type on
the right-hand side starts with an abstraction.

The following example is similar:

\begin{verbatim}
# type 'a t = A of ('a -> 'a);;
# let A f = A (fun x -> x);;
val f : 'a -> 'a = <fun>
\end{verbatim}

The expression on the right side of the
equals sign has type $\forall\alpha.\; \kwd{A} (\alpha\to\alpha)$: this pattern matching will also fail.

One might think about changing the expression into an equivalent one, so that
its type is correct. For instance, one can translate
$\Lambda.  A(\lambda x. x)$ into $A (\Lambda.\lambda x. x)$ to solve this issue.

This sounds like a good idea, but this is not applicable in situations such as
the first example. The expression in the first example is an
\emph{application}, so there is no way we can push the $\Lambda$ inside the
application, because this is not possible \emph{syntactically}.

The conclusion is we must operate on types, and we need \emph{coercions}.

\subsection{What is a coercion?}

The previous discussion shows there is a need for \emph{subtyping} in our
system. We need a subtyping judgement: $\Gamma \vdash \tau \leq \tau'$ that tells us
that $\tau'$ is a subtype of $\tau$. Moreover, we need to \emph{apply} these subtyping
judgements at some specific points in the expressions so that the type of an
expression $e$ can be cast from $\tau$ to $\tau'$. This is where we use coercions.

A coercion is a witness for subtyping. We say that a subtyping judgement is
witnessed by a coercion:
$$\Gamma \vdash c: \tau \leq \tau'$$
For instance, the coercion $(\forall\times)$ witnesses the distributivity of
$\forall$ with regard to tuples.
$$\Gamma \vdash (\forall\times): \forall (\tau_1, \dots, \tau_n) \leq
(\forall\tau_1,\dots,\forall\tau_n)$$
The typing rule for a coercion thus becomes:
\begin{prooftree}
  \AxiomC{$\Gamma \vdash e: \tau$}
  \AxiomC{$\Gamma \vdash c: \tau \leq \tau'$}
  \BinaryInfC{$\Gamma \vdash (e \blacktriangleright c): \tau'$}
\end{prooftree}

Other subtyping rules are pretty standard. Here is the $i$-th projection for a
tuple.

\begin{prooftree}
  \AxiomC{$\Gamma \vdash c: \tau_i \leq \tau_i'$}
  \UnaryInfC{$\Gamma \vdash (\times_i[c]): (\tau_1, \dots, \tau_i, \dots, \tau_n) \leq
  (\tau_1, \dots, \tau_i', \dots, \tau_n)$}
\end{prooftree}

Or the covariance of $\forall$.

\begin{prooftree}
  \AxiomC{$\Gamma \vdash c: \tau \leq \tau'$}
  \UnaryInfC{$\Gamma \vdash (\forall[c]): \forall\tau \leq \forall\tau$}
\end{prooftree}

\subsection{Coercions in patterns}

\begin{figure*}[t!]
  \TTtoprule
  \vspace*{-2ex}

  \begin{bnf}
    \llbracket \_ ,\;\tau\,\rrbracket
    \eq \kwd{\_}
    \\
    \llbracket \evar,\;\tau\,\rrbracket
    \eq \evar\blacktriangleright \kwd{elim}(\tau)
    \\
    \llbracket (p_1\;|\;p_2),\;\tau\,\rrbracket
    \eq 
    \big(\llbracket p_1,\;\tau\,\rrbracket\;
    |\; \llbracket p_2,\;\tau\,\rrbracket\big)
    \\
    \llbracket (p_1,\dots,p_n),\;\forallvec(\tau_1,\dots,\tau_n)\,\rrbracket
    \eq \big(
      \llbracket p_1, \forallvec\tau_1\rrbracket,
        \dots,
        \llbracket p_n, \forallvec\tau_n\rrbracket
      \big) \blacktriangleright \kwd{push}(\forallvec)
    \\
    \\
    \kwd{push}(\forall\forallvec)
    \eq
    \forall\big[\kwd{push}(\forallvec)\big];\;\forall\times
    \\
    \kwd{push}(\emptyset)
    \eq
    \kwd{id}
    \\
    \\
    \kwd{elim}(\forall\tau)
    \eq
    \forall\big[\kwd{elim}(\tau)\big];\; \bullet[\perp] \text{ if } 0\,\#\,\tau
    \\
    \kwd{elim}(\forall\tau)
    \eq
    \forall\big[\kwd{elim}(\tau)\big]\text{ otherwise }
    \\
    \kwd{elim}(\tau)
    \eq
    \kwd{id} \text{ when } \tau \neq \forall\tau'
    \\
  \end{bnf}%

  %\vspace*{-3ex}
  \TTbottomrule
  \vspace*{-2ex}
  \caption{\label{fig:coercion_generation}Coercion generation}
\end{figure*}

One feature of OCaml is that \emph{or}-patterns can be nested arbitrarily deep.
The following piece of code is valid:

\begin{verbatim}
type 'a t = A of ('a -> int) | B of ('a -> int) list;;
let (y, (A x | B [x])) = ...;;
\end{verbatim}

However, we cannot simply apply a coercion to the expression on the right-hand
side of the equals sign: the coercions needed on the left side and the right
side of the or-pattern are not the same.
\begin{itemize}
  \item The left side of the or-pattern will require $(\forall t)$, which states
    that $\forall$ can be distributed inside data type $t$.
  \item The right side will require
    $(\forall t);\;(t_0\left[ (\forall \kwd{list}) \right])$
    which states
    that $\forall$ can be distributed inside data type $\kwd{t}$, that its
    $0$-th parameter (it's \code{'a}) is covariant so we can move the $\forall$
    inside the $\kwd{t}$ data type under the $\kwd{A}$ constructor.
\end{itemize}
In the end, we want to assign $\forall.\;0\to\kwd{int}$ to $\evid$.

One cannot simply add a or-coercion, that is, a coercion $c_1\;|\;c_2$ that
mirrors the or-pattern and distributes $c_1$ and $c_2$ on the left side and the
right side of the or-pattern when matching. This is not powerful enough, because
we decided to implement a generalizing \code{match} (this is the topic of
section \vref{desugar}). As a consequence, there were some cases where the
coercions needed for each branch of a \code{match} construct were different. If
the \code{match} only has one branch, then the or-coercion is enough. But if the
\code{match} has different branches, one must choose a coercion for each branch
of the \code{match}.

In this scenario, the most reasonable option was to attach coercions to
patterns. That way, $e \blacktriangleright c$ becomes syntactic sugar for
$\kwd{let}\; \evid \blacktriangleright c = e\;\kwd{in}\; \evid$. Applying a
different coercion on each side of the or-pattern boils down to changing the
pattern into $p_1 \blacktriangleright c_1\;|\; p_2 \blacktriangleright c_2$.

\subsection{A set of rules}

What happens now is that, in the process of translating the CamlX term
(see figure \vref{fig:structure}), that has De Bruijn types but complex
expressions, into System F, we \emph{rewrite} patterns to introduce coercions
wherever needed.

Although the subtyping relation in $F_\eta$ is undecidable \cite{mitchell-88},
we have a deterministic procedure for adding the required coercions. This is due
to the fact that we only use very specific coercions that can be determined by
examining simultaneously the pattern and the type of the pattern.

As we anticipated in previous sections, the type of the whole pattern is a piece
of information we've attached to the decorated AST, and that we've forwarded
through the different passes. Thus, we just need to apply the rules in figure
\vref{fig:coercion_generation} to insert coercions in patterns when needed.

The rules for generating coercions are structured as follows.
\begin{itemize}
  \item The main function is $\llbracket\,\bullet\,\rrbracket$: it matches a
    pattern and its type in parallel, and generates a corresponding coercion. As
    we said before, in the case of an or-pattern, coercions are attached to the
    patterns on both sides.
  \item $\kwd{push}$ recursively applies $(\forall\times)$ under $\forall$. This
    allows $\llbracket\,\bullet\,\rrbracket$ to coerce $\forallvec (\tau_1,
    \dots, \tau_n)$ into $(\forallvec \tau_1, \dots, \forallvec \tau_n)$, and
    recursively generate the sub-coercions for each branch of the tuple.
  \item $\kwd{elim}$, when applied to $\forallvec\tau$, removes all the
    $\forall$s that quantify on type variables that do not appear in $\tau$.
    This allows $\llbracket\,\bullet\,\rrbracket$, when it hits an identifier at
    the end of the recursive calls, to remove all the unused quantifiers.
\end{itemize}

As an example, the $\llbracket\,\bullet\,\rrbracket$ function, when matching
$(x, y)$ against $$\forall\forall.\; (1 \to 1, 0 \to \kwd{int})$$
generates the following pattern
$$\big(x \blacktriangleright \forall[\bullet[\perp]],
   y \blacktriangleright \bullet[\perp]\big)
\blacktriangleright \forall\big[\forall\times\big]; \forall\times$$
which results in $x$ being assigned the type scheme $\forall.\; 0 \to 0$ and $y$
being assigned the type scheme $\forall.\; 0 \to \kwd{int}$.

We will come back later on this feature with a larger example.

\subsection{More on coercions}

One of the goals we haven't achieved yet consists in mapping the value
restriction \emph{à la} Garrigue into these coercions. Because this just boils
down to introducing a few $\forall$s and instantiating some of them to $\perp$,
this should be feasible rather easily. This kind of manipulation on types is
a perfect candidate for a translation in terms of coercions.

\section{Other simplifications\label{desugar}}

Out of the many constructs that are offered by OCaml, many of them are
redundant. For instance, the \code{function} keyword can be replaced by
\code{fun x -> match x with}. We might also want stronger guarantees, such as
the uniqueness of identifiers. These many simplifications are all performed in the
translation from CamlX to the Core language.

\subsection{Uniqueness of identifiers}

Previously, identifiers were simply scoped strings. That is, one had to
forward an environment with a mapping when walking the tree, in order to map
information to identifiers. This is not necessarily an issue, but one construct
of the original OCaml caused us some pain through our translations.

\begin{verbatim}
let _ =
  let i = 2 and j = 3 in
  let i = j + i and j = j - i in
  i, j
;;
- : int * int = (5, 1)
\end{verbatim}

Because of these simultaneous definitions, we had to keep not only one scheme in
the \code{let}-constraints, but a list of schemes. Similarly, we had to use
lists all the way to the final step of the translation. One could argue that
disambiguating this was possible early on. However, the initial steps are not
supposed to deal with program transformations, and we leave this to the final
step.

In order to recover the simple $\lambda$-calculus \code{let}-binding, we
translate all identifiers to \emph{atoms}. Atoms are implemented as records with
a globally unique identifier and possibly more information, such as the name of
the original identifier for errors and pretty-printing.

That way, we desugar the example above into:

\begin{verbatim}
let _/62 =
  let i/59 = 2 in
  let j/58 = 3 in
  let i/61 = (+)/56 j/58 i/59 in
  let j/60 = (-)/55 j/58 i/59 in
  (i/61, j/60)
in
()
\end{verbatim}

Without the unique numbers appended to the original identifiers, this example
would be semantically wrong\footnote{One might wonder why the \code{let \_} is
treated as an identifier and not a pattern. This is because the \code{let \_}
has a special status. The OCaml AST has a special \code{Pstr\_value} node for
it. We map it to a \code{let}-pattern in our ``term with holes''. Finally, for
the translation to CoreML, we translate it to a fresh identifier to avoid
introducing a useless \code{match}.}!

\subsection{Desugaring}

Here follow some quick descriptions of all the constructs we had to remove to
end up with a pure, System F-like language.

\subsubsection{Removing \code{let}-patterns}

Patterns can be used conveniently in many places in OCaml. After removing
simultaneous, multiple \code{let}-bindings, we endeavoured to remove
\code{let}-patterns. Here's a sample program that uses a pattern.

\begin{verbatim}
let (x, y) = (fun x -> x)(1, 2)
;;
\end{verbatim}
We translate it to a program that directly matches the corresponding expression.
This is nothing but the standard semantics of \code{let}-pattern.
\begin{verbatim}
match
  (λ (x/39: int * int) -> x/39) (1, 2)
with
  | (x/37, y/38) ▸ id; id -> ()
\end{verbatim}

\subsubsection{Removing \code{function}}

One feature that was trickier was removing \code{function}. This keyword allows
one to fuse a regular \code{fun} and a \code{match} together. Let us consider
the sample code below.

\begin{verbatim}
let fst = function x, y -> x
val fst: ∀ β α. α * β → α
\end{verbatim}

The second line is the output from our solver that prints, as an intermediate
result, the type of bound identifiers. If one decides to only keep the type
schemes for identifiers (that is, keeping the type scheme of \code{x} and
\code{y}), then type information is missing to introduce a temporary identifier.

Fortunately, because we also annotate the CamlX AST with type schemes
for the whole patterns, we can introduce an artificial identifier, switch to a
regular \code{fun} and then match on the fake identifier. The resulting program
is shown below -- this is the output of our pretty-printer.

\begin{verbatim}
let fst/29 =
  ΛΛ. λ (__internal/30: 1 * 0) -> 
    match __internal/30 with
      | (x/31, y/32) -> x/31
in
()
\end{verbatim}

One important thing to remember is that, because we are in ML, we don't need to
apply coercions to this identifier. Functions \emph{do not have polymorphic
arguments} in ML, so there is really no reason to apply coercions to the
\code{\_\_internal} identifier.

\subsection{Bonus features}

We also decided to include a new feature, the \emph{generalizing match}. The
following example type-checks with our tool but does not with OCaml.

\begin{verbatim}
let s, f = match ("", fun x -> x) with
  | _, f ->
      f 2, f 2.
  | _ ->
      42, 42.
\end{verbatim}

Because \code{f} needs to be polymorphic, we must generalize \code{e} in
\code{match e with\dots}. However, OCaml does not perform such a generalization.
This was originally started as a proof that such trivial changes only require
tweaking the constraint generator, but we decided to keep it as the required
changes to make it work in System F were minimal.

The important part is that because \code{match}es now can operate on
expressions with polymorphism inside, we need to apply coercions inside
\code{match}es. As we said previously, because we chose to attach coercions to
patterns, this generalizing match translates naturally in CoreML.

\begin{verbatim}
match
  match
    Λ. ("", (λ (x/69: 0) -> x/69))
  with
    | (_, f/70) ▸ ∀×; id ->
        ((f/70•[int]) 2, (f/70•[float]) 2.)
    | _ ->
        (42, 42.)
with
  | (s/67, f/68) ▸ id; id ->
      ()
\end{verbatim}

The sample output above is the pretty-printed F-term that results of this
translation. Different coercions are used in the branches of the \code{match}.

\section{An example}

We finish this part with a slightly bigger example, and all the intermediate
representations.

\subsection{Original OCaml program}

This program demonstrates the following features: \code{let}-patterns,
generalization, instantiation, coercions.

We do not show the generated constraints below, as they are quite huge and hard
to understand.

\begin{verbatim}
let a, b =
  let g, h = 2, fun x -> x in
  h 2, h
\end{verbatim}

\subsection{The decorated AST (CamlX)}

The schemes that are displayed have been converted to De Bruijn.
\code{let}-patterns are annotated with the scheme of the whole pattern, which is
necessary to generate proper coercions.

\begin{verbatim}
let (a, b): ∀. [int * (0 → 0)] = Λ.
  let (g, h): ∀. [int * (0 → 0)] = Λ.
    (2, (fun (x: 0) -> x))
  in
  (h [int] 2, h [0])
in
()
\end{verbatim}

Arguments to functions are annotated with their type as well, because we are
targeting a Curry-style System F.

This representation is still feature-rich, and coercions are implicit. We
introduce coercions and desugar in the next step.

\subsection{The core AST}

This AST is desugared. Once can notice that we coerce the initial
\code{let}-binding by eliminating the unused quantification in the first
component of the tuple: $\forall.\;\kwd{int}$ becomes just $\kwd{int}$.

We have chosen to compose the coercions for each branch of a tuple
\emph{outside} the tuple, for simplicity reasons. This is strictly equivalent to
attaching the coercions inside each branch of the tuple, as we wrote in the
rules for generating coercions.

\begin{verbatim}
match
  Λ. match
    Λ. (2, (λ (x/52: 0) -> x/52))
  with
    | (g/50, h/51) ▸ ∀×; ×0[•[bottom]] ->
        (h/51•[int] 2, h/51•[0])
with
  | (a/48, b/49) ▸ ∀×; ×0[•[bottom]] ->
      ()
\end{verbatim}

The resulting \code{match} is actually generalizing, and \code{h} is
instantiated twice with different arguments.

\part{A reflexion on this work}

\section{Future improvements}

\subsection{Actually writing the type-checker}

As time went missing, we did not complete the final type-checker before
finishing this report. It should be an easy task, though. Paving the way and
finding the correct representation was actually more difficult, and we hope to
finish this soon before the final presentation.

\subsection{Enriching the language}

Algebraic data types are missing, and we hope to add them soon, both in CamlX
and CoreML. The coercions shouldn't be changed very much, though, except adding
the coercions that witness the covariance of algebraic data types and the
distributivity of $\forall$ with regard to them.

Moreover, although the first half of our tool properly supports equirecursive
types, and properly prints inferred types that contains equirecursive types, the
translation process does not support them at all.  This will reveal some
interesting challenges: indeed, equirecursive types will require to introduce
$\mu$ combinators to pack and unpack recursive types, thus transforming our
target language into $F_{\mu\eta}$.

Finally, if we introduce data types, we might as well introduce records. We're
also thinking of introducing polymorphic variants to explore how well they
translate into System F.

\section{Related work}

\subsection{Modules}

One core feature of ML is modules. Related research \cite{rossberg2010f} has
explored a way to translate modules into System F. This matches our goals quite
nicely, but recursive modules seem to be unsupported. This is one area that
could be explored if we were to further enrich our language.

\subsection{A real compiler}

Simon Peyton Jones argues \cite{sulzmann2007system} that such an intermediate
representation is well-suited for compilation. Since LLVM \cite{llvm} has been
making a lot of progress lately, and already starts to provide hooks for
external GCs, it would be interesting to explore the feasibility of a LLVM
backend for our intermediate language.

\subsection{More modern features}

As the language is still quite ``clean'', it might be interesting to explore
some additions to the regular OCaml: better dealing with effects, possibly
adding more modern features such as GADTs \cite{simonet2004constraint}, or type
classes. Such work remains a far, distant sight.

\section{Conclusion}

So far, it seems possible to translate a reasonable subset of OCaml into System
F. This is interesting for checking the well-typedness of the intermediate
representation, and maybe performing some program analysis, although we have not
explored this path yet. Some features that initially seemed overly complicated
actually can be translated quite naturally into System F, which justifies their
existence. The framework we have built will allow us to type-check \emph{a
posteriori} some more complicated features, and we believe this work is already
promising.

Some interesting exploration lies ahead of us: we could, for instance, try to
translate more ``exotic'' features, such as polymorphic variants, and see how
well they interact with our subset of System $F_\eta$. They might translate
nicely, which would mean they correspond to some ``fundamental'' concept. If
they do not translate nicely, this might mean they are not the right
abstraction; perhaps changing their original semantics might help them express
more ``basic'' ideas.

Finally, this work could be reused as a framework for performing type-checking
\emph{à la} ML for other languages. Since we have paid much attention to the
general structure of our program, it is perfectly feasible to write a parser and
a constraint generator for another language, without touching the rest of the
tool. We hope to show with this experiment that a fresh design for type-checking
is actually doable, and that it helps us augment our trust in the compilation
process.

\part{Bibliography}

\bibliographystyle{alpha}
\bibliography{rapport}

\end{document}

%TODO:
% Bien utiliser tous les noms des langages intermédiaires histoire de clarifier
% les choses
% Mettre la syntaxe de CamlX en annexe
% Faire une figure avec toutes les règles de sous-typage
